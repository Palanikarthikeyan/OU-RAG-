{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99818893-ea79-420b-8f74-5ac7fcb03bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Python\n",
    " - class and object\n",
    " python.org ----> python -- develop python code - pip install numpy pandas matplotlib scikilearn ..\n",
    " ------------------------------\n",
    "anaconda python/download --> python + DE+ML+DL libs \n",
    "--------------------------------\n",
    "\n",
    "CProgram ==> CPython --- GIL is enabled (Global Interpreter Lock) //mutext -lock and unlock\n",
    "Vs\n",
    "Ipython - GIL is disabled\n",
    "Jypthon - GIL is disabled \n",
    "-- create threads + synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f246fd9-c3fb-4691-b65d-c73dcd56e116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x7ffb5ce2b4c8\n",
      "0x7ffb5ce2b4c8\n",
      "0x7ffb5ce2b4c8\n",
      "0x7ffb5ce2b4c8\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "j = 4+6\n",
    "k = 3+7\n",
    "print(hex(id(10)))\n",
    "print(hex(id(i)))\n",
    "print(hex(id(j)))\n",
    "print(hex(id(k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c079ea75-795e-4794-b162-9abb144d5f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1\n"
     ]
    }
   ],
   "source": [
    "def display(a):\n",
    "    print(a)\n",
    "\n",
    "display('data1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d950dff1-e886-4825-ad02-75727a85d3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.box"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class box:\n",
    "    def display():\n",
    "        print('display block')\n",
    "\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d3ed783-a9a4-4bbc-9745-562406370776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.box at 0x1cd1f22cc20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b88e9c8a-2029-4dd3-88c3-3f491d84e4a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "box.display() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m obj1 \u001b[38;5;241m=\u001b[39m box()\n\u001b[1;32m----> 2\u001b[0m obj1\u001b[38;5;241m.\u001b[39mdisplay()\n",
      "\u001b[1;31mTypeError\u001b[0m: box.display() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "obj1 = box()\n",
    "obj1.display() --> display(obj1)\n",
    "TypeError: box.display() takes 0 positional arguments but 1 was given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3e8ae58-15c3-4918-8578-ade6b27e548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self= <__main__.box object at 0x000001CD217723C0>\n",
      "self= <__main__.box object at 0x000001CD2177D950>\n"
     ]
    }
   ],
   "source": [
    "class box:\n",
    "    def display(self):\n",
    "        print(\"self=\",self)\n",
    "\n",
    "obj1 = box()\n",
    "obj1.display() # display(obj1)\n",
    "\n",
    "obj2 = box()\n",
    "obj2.display() # display(obj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844069e-6491-4107-8b22-67cf971be731",
   "metadata": {},
   "outputs": [],
   "source": [
    "file: ab.py                                            file: p1.py\n",
    "----------------                                    ================\n",
    "def embedding():                                   import ab\n",
    "    return [1.0,2.0]                               ab.embedding()\n",
    "\n",
    "class OpenAI:                                      myobj = ab.OpenAI()\n",
    "    def vector_emb(self):                          myobj.vector_emb() \n",
    "        ...                                            file: p2.py\n",
    "        return [10.24,30.3,344.55]                    ==============\n",
    "---------------------------------------                from ab import embedding,OpenAI\n",
    "                                                       result = embedding()\n",
    "                                                       myobj = OpenAI()\n",
    "                                                       myobj.vector_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d19a27-edad-468d-9c73-e55aabb0595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP Terms\n",
    "==========\n",
    "    1. Corpus - Paragraph \n",
    "    2. Documents - Sentence \n",
    "    3. Vocabulary - unique words\n",
    "\n",
    "Hello Good morning had your food  ----------------->  French /..../.../... (Google Translate) \n",
    "\n",
    "\"I like to drink apple juice\n",
    "my friend likes to drink mango juice\" //Corpus \n",
    "    |\n",
    "    I like to drink apple juice - Doc1\n",
    "    my friend likes to drink mango juice - Doc2\n",
    "    |\n",
    "    i like to drink apple juice   - Doc1\n",
    "    my friend likes to drink mango juice - Doc2\n",
    "    |\n",
    "    i like to drink apple juice my friend likes mango //unique words \n",
    "\n",
    "Step 1: Tokenization = NLP Text processing \n",
    "Step 2: Text processing - BOW,ngrams\n",
    "Step 3: Text processing -  word2vector - Deeplearning\n",
    "Step 4: Neural network\n",
    "Step 5: Word embedding\n",
    "Step 6: Transformer \n",
    "Step 7: BERT \n",
    "\n",
    "[DataSet] ---> [Text processing]->[Stemming/lemmatization and stop words] ->[vectors]\n",
    "               =========================================================\n",
    "\n",
    "Stemming - reducing root word => eating Vs eat\n",
    "                                 history =>histori\n",
    "lemmitazation - lemma \n",
    "    |->Parts of Speech (POS)\n",
    "\n",
    "One Hot Encoding\n",
    "===================\n",
    "    D1 - the food good bad \n",
    "\n",
    "    the -  1 0 0 0\n",
    "    food - 0 1 0 0\n",
    "    good - 0 0 1 0\n",
    "    bad  - 0 0 0 1\n",
    "\n",
    "    BOW + frequency\n",
    "\n",
    "    ngrams \n",
    "    n=1\n",
    "    the => 1 0 0 0 \n",
    "    food => 0 1 0 0\n",
    "\n",
    "    n=2\n",
    "    the food => 1 0 0\n",
    "\n",
    "    n=3\n",
    "    the food good => 1 0\n",
    "\n",
    "    n=4\n",
    "    the food good bad => 1\n",
    "    ...                  ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67d78951-06b8-4783-9c62-4f71866924a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97947b-9c02-43f4-be87-c3745c84c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "662fc306-bdb2-4e18-9e55-7efa6be9a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '''Hello welcome,to Gen AI NLP Lecture\n",
    "Please do activity ! to become expert in NLP'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "010cda44-8f0c-468d-ba50-4500fa882ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello welcome,to gen ai nlp lecture\\nplease do activity ! to become expert in nlp'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dd323f4-edc2-47e9-8d36-2b62ad153053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello welcome,to gen ai nlp lecture\\nplease do activity ! to become expert in nlp'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = corpus.lower()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53269ab-ec08-4f7e-9e8a-fac56d016493",
   "metadata": {},
   "outputs": [],
   "source": [
    "LookupError\n",
    "...\n",
    ">>> import nltk\n",
    ">>> nltk.download('<tokenName>')(ex: nltk.download('punkt_tab'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8788250-6aaa-4c1a-aa8d-63c9d3d1d91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function nltk.tokenize.word_tokenize(text, language='english', preserve_line=False)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78a45fed-cb25-45d2-8ca1-13b5878f70df",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'gen',\n",
       " 'ai',\n",
       " 'nlp',\n",
       " 'lecture',\n",
       " 'please',\n",
       " 'do',\n",
       " 'activity',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'nlp']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1462832-7f43-4ebf-9239-cf441cf4c6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello welcome,to gen ai nlp lecture\\nplease do activity !',\n",
       " 'to become expert in nlp']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34f77925-4ff3-4b74-abf6-db7bdfc8f350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello arun,how are you doing?',\n",
       " \"ok. i am learning nlp activities's code ab'c data1.data2\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg=\"hello arun,how are you doing? ok. i am learning nlp activities's code ab'c data1.data2\"\n",
    "sent_tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c5a71bf-3f3d-4230-962b-500916b086ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'arun',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'ok.',\n",
       " 'i',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'nlp',\n",
       " 'activities',\n",
       " \"'s\",\n",
       " 'code',\n",
       " 'ab',\n",
       " \"'\",\n",
       " 'c',\n",
       " 'data1.data2']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0e4eaee-2cee-48ce-8579-f0bbba778e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'arun',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'ok',\n",
       " '.',\n",
       " 'i',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'nlp',\n",
       " 'activities',\n",
       " \"'\",\n",
       " 's',\n",
       " 'code',\n",
       " 'ab',\n",
       " \"'\",\n",
       " 'c',\n",
       " 'data1',\n",
       " '.',\n",
       " 'data2']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6428b368-9124-4f81-8145-0f40585584cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming - stem - reduce root word\n",
    "from nltk.stem import PorterStemmer\n",
    "obj_stemming = PorterStemmer()\n",
    "obj_stemming.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e51db10-88a9-4f2a-8455-456c31a2f379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_stemming.stem('history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16a741cf-11f4-43a9-a550-f1d6bb098c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_stemming.stem('Congratulations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc07c744-87ab-4357-b63d-860cffb00f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"eating\",\"eats\",\"eatern\",\"writing\",\"writes\",\n",
    "         \"programming\",\"program\",\"history\",\n",
    "         \"finally\",\"finalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae4abfa5-6f0d-422e-8f03-925bee3130c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating =====> eat\n",
      "eats =====> eat\n",
      "eatern =====> eatern\n",
      "writing =====> write\n",
      "writes =====> write\n",
      "programming =====> program\n",
      "program =====> program\n",
      "history =====> histori\n",
      "finally =====> final\n",
      "finalized =====> final\n"
     ]
    }
   ],
   "source": [
    "'''each and every word apply to stemming word'''\n",
    "\n",
    "for var in words:\n",
    "    print(var+\" =====> \"+obj_stemming.stem(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cad371b-36d3-464f-8d10-0a7def446279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regx Stemming\n",
    "from nltk.stem import RegexpStemmer\n",
    "# stem_obj = RegexpStemmer('RegxPattern') <= Constructor\n",
    "\n",
    "reg_obj = RegexpStemmer('ing$|s$')\n",
    "reg_obj.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f19f8c1c-bc25-4a68-8faa-daf38cea0d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'service'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_obj.stem('service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93aab2f0-86f4-412e-9871-1e43ffb955f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'service'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_obj.stem('services')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0311e929-0264-4cce-914a-3abecc814a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'systemd'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_obj.stem('systemd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04d41d58-d99f-49e4-aec1-2599ee8c0c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stem = SnowballStemmer('english')\n",
    "snowball_stem.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30de6c17-e8da-4914-a87a-f9ab7eb7fa75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(SnowballStemmer)\n",
    "snowball_stem.stem('history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3724c971-5283-42c1-94d5-809cc7c90a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'fair')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_stemming = PorterStemmer()\n",
    "snowball_stem = SnowballStemmer('english')\n",
    "\n",
    "obj_stemming.stem(\"fairly\"),snowball_stem.stem(\"fairly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46b28331-90b5-420b-aa66-90cceee85d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairly'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "lemma.lemmatize('fairly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba818853-cdc0-47d7-97bd-e68d86b3167d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eating'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1689369-681b-4db2-9392-fd61bcdf46aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('eating',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "448ecae9-61de-4fb1-bd61-8c35a99cd194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairly'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('fairly',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "284ce031-55b7-4d3a-a814-cd922e33b5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating =====> eat\n",
      "eats =====> eat\n",
      "eatern =====> eatern\n",
      "writing =====> write\n",
      "writes =====> write\n",
      "programming =====> program\n",
      "program =====> program\n",
      "history =====> histori\n",
      "finally =====> final\n",
      "finalized =====> final\n"
     ]
    }
   ],
   "source": [
    "words = [\"eating\",\"eats\",\"eatern\",\"writing\",\"writes\",\n",
    "         \"programming\",\"program\",\"history\",\n",
    "         \"finally\",\"finalized\"]\n",
    "\n",
    "for var in words:\n",
    "    print(var+\" =====> \"+obj_stemming.stem(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5822b979-d45f-4fb4-897e-8780efcfdab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating----->eat\n",
      "eats----->eat\n",
      "eatern----->eatern\n",
      "writing----->write\n",
      "writes----->write\n",
      "programming----->program\n",
      "program----->program\n",
      "history----->history\n",
      "finally----->finally\n",
      "finalized----->finalize\n"
     ]
    }
   ],
   "source": [
    "for var in words:\n",
    "    print(var+'----->'+lemma.lemmatize(var,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ae93eeb-e0e7-479c-a406-d86257bf2580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\karth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ae8d3be-eb9f-469b-a06a-98e64e7da070",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4938a6e4-21d9-4089-a99f-6f7326fa7244",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "224bb0df-2cbd-47f8-a688-49b2690587e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package nltk.corpus in nltk:\n",
      "\n",
      "NAME\n",
      "    nltk.corpus\n",
      "\n",
      "DESCRIPTION\n",
      "    NLTK corpus readers.  The modules in this package provide functions\n",
      "    that can be used to read corpus files in a variety of formats.  These\n",
      "    functions can be used to read both the corpus files that are\n",
      "    distributed in the NLTK corpus package, and corpus files that are part\n",
      "    of external corpora.\n",
      "\n",
      "    Available Corpora\n",
      "    =================\n",
      "\n",
      "    Please see https://www.nltk.org/nltk_data/ for a complete list.\n",
      "    Install corpora using nltk.download().\n",
      "\n",
      "    Corpus Reader Functions\n",
      "    =======================\n",
      "    Each corpus module defines one or more \"corpus reader functions\",\n",
      "    which can be used to read documents from that corpus.  These functions\n",
      "    take an argument, ``item``, which is used to indicate which document\n",
      "    should be read from the corpus:\n",
      "\n",
      "    - If ``item`` is one of the unique identifiers listed in the corpus\n",
      "      module's ``items`` variable, then the corresponding document will\n",
      "      be loaded from the NLTK corpus package.\n",
      "    - If ``item`` is a filename, then that file will be read.\n",
      "\n",
      "    Additionally, corpus reader functions can be given lists of item\n",
      "    names; in which case, they will return a concatenation of the\n",
      "    corresponding documents.\n",
      "\n",
      "    Corpus reader functions are named based on the type of information\n",
      "    they return.  Some common examples, and their return types, are:\n",
      "\n",
      "    - words(): list of str\n",
      "    - sents(): list of (list of str)\n",
      "    - paras(): list of (list of (list of str))\n",
      "    - tagged_words(): list of (str,str) tuple\n",
      "    - tagged_sents(): list of (list of (str,str))\n",
      "    - tagged_paras(): list of (list of (list of (str,str)))\n",
      "    - chunked_sents(): list of (Tree w/ (str,str) leaves)\n",
      "    - parsed_sents(): list of (Tree with str leaves)\n",
      "    - parsed_paras(): list of (list of (Tree with str leaves))\n",
      "    - xml(): A single xml ElementTree\n",
      "    - raw(): unprocessed corpus contents\n",
      "\n",
      "    For example, to read a list of the words in the Brown Corpus, use\n",
      "    ``nltk.corpus.brown.words()``:\n",
      "\n",
      "        >>> from nltk.corpus import brown\n",
      "        >>> print(\", \".join(brown.words())) # doctest: +ELLIPSIS\n",
      "        The, Fulton, County, Grand, Jury, said, ...\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    europarl_raw\n",
      "    reader (package)\n",
      "    util\n",
      "\n",
      "FUNCTIONS\n",
      "    demo()\n",
      "        # FIXME:  override any imported demo from various corpora, see https://github.com/nltk/nltk/issues/2116\n",
      "\n",
      "DATA\n",
      "    __annotations__ = {'abc': <class 'nltk.corpus.reader.plaintext.Plainte...\n",
      "    abc = <PlaintextCorpusReader in '.../corpora/abc' (not loaded yet)>\n",
      "    alpino = <AlpinoCorpusReader in '.../corpora/alpino' (not loaded yet)>\n",
      "    bcp47 = <BCP47CorpusReader in '.../corpora/bcp47' (not loaded yet)>\n",
      "    brown = <CategorizedTaggedCorpusReader in '.../corpora/brown' (not loa...\n",
      "    cess_cat = <BracketParseCorpusReader in '.../corpora/cess_cat' (not lo...\n",
      "    cess_esp = <BracketParseCorpusReader in '.../corpora/cess_esp' (not lo...\n",
      "    cmudict = <CMUDictCorpusReader in '.../corpora/cmudict' (not loaded ye...\n",
      "    comparative_sentences = <ComparativeSentencesCorpusReader in '.../corp...\n",
      "    comtrans = <AlignedCorpusReader in '.../corpora/comtrans' (not loaded ...\n",
      "    conll2000 = <ConllChunkCorpusReader in '.../corpora/conll2000' (not lo...\n",
      "    conll2002 = <ConllChunkCorpusReader in '.../corpora/conll2002' (not lo...\n",
      "    conll2007 = <DependencyCorpusReader in '.../corpora/conll2007' (not lo...\n",
      "    crubadan = <CrubadanCorpusReader in '.../corpora/crubadan' (not loaded...\n",
      "    dependency_treebank = <DependencyCorpusReader in '.../corpora/dependen...\n",
      "    extended_omw = <CorpusReader in '.../corpora/extended_omw' (not loaded...\n",
      "    floresta = <BracketParseCorpusReader in '.../corpora/floresta' (not lo...\n",
      "    framenet = <FramenetCorpusReader in '.../corpora/framenet_v17' (not lo...\n",
      "    framenet15 = <FramenetCorpusReader in '.../corpora/framenet_v15' (not ...\n",
      "    gazetteers = <WordListCorpusReader in '.../corpora/gazetteers' (not lo...\n",
      "    genesis = <PlaintextCorpusReader in '.../corpora/genesis' (not loaded ...\n",
      "    gutenberg = <PlaintextCorpusReader in '.../corpora/gutenberg' (not loa...\n",
      "    ieer = <IEERCorpusReader in '.../corpora/ieer' (not loaded yet)>\n",
      "    inaugural = <PlaintextCorpusReader in '.../corpora/inaugural' (not loa...\n",
      "    indian = <IndianCorpusReader in '.../corpora/indian' (not loaded yet)>\n",
      "    jeita = <ChasenCorpusReader in '.../corpora/jeita' (not loaded yet)>\n",
      "    knbc = <KNBCorpusReader in '.../corpora/knbc/corpus1' (not loaded yet)...\n",
      "    lin_thesaurus = <LinThesaurusCorpusReader in '.../corpora/lin_thesauru...\n",
      "    mac_morpho = <MacMorphoCorpusReader in '.../corpora/mac_morpho' (not l...\n",
      "    machado = <PortugueseCategorizedPlaintextCorpusReader in '.../corpora/...\n",
      "    masc_tagged = <CategorizedTaggedCorpusReader in '.../corpora/masc_tagg...\n",
      "    movie_reviews = <CategorizedPlaintextCorpusReader in '.../corpora/movi...\n",
      "    multext_east = <MTECorpusReader in '.../corpora/mte_teip5' (not loaded...\n",
      "    names = <WordListCorpusReader in '.../corpora/names' (not loaded yet)>\n",
      "    nombank = <NombankCorpusReader in '.../corpora/nombank.1.0' (not loade...\n",
      "    nombank_ptb = <NombankCorpusReader in '.../corpora/nombank.1.0' (not l...\n",
      "    nonbreaking_prefixes = <NonbreakingPrefixesCorpusReader in '.../corpor...\n",
      "    nps_chat = <NPSChatCorpusReader in '.../corpora/nps_chat' (not loaded ...\n",
      "    opinion_lexicon = <OpinionLexiconCorpusReader in '.../corpora/opinion_...\n",
      "    perluniprops = <UnicharsCorpusReader in '.../corpora/perluniprops' (no...\n",
      "    ppattach = <PPAttachmentCorpusReader in '.../corpora/ppattach' (not lo...\n",
      "    product_reviews_1 = <ReviewsCorpusReader in '.../corpora/product_revie...\n",
      "    product_reviews_2 = <ReviewsCorpusReader in '.../corpora/product_revie...\n",
      "    propbank = <PropbankCorpusReader in '.../corpora/propbank' (not loaded...\n",
      "    propbank_ptb = <PropbankCorpusReader in '.../corpora/propbank' (not lo...\n",
      "    pros_cons = <ProsConsCorpusReader in '.../corpora/pros_cons' (not load...\n",
      "    ptb = <CategorizedBracketParseCorpusReader in '.../corpora/ptb' (not l...\n",
      "    qc = <StringCategoryCorpusReader in '.../corpora/qc' (not loaded yet)>\n",
      "    reuters = <CategorizedPlaintextCorpusReader in '.../corpora/reuters' (...\n",
      "    rte = <RTECorpusReader in '.../corpora/rte' (not loaded yet)>\n",
      "    semcor = <SemcorCorpusReader in '.../corpora/semcor' (not loaded yet)>\n",
      "    senseval = <SensevalCorpusReader in '.../corpora/senseval' (not loaded...\n",
      "    sentence_polarity = <CategorizedSentencesCorpusReader in '.../corpora/...\n",
      "    sentiwordnet = <SentiWordNetCorpusReader in '.../corpora/sentiwordnet'...\n",
      "    shakespeare = <XMLCorpusReader in '.../corpora/shakespeare' (not loade...\n",
      "    sinica_treebank = <SinicaTreebankCorpusReader in '.../corpora/sinica_t...\n",
      "    state_union = <PlaintextCorpusReader in '.../corpora/state_union' (not...\n",
      "    stopwords = <WordListCorpusReader in 'C:\\\\Users\\\\karth\\\\AppData\\\\Roami...\n",
      "    subjectivity = <CategorizedSentencesCorpusReader in '.../corpora/subje...\n",
      "    swadesh = <SwadeshCorpusReader in '.../corpora/swadesh' (not loaded ye...\n",
      "    swadesh110 = <PanlexSwadeshCorpusReader in '.../corpora/panlex_swadesh...\n",
      "    swadesh207 = <PanlexSwadeshCorpusReader in '.../corpora/panlex_swadesh...\n",
      "    switchboard = <SwitchboardCorpusReader in '.../corpora/switchboard' (n...\n",
      "    timit = <TimitCorpusReader in '.../corpora/timit' (not loaded yet)>\n",
      "    timit_tagged = <TimitTaggedCorpusReader in '.../corpora/timit' (not lo...\n",
      "    toolbox = <ToolboxCorpusReader in '.../corpora/toolbox' (not loaded ye...\n",
      "    treebank = <BracketParseCorpusReader in '.../corpora/treebank/combined...\n",
      "    treebank_chunk = <ChunkedCorpusReader in '.../corpora/treebank/tagged'...\n",
      "    treebank_raw = <PlaintextCorpusReader in '.../corpora/treebank/raw' (n...\n",
      "    twitter_samples = <TwitterCorpusReader in '.../corpora/twitter_samples...\n",
      "    udhr = <UdhrCorpusReader in '.../corpora/udhr' (not loaded yet)>\n",
      "    udhr2 = <PlaintextCorpusReader in '.../corpora/udhr2' (not loaded yet)...\n",
      "    universal_treebanks = <ConllCorpusReader in '.../corpora/universal_tre...\n",
      "    verbnet = <VerbnetCorpusReader in '.../corpora/verbnet' (not loaded ye...\n",
      "    webtext = <PlaintextCorpusReader in '.../corpora/webtext' (not loaded ...\n",
      "    wordnet = <WordNetCorpusReader in 'C:\\\\Users\\\\karth\\\\AppDa...aming\\\\nl...\n",
      "    wordnet2021 = <WordNetCorpusReader in '.../corpora/wordnet2021' (not l...\n",
      "    wordnet2022 = <WordNetCorpusReader in '.../corpora/wordnet2022' (not l...\n",
      "    wordnet31 = <WordNetCorpusReader in '.../corpora/wordnet31' (not loade...\n",
      "    wordnet_ic = <WordNetICCorpusReader in '.../corpora/wordnet_ic' (not l...\n",
      "    words = <WordListCorpusReader in '.../corpora/words' (not loaded yet)>\n",
      "\n",
      "FILE\n",
      "    c:\\programdata\\anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#help(nltk.corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "461b45b8-6911-473d-9aa1-1546815418e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ',\n",
       " 'إذا',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'أف',\n",
       " 'أقل',\n",
       " 'أكثر',\n",
       " 'ألا',\n",
       " 'إلا',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللاتي',\n",
       " 'اللائي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أما',\n",
       " 'إما',\n",
       " 'أن',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'أنا',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'أنى',\n",
       " 'أنى',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'أو',\n",
       " 'أولاء',\n",
       " 'أولئك',\n",
       " 'أوه',\n",
       " 'آي',\n",
       " 'أي',\n",
       " 'أيها',\n",
       " 'إي',\n",
       " 'أين',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'إيه',\n",
       " 'بخ',\n",
       " 'بس',\n",
       " 'بعد',\n",
       " 'بعض',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بين',\n",
       " 'بيد',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'ثم',\n",
       " 'ثمة',\n",
       " 'حاشا',\n",
       " 'حبذا',\n",
       " 'حتى',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'خلا',\n",
       " 'دون',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ريث',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'شتان',\n",
       " 'عدا',\n",
       " 'عسى',\n",
       " 'عل',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'غير',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فلا',\n",
       " 'فمن',\n",
       " 'في',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'قد',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كم',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'لا',\n",
       " 'لاسيما',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'لئن',\n",
       " 'ليت',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'ما',\n",
       " 'ماذا',\n",
       " 'متى',\n",
       " 'مذ',\n",
       " 'مع',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'منذ',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'ها',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاهنا',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هؤلاء',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهو',\n",
       " 'يا',\n",
       " 'أبٌ',\n",
       " 'أخٌ',\n",
       " 'حمٌ',\n",
       " 'فو',\n",
       " 'أنتِ',\n",
       " 'يناير',\n",
       " 'فبراير',\n",
       " 'مارس',\n",
       " 'أبريل',\n",
       " 'مايو',\n",
       " 'يونيو',\n",
       " 'يوليو',\n",
       " 'أغسطس',\n",
       " 'سبتمبر',\n",
       " 'أكتوبر',\n",
       " 'نوفمبر',\n",
       " 'ديسمبر',\n",
       " 'جانفي',\n",
       " 'فيفري',\n",
       " 'مارس',\n",
       " 'أفريل',\n",
       " 'ماي',\n",
       " 'جوان',\n",
       " 'جويلية',\n",
       " 'أوت',\n",
       " 'كانون',\n",
       " 'شباط',\n",
       " 'آذار',\n",
       " 'نيسان',\n",
       " 'أيار',\n",
       " 'حزيران',\n",
       " 'تموز',\n",
       " 'آب',\n",
       " 'أيلول',\n",
       " 'تشرين',\n",
       " 'دولار',\n",
       " 'دينار',\n",
       " 'ريال',\n",
       " 'درهم',\n",
       " 'ليرة',\n",
       " 'جنيه',\n",
       " 'قرش',\n",
       " 'مليم',\n",
       " 'فلس',\n",
       " 'هللة',\n",
       " 'سنتيم',\n",
       " 'يورو',\n",
       " 'ين',\n",
       " 'يوان',\n",
       " 'شيكل',\n",
       " 'واحد',\n",
       " 'اثنان',\n",
       " 'ثلاثة',\n",
       " 'أربعة',\n",
       " 'خمسة',\n",
       " 'ستة',\n",
       " 'سبعة',\n",
       " 'ثمانية',\n",
       " 'تسعة',\n",
       " 'عشرة',\n",
       " 'أحد',\n",
       " 'اثنا',\n",
       " 'اثني',\n",
       " 'إحدى',\n",
       " 'ثلاث',\n",
       " 'أربع',\n",
       " 'خمس',\n",
       " 'ست',\n",
       " 'سبع',\n",
       " 'ثماني',\n",
       " 'تسع',\n",
       " 'عشر',\n",
       " 'ثمان',\n",
       " 'سبت',\n",
       " 'أحد',\n",
       " 'اثنين',\n",
       " 'ثلاثاء',\n",
       " 'أربعاء',\n",
       " 'خميس',\n",
       " 'جمعة',\n",
       " 'أول',\n",
       " 'ثان',\n",
       " 'ثاني',\n",
       " 'ثالث',\n",
       " 'رابع',\n",
       " 'خامس',\n",
       " 'سادس',\n",
       " 'سابع',\n",
       " 'ثامن',\n",
       " 'تاسع',\n",
       " 'عاشر',\n",
       " 'حادي',\n",
       " 'أ',\n",
       " 'ب',\n",
       " 'ت',\n",
       " 'ث',\n",
       " 'ج',\n",
       " 'ح',\n",
       " 'خ',\n",
       " 'د',\n",
       " 'ذ',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'س',\n",
       " 'ش',\n",
       " 'ص',\n",
       " 'ض',\n",
       " 'ط',\n",
       " 'ظ',\n",
       " 'ع',\n",
       " 'غ',\n",
       " 'ف',\n",
       " 'ق',\n",
       " 'ك',\n",
       " 'ل',\n",
       " 'م',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ي',\n",
       " 'ء',\n",
       " 'ى',\n",
       " 'آ',\n",
       " 'ؤ',\n",
       " 'ئ',\n",
       " 'أ',\n",
       " 'ة',\n",
       " 'ألف',\n",
       " 'باء',\n",
       " 'تاء',\n",
       " 'ثاء',\n",
       " 'جيم',\n",
       " 'حاء',\n",
       " 'خاء',\n",
       " 'دال',\n",
       " 'ذال',\n",
       " 'راء',\n",
       " 'زاي',\n",
       " 'سين',\n",
       " 'شين',\n",
       " 'صاد',\n",
       " 'ضاد',\n",
       " 'طاء',\n",
       " 'ظاء',\n",
       " 'عين',\n",
       " 'غين',\n",
       " 'فاء',\n",
       " 'قاف',\n",
       " 'كاف',\n",
       " 'لام',\n",
       " 'ميم',\n",
       " 'نون',\n",
       " 'هاء',\n",
       " 'واو',\n",
       " 'ياء',\n",
       " 'همزة',\n",
       " 'ي',\n",
       " 'نا',\n",
       " 'ك',\n",
       " 'كن',\n",
       " 'ه',\n",
       " 'إياه',\n",
       " 'إياها',\n",
       " 'إياهما',\n",
       " 'إياهم',\n",
       " 'إياهن',\n",
       " 'إياك',\n",
       " 'إياكما',\n",
       " 'إياكم',\n",
       " 'إياك',\n",
       " 'إياكن',\n",
       " 'إياي',\n",
       " 'إيانا',\n",
       " 'أولالك',\n",
       " 'تانِ',\n",
       " 'تانِك',\n",
       " 'تِه',\n",
       " 'تِي',\n",
       " 'تَيْنِ',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'ذانِ',\n",
       " 'ذِه',\n",
       " 'ذِي',\n",
       " 'ذَيْنِ',\n",
       " 'هَؤلاء',\n",
       " 'هَاتانِ',\n",
       " 'هَاتِه',\n",
       " 'هَاتِي',\n",
       " 'هَاتَيْنِ',\n",
       " 'هَذا',\n",
       " 'هَذانِ',\n",
       " 'هَذِه',\n",
       " 'هَذِي',\n",
       " 'هَذَيْنِ',\n",
       " 'الألى',\n",
       " 'الألاء',\n",
       " 'أل',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'ذيت',\n",
       " 'كأيّ',\n",
       " 'كأيّن',\n",
       " 'بضع',\n",
       " 'فلان',\n",
       " 'وا',\n",
       " 'آمينَ',\n",
       " 'آهِ',\n",
       " 'آهٍ',\n",
       " 'آهاً',\n",
       " 'أُفٍّ',\n",
       " 'أُفٍّ',\n",
       " 'أفٍّ',\n",
       " 'أمامك',\n",
       " 'أمامكَ',\n",
       " 'أوّهْ',\n",
       " 'إلَيْكَ',\n",
       " 'إلَيْكَ',\n",
       " 'إليكَ',\n",
       " 'إليكنّ',\n",
       " 'إيهٍ',\n",
       " 'بخٍ',\n",
       " 'بسّ',\n",
       " 'بَسْ',\n",
       " 'بطآن',\n",
       " 'بَلْهَ',\n",
       " 'حاي',\n",
       " 'حَذارِ',\n",
       " 'حيَّ',\n",
       " 'حيَّ',\n",
       " 'دونك',\n",
       " 'رويدك',\n",
       " 'سرعان',\n",
       " 'شتانَ',\n",
       " 'شَتَّانَ',\n",
       " 'صهْ',\n",
       " 'صهٍ',\n",
       " 'طاق',\n",
       " 'طَق',\n",
       " 'عَدَسْ',\n",
       " 'كِخ',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانكم',\n",
       " 'مكانكما',\n",
       " 'مكانكنّ',\n",
       " 'نَخْ',\n",
       " 'هاكَ',\n",
       " 'هَجْ',\n",
       " 'هلم',\n",
       " 'هيّا',\n",
       " 'هَيْهات',\n",
       " 'وا',\n",
       " 'واهاً',\n",
       " 'وراءَك',\n",
       " 'وُشْكَانَ',\n",
       " 'وَيْ',\n",
       " 'يفعلان',\n",
       " 'تفعلان',\n",
       " 'يفعلون',\n",
       " 'تفعلون',\n",
       " 'تفعلين',\n",
       " 'اتخذ',\n",
       " 'ألفى',\n",
       " 'تخذ',\n",
       " 'ترك',\n",
       " 'تعلَّم',\n",
       " 'جعل',\n",
       " 'حجا',\n",
       " 'حبيب',\n",
       " 'خال',\n",
       " 'حسب',\n",
       " 'خال',\n",
       " 'درى',\n",
       " 'رأى',\n",
       " 'زعم',\n",
       " 'صبر',\n",
       " 'ظنَّ',\n",
       " 'عدَّ',\n",
       " 'علم',\n",
       " 'غادر',\n",
       " 'ذهب',\n",
       " 'وجد',\n",
       " 'ورد',\n",
       " 'وهب',\n",
       " 'أسكن',\n",
       " 'أطعم',\n",
       " 'أعطى',\n",
       " 'رزق',\n",
       " 'زود',\n",
       " 'سقى',\n",
       " 'كسا',\n",
       " 'أخبر',\n",
       " 'أرى',\n",
       " 'أعلم',\n",
       " 'أنبأ',\n",
       " 'حدَث',\n",
       " 'خبَّر',\n",
       " 'نبَّا',\n",
       " 'أفعل به',\n",
       " 'ما أفعله',\n",
       " 'بئس',\n",
       " 'ساء',\n",
       " 'طالما',\n",
       " 'قلما',\n",
       " 'لات',\n",
       " 'لكنَّ',\n",
       " 'ءَ',\n",
       " 'أجل',\n",
       " 'إذاً',\n",
       " 'أمّا',\n",
       " 'إمّا',\n",
       " 'إنَّ',\n",
       " 'أنًّ',\n",
       " 'أى',\n",
       " 'إى',\n",
       " 'أيا',\n",
       " 'ب',\n",
       " 'ثمَّ',\n",
       " 'جلل',\n",
       " 'جير',\n",
       " 'رُبَّ',\n",
       " 'س',\n",
       " 'علًّ',\n",
       " 'ف',\n",
       " 'كأنّ',\n",
       " 'كلَّا',\n",
       " 'كى',\n",
       " 'ل',\n",
       " 'لات',\n",
       " 'لعلَّ',\n",
       " 'لكنَّ',\n",
       " 'لكنَّ',\n",
       " 'م',\n",
       " 'نَّ',\n",
       " 'هلّا',\n",
       " 'وا',\n",
       " 'أل',\n",
       " 'إلّا',\n",
       " 'ت',\n",
       " 'ك',\n",
       " 'لمّا',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ا',\n",
       " 'ي',\n",
       " 'تجاه',\n",
       " 'تلقاء',\n",
       " 'جميع',\n",
       " 'حسب',\n",
       " 'سبحان',\n",
       " 'شبه',\n",
       " 'لعمر',\n",
       " 'مثل',\n",
       " 'معاذ',\n",
       " 'أبو',\n",
       " 'أخو',\n",
       " 'حمو',\n",
       " 'فو',\n",
       " 'مئة',\n",
       " 'مئتان',\n",
       " 'ثلاثمئة',\n",
       " 'أربعمئة',\n",
       " 'خمسمئة',\n",
       " 'ستمئة',\n",
       " 'سبعمئة',\n",
       " 'ثمنمئة',\n",
       " 'تسعمئة',\n",
       " 'مائة',\n",
       " 'ثلاثمائة',\n",
       " 'أربعمائة',\n",
       " 'خمسمائة',\n",
       " 'ستمائة',\n",
       " 'سبعمائة',\n",
       " 'ثمانمئة',\n",
       " 'تسعمائة',\n",
       " 'عشرون',\n",
       " 'ثلاثون',\n",
       " 'اربعون',\n",
       " 'خمسون',\n",
       " 'ستون',\n",
       " 'سبعون',\n",
       " 'ثمانون',\n",
       " 'تسعون',\n",
       " 'عشرين',\n",
       " 'ثلاثين',\n",
       " 'اربعين',\n",
       " 'خمسين',\n",
       " 'ستين',\n",
       " 'سبعين',\n",
       " 'ثمانين',\n",
       " 'تسعين',\n",
       " 'بضع',\n",
       " 'نيف',\n",
       " 'أجمع',\n",
       " 'جميع',\n",
       " 'عامة',\n",
       " 'عين',\n",
       " 'نفس',\n",
       " 'لا سيما',\n",
       " 'أصلا',\n",
       " 'أهلا',\n",
       " 'أيضا',\n",
       " 'بؤسا',\n",
       " 'بعدا',\n",
       " 'بغتة',\n",
       " 'تعسا',\n",
       " 'حقا',\n",
       " 'حمدا',\n",
       " 'خلافا',\n",
       " 'خاصة',\n",
       " 'دواليك',\n",
       " 'سحقا',\n",
       " 'سرا',\n",
       " 'سمعا',\n",
       " 'صبرا',\n",
       " 'صدقا',\n",
       " 'صراحة',\n",
       " 'طرا',\n",
       " 'عجبا',\n",
       " 'عيانا',\n",
       " 'غالبا',\n",
       " 'فرادى',\n",
       " 'فضلا',\n",
       " 'قاطبة',\n",
       " 'كثيرا',\n",
       " 'لبيك',\n",
       " 'معاذ',\n",
       " 'أبدا',\n",
       " 'إزاء',\n",
       " 'أصلا',\n",
       " 'الآن',\n",
       " 'أمد',\n",
       " 'أمس',\n",
       " 'آنفا',\n",
       " 'آناء',\n",
       " 'أنّى',\n",
       " 'أول',\n",
       " 'أيّان',\n",
       " 'تارة',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'حقا',\n",
       " 'صباح',\n",
       " 'مساء',\n",
       " 'ضحوة',\n",
       " 'عوض',\n",
       " 'غدا',\n",
       " 'غداة',\n",
       " 'قطّ',\n",
       " 'كلّما',\n",
       " 'لدن',\n",
       " 'لمّا',\n",
       " 'مرّة',\n",
       " 'قبل',\n",
       " 'خلف',\n",
       " 'أمام',\n",
       " 'فوق',\n",
       " 'تحت',\n",
       " 'يمين',\n",
       " 'شمال',\n",
       " 'ارتدّ',\n",
       " 'استحال',\n",
       " 'أصبح',\n",
       " 'أضحى',\n",
       " 'آض',\n",
       " 'أمسى',\n",
       " 'انقلب',\n",
       " 'بات',\n",
       " 'تبدّل',\n",
       " 'تحوّل',\n",
       " 'حار',\n",
       " 'رجع',\n",
       " 'راح',\n",
       " 'صار',\n",
       " 'ظلّ',\n",
       " 'عاد',\n",
       " 'غدا',\n",
       " 'كان',\n",
       " 'ما انفك',\n",
       " 'ما برح',\n",
       " 'مادام',\n",
       " 'مازال',\n",
       " 'مافتئ',\n",
       " 'ابتدأ',\n",
       " 'أخذ',\n",
       " 'اخلولق',\n",
       " 'أقبل',\n",
       " 'انبرى',\n",
       " 'أنشأ',\n",
       " 'أوشك',\n",
       " 'جعل',\n",
       " 'حرى',\n",
       " 'شرع',\n",
       " 'طفق',\n",
       " 'علق',\n",
       " 'قام',\n",
       " 'كرب',\n",
       " 'كاد',\n",
       " 'هبّ']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dbb7439d-58a9-43ca-9005-c565cb43da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Corpus ->Sentence ->words ->stemming/lemma ->stopwords  ->result\n",
    "# -------------------------------------------------------------- =======\n",
    "paragraph='''Natural Language Processing (NLP) is a field of Artificial Intelligence (AI) \n",
    "that enables computers to understand, interpret, and generate human language.\n",
    "It combines computer science, linguistics, and machine learning to\n",
    "process both text and speech, allowing machines to derive meaning \n",
    "from human communication. Common applications include virtual assistants like Siri and Alexa,\n",
    "language translation, chatbots, search engines, and sentiment analysis.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f568f16b-079b-4574-b453-5a96eebaf0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(paragraph)\n",
    "print(type(sentences),len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c757ed20-2fae-49f7-97ba-484338680984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing (NLP) is a field of Artificial Intelligence (AI) \\nthat enables computers to understand, interpret, and generate human language.',\n",
       " 'It combines computer science, linguistics, and machine learning to\\nprocess both text and speech, allowing machines to derive meaning \\nfrom human communication.',\n",
       " 'Common applications include virtual assistants like Siri and Alexa,\\nlanguage translation, chatbots, search engines, and sentiment analysis.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c716c30f-1dc8-4c39-ba71-18f4ba3642f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stopwords ->filter ->steamming\n",
    "\n",
    "stopwords.words('english') ->collection - we can iterate one by one\n",
    " - steam_obj.stem(<collectionInput>) -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b2cefaeb-54aa-477b-8ad4-12d7743af7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "L=[]\n",
    "stemmer = PorterStemmer()\n",
    "for var in stopwords.words('english'):\n",
    "    r = stemmer.stem(var)\n",
    "    L.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b793b95-0d08-4f0c-857d-263b876ec46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[var])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[var] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7db46202-0217-4522-aae1-ef775f1f2990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natur languag process ( nlp ) field artifici intellig ( ai ) enabl comput understand , interpret , gener human languag .',\n",
       " 'it combin comput scienc , linguist , machin learn process text speech , allow machin deriv mean human commun .',\n",
       " 'common applic includ virtual assist like siri alexa , languag translat , chatbot , search engin , sentiment analysi .']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9232de17-2fd7-4bb7-ae7f-efec6f298ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d64dfdd2-345a-4a99-80b7-865e5aa80da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing ( NLP ) field Artificial Intelligence ( AI ) enables computer understand , interpret , generate human language .', 'It combine computer science , linguistics , machine learning process text speech , allowing machine derive meaning human communication .', 'Common application include virtual assistant like Siri Alexa , language translation , chatbots , search engine , sentiment analysis .']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "for var in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[var])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[var] = ' '.join(words)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b7218252-f56f-4eb9-832d-141045ff3214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing ( NLP ) field Artificial Intelligence ( AI ) enable computers understand , interpret , generate human language .', 'It combine computer science , linguistics , machine learn process text speech , allow machine derive mean human communication .', 'Common applications include virtual assistants like Siri Alexa , language translation , chatbots , search engines , sentiment analysis .']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "for var in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[var])\n",
    "    words = [lemmatizer.lemmatize(word,pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[var] = ' '.join(words)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f180def5-578e-4ae9-b02b-dbc6cac97d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bag-of-Words (BOW)\n",
    "---------------------\n",
    "set of word count \n",
    "- ignore => grammar and word order - frequency \n",
    "\n",
    "sentence = \"i likes to read nlp\" =>[\"likes\",\"nlp\",\"i\",\"to\",\"read\"]\n",
    "                                    [ 1,1,1,1,1]\n",
    "\n",
    "corpus = [\"the paper is white\",\n",
    "          \"the paper is nice\",\n",
    "          \"the sun is bright\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4325aa55-f82d-46ef-8378-3934efba3dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 1, 0],\n",
       "       [1, 1, 0, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vector_obj = CountVectorizer()\n",
    "\n",
    "corpus = [\"the paper is white\",\n",
    "          \"the paper is nice\",\n",
    "          \"the sun is bright\"]\n",
    "X = vector_obj.fit_transform(corpus)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e9a1305b-b731-49b1-915f-ea7e0d88699d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bright', 'is', 'nice', 'paper', 'sun', 'the', 'white'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_obj.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "faeb1028-487f-4cac-8c7d-7e70d148b8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bright' 'in' 'is' 'paper' 'sun' 'the' 'white']\n",
      "[[0 0 1 1 0 1 1]\n",
      " [1 0 1 1 0 1 0]\n",
      " [1 1 2 1 1 2 0]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"the paper is white\",\n",
    "          \"the paper is bright\",\n",
    "          \"the sun is in the paper is bright\"]\n",
    "\n",
    "vector_obj = CountVectorizer()\n",
    "X = vector_obj.fit_transform(corpus)\n",
    "print(vector_obj.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc368ec4-d26f-44c6-b86a-63023631d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "One Hot Encoding\n",
    "===================\n",
    "    D1 - the food good bad \n",
    "\n",
    "    the -  1 0 0 0\n",
    "    food - 0 1 0 0\n",
    "    good - 0 0 1 0\n",
    "    bad  - 0 0 0 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1c1b1d49-4cda-4dc0-9519-16804cc2356d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-Hot Encoding\n",
    "#---------------------\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "voc = ['food','good','bad','ok']\n",
    "\n",
    "label_obj = LabelEncoder() # Converts from categorical ->numerical \n",
    "label_encoded = label_obj.fit_transform(voc) # assign unique number\n",
    "print(label_encoded) # 1D\n",
    "obj = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "label_encoded = label_encoded.reshape(len(label_encoded),1)\n",
    "obj.fit_transform(label_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98753e22-4bae-47b8-925b-f9d92f59c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams\n",
    "- word combinations ->add context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3d18f2f7-a412-4b21-ab5b-018e0fcf5602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ngrams at 0x000001CD23656240>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = \"the food is good\"\n",
    "words = word_tokenize(s)\n",
    "ngrams(words,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d767aee1-0fb2-4d5a-a2b9-aa1982bdd0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b27217c0-8baa-4adf-bc72-72656c2fb1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object f at 0x000001CD30642AE0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generator ->function ->iterator \n",
    "def f():\n",
    "    yield 'd1'\n",
    "    yield 'd2','d2'\n",
    "    yield 'd3','d4','d5'\n",
    "f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea45a6d2-03e8-4064-abc3-a1acb33b194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. next(gen_obj)...StopIteration\n",
    "2. for loop - gen_obj\n",
    "3. typecast to list -> list(gen_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6ebc7655-7c89-4e22-894b-d8cc0af91e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the',), ('food',), ('is',), ('good',)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(s)\n",
    "list(ngrams(words,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "61240442-e27b-4e9c-9f01-473208c1851f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the',), ('food',), ('is',), ('good',)]\n",
      "[('the', 'food'), ('food', 'is'), ('is', 'good')]\n",
      "[('the', 'food', 'is'), ('food', 'is', 'good')]\n",
      "[('the', 'food', 'is', 'good')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "s = \"the food is good\"\n",
    "words = word_tokenize(s)\n",
    "print(list(ngrams(words,1)))\n",
    "print(list(ngrams(words,2)))\n",
    "print(list(ngrams(words,3)))\n",
    "print(list(ngrams(words,4)))\n",
    "print(list(ngrams(words,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d42510f8-941d-4a18-9f8c-e01896ee1688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(nltk.pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae6384-93ef-49c8-9873-6b927b310513",
   "metadata": {},
   "outputs": [],
   "source": [
    "Langchain\n",
    "=========\n",
    " |->Open Source framework - build applications(llm)\n",
    " |->GPT,Gemini,Ollama ..\n",
    "\n",
    " |->External data source ->Memory(store) ->..Agents //core parts\n",
    "\n",
    " langchain community\n",
    "-------------------------\n",
    "  |->integrated packages\n",
    "langchain-openai\n",
    "langchain-Ollama\n",
    "...\n",
    "---------------------\n",
    " +--------------+\n",
    " |              | \n",
    " +--------------+\n",
    "-----------------------\n",
    "\n",
    "1. download ollama => https://ollama.com/download {Enter}\n",
    "|\n",
    "2. select your OS -> Click donwload ->run\n",
    "|\n",
    "3. Go to commandprompt/terminal => ollama  pull  <modelName>  (ex: ollama pull gemma:2b {enter})\n",
    "                                   ------  =====  ------------\n",
    "4. To list available/installed models => ollama list {enter}\n",
    "|\n",
    "5. To run this model => ollama run <modelName> (ex: ollama run gemma:2b {enter})\n",
    "\n",
    "ollama pull <model>\n",
    "ollama run <model>\n",
    "ollama list\n",
    "ollama rm <model> # delete this model from local env\n",
    "rm ~/.ollama/models \n",
    "\n",
    "To delete more than one model\n",
    "----------=====================\n",
    "ollama rm llama3 gemma2:2b mistral \n",
    "\n",
    "To confirm - list\n",
    "--------------\n",
    "ollama list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084aef88-6d8e-4030-b471-c979eb21765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt\n",
    "# https://github.com/Palanikarthikeyan/OU-RAG-/blob/main/DAY1/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67d8f8-5877-4a62-a75c-e34e9a7d048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\karth>ollama list\n",
    "NAME    ID    SIZE    MODIFIED\n",
    "\n",
    "C:\\Users\\karth>ollama pull gemma2:2b\n",
    "pulling manifest\n",
    "pulling 7462734796d6: 100% ▕██████████████████████████▏ 1.6 GB\n",
    "pulling e0a42594d802: 100% ▕██████████████████████████▏  358 B\n",
    "pulling 097a36493f71: 100% ▕██████████████████████████▏ 8.4 KB\n",
    "pulling 2490e7468436: 100% ▕██████████████████████████▏   65 B\n",
    "pulling e18ad7af7efb: 100% ▕██████████████████████████▏  487 B\n",
    "verifying sha256 digest\n",
    "writing manifest\n",
    "success\n",
    "\n",
    "C:\\Users\\karth>ollama list\n",
    "NAME         ID              SIZE      MODIFIED\n",
    "gemma2:2b    8ccf136fdd52    1.6 GB    About a minute ago\n",
    "\n",
    "C:\\Users\\karth>\n",
    "C:\\Users\\karth>ollama run gemma2:2b\n",
    ">>>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a0bbd28b-9df3-4d52-8b43-f56de877b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "018600a9-0f03-4dbb-be8c-b97d5d533ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a70792c8-153e-4cb9-ae15-c64ee5998eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_ollama.llms.OllamaLLM'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "print(OllamaLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc8a70-ba4c-4b68-a2b1-c11ba5cb6a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_ollam/\n",
    "          |->llms.py\n",
    "                |__ class OllamaLLM:\n",
    "                        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e26de06-f7cd-4410-bb61-33b97823ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-community "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ca9c513e-42d5-4ad1-beb5-3fd4b5f94763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b9913622-f3da-410d-b03b-7be813ed86b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\os.py'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1777e4af-0372-48d6-88f3-70e109045e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\karth\\\\AppData\\\\Roaming\\\\Python\\\\Python313\\\\site-packages\\\\langchain_community\\\\__init__.py'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_community.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714aaf6-dedc-427b-902a-1d84e9cd4f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-ollam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52878fd5-c50b-4150-beb3-c62caa57b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollam import OllamaLLM\n",
    "# llm_object = OllamLLM(model=<modelName>)\n",
    "# llm_object.invoke('user_Query') ->result\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5e43b012-10e7-46ce-b363-9ad2f84001f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-ollama in c:\\programdata\\anaconda3\\lib\\site-packages (0.3.10)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-ollama) (0.6.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=0.3.76 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-ollama) (0.3.79)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\karth\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<2.0.0,>=0.3.76->langchain-ollama) (0.4.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.76->langchain-ollama) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.76->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.76->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.76->langchain-ollama) (4.12.2)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.76->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\karth\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<2.0.0,>=0.3.76->langchain-ollama) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=0.3.76->langchain-ollama) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\karth\\appdata\\roaming\\python\\python313\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain-ollama) (3.11.0)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain-ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain-ollama) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain-ollama) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain-ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain-ollama) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain-ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=0.3.76->langchain-ollama) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\karth\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=0.3.76->langchain-ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\karth\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=0.3.76->langchain-ollama) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain-ollama) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain-ollama) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain-ollama) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df9ac3e3-3e37-423b-b101-b232c9c3fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44cb776a-73e1-4e45-b303-1a037ce19dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> 3366\n"
     ]
    }
   ],
   "source": [
    "llm_obj = OllamaLLM(model='gemma2:2b')\n",
    "result1 = llm_obj.invoke('explain about langchain')\n",
    "print(type(result1),len(result1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c204fa04-8815-4125-9a1c-29c3b3e22c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## LangChain: Your AI Toolkit for Building Powerful Applications\n",
      "\n",
      "LangChain is a powerful and flexible framework designed to simplify the development of applications using large language models (LLMs) like GPT-3. Think of it as a toolbox that helps you build sophisticated AI systems by connecting LLMs with other data sources, tasks, and tools. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "**What LangChain does:**\n",
      "\n",
      "* **Connects LLMs to real-world information:**  It allows you to access external APIs (e.g., web search, databases), read documents, or even control other AI models within your applications. \n",
      "* **Builds chains of logic:**  Instead of relying solely on prompts, LangChain enables you to create complex workflows using \"chains\" where multiple steps are executed sequentially, allowing for sophisticated interactions and responses.\n",
      "* **Customizes the LLM experience:** You can customize how LLMs behave in your application by applying prompts, defining specific data formats, adjusting context, and more. \n",
      "\n",
      "**Why is LangChain important?**\n",
      "\n",
      "1. **Democratizes AI development:**  Before LangChain, building powerful LLM applications was complex and resource-intensive. It required extensive programming knowledge and access to specific tools. LangChain brings a user-friendly interface and reusable components, making it easier for developers of all levels to create impactful AI solutions.\n",
      "2. **Unlocks the full potential of LLMs:**  Beyond basic text generation, LangChain enables you to:\n",
      "    * Build conversational AI assistants \n",
      "    * Create document summarizers and question-answering systems\n",
      "    * Develop custom data analysis tools\n",
      "    * Generate code in various programming languages\n",
      "    * Automate tasks like data entry and research.\n",
      "\n",
      "**How LangChain works:**\n",
      "\n",
      "1. **Choose an LLM model:** Select the appropriate LLM based on your requirements (e.g., OpenAI's GPT-3, Google's PaLM). \n",
      "2. **Connect to external resources:** Integrate with APIs or databases to access relevant data for your application.\n",
      "3. **Build chains of actions:** Define a sequence of tasks and actions within the LangChain framework. This could involve fetching data, processing it using LLMs, interacting with other systems, and generating desired outputs. \n",
      "\n",
      "**Key components of LangChain:**\n",
      "\n",
      "* **Models:** Pre-configured LLM models or custom models can be loaded for integration.\n",
      "* **Prompts:** Tailor prompts to your application's specific needs. \n",
      "* **Data sources:** Access external data through APIs, files, and databases using the provided connectors. \n",
      "* **Chains:** Define sequences of actions, leveraging tools like \"memory\" to maintain context between steps. \n",
      "\n",
      "**LangChain benefits in a nutshell:**\n",
      "\n",
      "* **Flexibility:** Tailor your LLM application's behavior. \n",
      "* **Efficiency:** Automate complex tasks and workflows. \n",
      "* **Modularity:** Build applications with reusable components. \n",
      "* **Scalability:** Easily expand your AI systems as needed.\n",
      "\n",
      "\n",
      "If you're interested in exploring how LangChain can be used to develop powerful AI solutions, there are numerous resources available online:\n",
      "\n",
      "* **LangChain website:** https://langchain.com/ \n",
      "* **GitHub repository:** https://github.com/hwchase17/langchain\n",
      "\n",
      "\n",
      "LangChain is revolutionizing the development landscape of AI applications by offering a user-friendly and comprehensive toolkit that empowers anyone to unleash the full potential of LLMs!  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8adf598d-b8f3-4993-bac1-e17cfe3570dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def factorial(n):\n",
      "  \"\"\"Calculates the factorial of a non-negative integer.\n",
      "\n",
      "  Args:\n",
      "    n: The non-negative integer for which to calculate the factorial.\n",
      "\n",
      "  Returns:\n",
      "    The factorial of n if n is non-negative, otherwise raises ValueError. \n",
      "  \"\"\"\n",
      "  if n < 0:\n",
      "      raise ValueError(\"Factorial is not defined for negative numbers\")\n",
      "  elif n == 0:\n",
      "    return 1 # Factorial of 0 is 1\n",
      "  else:\n",
      "    result = 1\n",
      "    for i in range(1, n + 1):\n",
      "      result *= i\n",
      "    return result\n",
      "\n",
      "# Get user input\n",
      "num = int(input(\"Enter a non-negative integer: \"))\n",
      "\n",
      "# Calculate and print the factorial\n",
      "print(f\"The factorial of {num} is {factorial(num)}\") \n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. **Function Definition (`def factorial(n):`)**: This defines a function called `factorial` that takes an integer `n` as input.  This makes your code reusable!\n",
      "2. **Error Handling (`if n < 0:`)**: We first check if the input number is negative. If it is, we raise a `ValueError` to signal that factorial is not defined for negative numbers.\n",
      "3. **Base Case (`elif n == 0:`)**:  The factorial of 0 is 1. This is our base case; all other calculations will depend on this.\n",
      "4. **Iterative Calculation (`else:`):** If `n` is positive, we use a loop to calculate the factorial:\n",
      "   - `result = 1`: We start with `result` initialized to 1 because we'll multiply by increasing numbers.\n",
      "   - `for i in range(1, n + 1):`:  We iterate through numbers from 1 to `n` (inclusive).\n",
      "   - `result *= i`: In each iteration, the `result` is multiplied by the current number `i`. This accumulates the product.\n",
      "5. **Return Value (`return result`)**: Once the loop completes, we return the final value of `result`, which represents the factorial.\n",
      "\n",
      "6.  **User Input and Output:** \n",
      "   - The code prompts the user to enter a non-negative integer.\n",
      "   - It then calls the `factorial()` function, passes the input number as an argument, and prints the calculated factorial result using f-strings (for formatted output).\n",
      "\n",
      "\n",
      "**How the Factorial Calculation Works:**\n",
      "\n",
      "The factorial of a non-negative integer *n*, denoted by *n!*, is the product of all positive integers less than or equal to *n*. For example:\n",
      "* 5! = 5 * 4 * 3 * 2 * 1 = 120\n",
      "\n",
      "This code implements this calculation using a loop.  It starts at 1 and multiplies it by increasing numbers until it reaches the input number (`n`), effectively summing up all these products to give you the factorial value.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like me to explain any part of this in more detail or have other programming challenges! \n"
     ]
    }
   ],
   "source": [
    "result2 = llm_obj.invoke('How to write factorial program using python?')\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11e5e06e-538d-4973-908f-6dd5f74073e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide me with more information about the \"XYZ Organization\" you're interested in! \n",
      "\n",
      "To help me understand which organization you mean, please tell me:\n",
      "\n",
      "* **What industry is it related to?** (e.g., technology, healthcare, education)\n",
      "* **What country or region are they based in?** (e.g., United States, China)\n",
      "* **Do you know anything else about them?** (e.g., their mission statement, a specific product or service they offer)\n",
      "\n",
      "\n",
      "The more details you give me, the better I can assist you! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "result3 = llm_obj.invoke('Tell me about XYZ organization')\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a811d-5581-4b90-985f-6b390012d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our dataset - langchain_community.document_loaders\n",
    "                                              |->TextLoader\n",
    "                                              |->PDFLoader\n",
    "                                              |->...\n",
    "loader_data.load() -> [Document(metdata={},page_content='<dataset_content>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1a290ed-c5c4-4d14-8d7b-d4b24c109b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\karth'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bc48365-21cf-46c5-8728-b57d92cdefaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee55cd07-870f-49fd-ae2c-6aec75f48a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.document_loaders.text.TextLoader'>\n"
     ]
    }
   ],
   "source": [
    "print(TextLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e586b599-eb41-4928-bf82-1129fc7c473e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.document_loaders.text.TextLoader'>\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader('my_docs.txt')\n",
    "print(type(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a3e9312-0558-45eb-a118-87ecd2d3bab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'my_docs.txt'}, page_content=\"LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n\\nDevelopment: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\")]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0678df3-192e-467e-951d-393a4226584e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader('my_docs.txt')\n",
    "docs = loader.load()\n",
    "print(type(docs),len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d50ed59-cef5-4f97-8d7d-18e44bbb86ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'my_docs.txt'}, page_content=\"LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n\\nDevelopment: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\")]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "799deab8-097d-439d-aa64-ff28df669ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "pdf_loader = PyPDFLoader('attention.pdf')\n",
    "docs = pdf_loader.load()\n",
    "print(type(docs),len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ba65419-dfa9-4e58-9d27-f6fa9033caa3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]\n"
     ]
    }
   ],
   "source": [
    "docs = pdf_loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4dd58a4a-7687-402a-9a24-3397beec4476",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get('https://api.github.com/users/hadley/orgs')\n",
    "jd = r.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ef2345c-1b31-4736-a1f8-ec4bf6feb282",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'login': 'ggobi', 'id': 423638, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjQyMzYzOA==', 'url': 'https://api.github.com/orgs/ggobi', 'repos_url': 'https://api.github.com/orgs/ggobi/repos', 'events_url': 'https://api.github.com/orgs/ggobi/events', 'hooks_url': 'https://api.github.com/orgs/ggobi/hooks', 'issues_url': 'https://api.github.com/orgs/ggobi/issues', 'members_url': 'https://api.github.com/orgs/ggobi/members{/member}', 'public_members_url': 'https://api.github.com/orgs/ggobi/public_members{/member}', 'avatar_url': 'https://avatars.githubusercontent.com/u/423638?v=4', 'description': ''}, {'login': 'rstudio', 'id': 513560, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjUxMzU2MA==', 'url': 'https://api.github.com/orgs/rstudio', 'repos_url': 'https://api.github.com/orgs/rstudio/repos', 'events_url': 'https://api.github.com/orgs/rstudio/events', 'hooks_url': 'https://api.github.com/orgs/rstudio/hooks', 'issues_url': 'https://api.github.com/orgs/rstudio/issues', 'members_url': 'https://api.github.com/orgs/rstudio/members{/member}', 'public_members_url': 'https://api.github.com/orgs/rstudio/public_members{/member}', 'avatar_url': 'https://avatars.githubusercontent.com/u/513560?v=4', 'description': ''}, {'login': 'rstats', 'id': 722735, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjcyMjczNQ==', 'url': 'https://api.github.com/orgs/rstats', 'repos_url': 'https://api.github.com/orgs/rstats/repos', 'events_url': 'https://api.github.com/orgs/rstats/events', 'hooks_url': 'https://api.github.com/orgs/rstats/hooks', 'issues_url': 'https://api.github.com/orgs/rstats/issues', 'members_url': 'https://api.github.com/orgs/rstats/members{/member}', 'public_members_url': 'https://api.github.com/orgs/rstats/public_members{/member}', 'avatar_url': 'https://avatars.githubusercontent.com/u/722735?v=4', 'description': None}, {'login': 'ropensci', 'id': 1200269, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjEyMDAyNjk=', 'url': 'https://api.github.com/orgs/ropensci', 'repos_url': 'https://api.github.com/orgs/ropensci/repos', 'events_url': 'https://api.github.com/orgs/ropensci/events', 'hooks_url': 'https://api.github.com/orgs/ropensci/hooks', 'issues_url': 'https://api.github.com/orgs/ropensci/issues', 'members_url': 'https://api.github.com/orgs/ropensci/members{/member}', 'public_members_url': 'https://api.github.com/orgs/ropensci/public_members{/member}', 'avatar_url': 'https://avatars.githubusercontent.com/u/1200269?v=4', 'description': 'Tools and R Packages for Open Science'}, {'login': 'rjournal', 'id': 3330561, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjMzMzA1NjE=', 'url': 'https://api.github.com/orgs/rjournal', 'repos_url': 'https://api.github.com/orgs/rjournal/repos', 'events_url': 'https://api.github.com/orgs/rjournal/events', 'hooks_url': 'https://api.github.com/orgs/rjournal/hooks', 'issues_url': 'https://api.github.com/orgs/rjournal/issues', 'members_url': 'https://api.github.com/orgs/rjournal/members{/member}', 'public_members_url': 'https://api.github.com/orgs/rjournal/public_members{/member}', 'avatar_url': 'https://avatars.githubusercontent.com/u/3330561?v=4', 'description': None}, {'login': 'r-dbi', 'id': 5695665, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjU2OTU2NjU=', 'url': 'https://api.github.com/orgs/r-dbi', 'repos_url': 'https://api.github.com/orgs/r-dbi/repos', 'events_url': 'https://api.github.com/orgs/r-dbi/events', 'hooks_url': 'https://api.github.com/orgs/r-dbi/hooks', 'issues_url': 'https://api.github.com/orgs/r-dbi/issues', 'members_url': 'https://api.github.com/orgs/r-dbi/members{/member}', 'public_members_url': 'https://api.github.com/orgs/r-dbi/public_members{/member}', 'avatar_url': 'https://avatars.githubusercontent.com/u/5695665?v=4', 'description': 'R + databases'}, {'login': 'RConsortium', 'id': 15366137, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjE1MzY2MTM3', 'url': 'https://api.github.com/orgs/RConsortium', 'repos_url': 'https://api.github.com/orgs/RConsortium/repos', 'events_url': 'https://api.github.com/orgs/RConsortium/events', 'hooks_url': 'https://api.github.com/orgs/RConsortium/hooks', 'issues_url': 'https://api.github.com/orgs/RConsortium/issues', 'members_url': 'https://api.github.com/orgs/RConsortium/members{/member}', 'public_members_url': 'https://api.github.com/orgs/RConsortium/public_members{/member}', 'avatar_url': 'https://avatars.githubusercontent.com/u/15366137?v=4', 'description': 'The R Consortium, Inc was established to provide support to the R Foundation and R Community, using maintaining and distributing R software.'}, {'login': 'tidyverse', 'id': 22032646, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjIyMDMyNjQ2', 'url': 'https://api.github.com/orgs/tidyverse', 'repos_url': 'https://api.github.com/orgs/tidyverse/repos', 'events_url': 'https://api.github.com/orgs/tidyverse/events', 'hooks_url': 'https://api.github.com/orgs/tidyverse/hooks', 'issues_url': 'https://api.github.com/orgs/tidyverse/issues', 'members_url': 'https://api.github.com/orgs/tidyverse/members{/member}', 'public_members_url': 'https://api.github.com/orgs/tidyverse/public_members{/member}', 'avatar_url': 'https://avatars.githubusercontent.com/u/22032646?v=4', 'description': 'The tidyverse is a collection of R packages that share common principles and are designed to work together seamlessly'}, {'login': 'r-lib', 'id': 22618716, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjIyNjE4NzE2', 'url': 'https://api.github.com/orgs/r-lib', 'repos_url': 'https://api.github.com/orgs/r-lib/repos', 'events_url': 'https://api.github.com/orgs/r-lib/events', 'hooks_url': 'https://api.github.com/orgs/r-lib/hooks', 'issues_url': 'https://api.github.com/orgs/r-lib/issues', 'members_url': 'https://api.github.com/orgs/r-lib/members{/member}', 'public_members_url': 'https://api.github.com/orgs/r-lib/public_members{/member}', 'avatar_url': 'https://avatars.githubusercontent.com/u/22618716?v=4', 'description': ''}, {'login': 'rstudio-education', 'id': 34165516, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjM0MTY1NTE2', 'url': 'https://api.github.com/orgs/rstudio-education', 'repos_url': 'https://api.github.com/orgs/rstudio-education/repos', 'events_url': 'https://api.github.com/orgs/rstudio-education/events', 'hooks_url': 'https://api.github.com/orgs/rstudio-education/hooks', 'issues_url': 'https://api.github.com/orgs/rstudio-education/issues', 'members_url': 'https://api.github.com/orgs/rstudio-education/members{/member}', 'public_members_url': 'https://api.github.com/orgs/rstudio-education/public_members{/member}', 'avatar_url': 'https://avatars.githubusercontent.com/u/34165516?v=4', 'description': ''}]\n"
     ]
    }
   ],
   "source": [
    "print(jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd6e9c6c-d103-4edc-b085-124319cb904c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'avatar_url': 'https://avatars.githubusercontent.com/u/423638?v=4',\n",
      "  'description': '',\n",
      "  'events_url': 'https://api.github.com/orgs/ggobi/events',\n",
      "  'hooks_url': 'https://api.github.com/orgs/ggobi/hooks',\n",
      "  'id': 423638,\n",
      "  'issues_url': 'https://api.github.com/orgs/ggobi/issues',\n",
      "  'login': 'ggobi',\n",
      "  'members_url': 'https://api.github.com/orgs/ggobi/members{/member}',\n",
      "  'node_id': 'MDEyOk9yZ2FuaXphdGlvbjQyMzYzOA==',\n",
      "  'public_members_url': 'https://api.github.com/orgs/ggobi/public_members{/member}',\n",
      "  'repos_url': 'https://api.github.com/orgs/ggobi/repos',\n",
      "  'url': 'https://api.github.com/orgs/ggobi'},\n",
      " {'avatar_url': 'https://avatars.githubusercontent.com/u/513560?v=4',\n",
      "  'description': '',\n",
      "  'events_url': 'https://api.github.com/orgs/rstudio/events',\n",
      "  'hooks_url': 'https://api.github.com/orgs/rstudio/hooks',\n",
      "  'id': 513560,\n",
      "  'issues_url': 'https://api.github.com/orgs/rstudio/issues',\n",
      "  'login': 'rstudio',\n",
      "  'members_url': 'https://api.github.com/orgs/rstudio/members{/member}',\n",
      "  'node_id': 'MDEyOk9yZ2FuaXphdGlvbjUxMzU2MA==',\n",
      "  'public_members_url': 'https://api.github.com/orgs/rstudio/public_members{/member}',\n",
      "  'repos_url': 'https://api.github.com/orgs/rstudio/repos',\n",
      "  'url': 'https://api.github.com/orgs/rstudio'},\n",
      " {'avatar_url': 'https://avatars.githubusercontent.com/u/722735?v=4',\n",
      "  'description': None,\n",
      "  'events_url': 'https://api.github.com/orgs/rstats/events',\n",
      "  'hooks_url': 'https://api.github.com/orgs/rstats/hooks',\n",
      "  'id': 722735,\n",
      "  'issues_url': 'https://api.github.com/orgs/rstats/issues',\n",
      "  'login': 'rstats',\n",
      "  'members_url': 'https://api.github.com/orgs/rstats/members{/member}',\n",
      "  'node_id': 'MDEyOk9yZ2FuaXphdGlvbjcyMjczNQ==',\n",
      "  'public_members_url': 'https://api.github.com/orgs/rstats/public_members{/member}',\n",
      "  'repos_url': 'https://api.github.com/orgs/rstats/repos',\n",
      "  'url': 'https://api.github.com/orgs/rstats'},\n",
      " {'avatar_url': 'https://avatars.githubusercontent.com/u/1200269?v=4',\n",
      "  'description': 'Tools and R Packages for Open Science',\n",
      "  'events_url': 'https://api.github.com/orgs/ropensci/events',\n",
      "  'hooks_url': 'https://api.github.com/orgs/ropensci/hooks',\n",
      "  'id': 1200269,\n",
      "  'issues_url': 'https://api.github.com/orgs/ropensci/issues',\n",
      "  'login': 'ropensci',\n",
      "  'members_url': 'https://api.github.com/orgs/ropensci/members{/member}',\n",
      "  'node_id': 'MDEyOk9yZ2FuaXphdGlvbjEyMDAyNjk=',\n",
      "  'public_members_url': 'https://api.github.com/orgs/ropensci/public_members{/member}',\n",
      "  'repos_url': 'https://api.github.com/orgs/ropensci/repos',\n",
      "  'url': 'https://api.github.com/orgs/ropensci'},\n",
      " {'avatar_url': 'https://avatars.githubusercontent.com/u/3330561?v=4',\n",
      "  'description': None,\n",
      "  'events_url': 'https://api.github.com/orgs/rjournal/events',\n",
      "  'hooks_url': 'https://api.github.com/orgs/rjournal/hooks',\n",
      "  'id': 3330561,\n",
      "  'issues_url': 'https://api.github.com/orgs/rjournal/issues',\n",
      "  'login': 'rjournal',\n",
      "  'members_url': 'https://api.github.com/orgs/rjournal/members{/member}',\n",
      "  'node_id': 'MDEyOk9yZ2FuaXphdGlvbjMzMzA1NjE=',\n",
      "  'public_members_url': 'https://api.github.com/orgs/rjournal/public_members{/member}',\n",
      "  'repos_url': 'https://api.github.com/orgs/rjournal/repos',\n",
      "  'url': 'https://api.github.com/orgs/rjournal'},\n",
      " {'avatar_url': 'https://avatars.githubusercontent.com/u/5695665?v=4',\n",
      "  'description': 'R + databases',\n",
      "  'events_url': 'https://api.github.com/orgs/r-dbi/events',\n",
      "  'hooks_url': 'https://api.github.com/orgs/r-dbi/hooks',\n",
      "  'id': 5695665,\n",
      "  'issues_url': 'https://api.github.com/orgs/r-dbi/issues',\n",
      "  'login': 'r-dbi',\n",
      "  'members_url': 'https://api.github.com/orgs/r-dbi/members{/member}',\n",
      "  'node_id': 'MDEyOk9yZ2FuaXphdGlvbjU2OTU2NjU=',\n",
      "  'public_members_url': 'https://api.github.com/orgs/r-dbi/public_members{/member}',\n",
      "  'repos_url': 'https://api.github.com/orgs/r-dbi/repos',\n",
      "  'url': 'https://api.github.com/orgs/r-dbi'},\n",
      " {'avatar_url': 'https://avatars.githubusercontent.com/u/15366137?v=4',\n",
      "  'description': 'The R Consortium, Inc was established to provide support to '\n",
      "                 'the R Foundation and R Community, using maintaining and '\n",
      "                 'distributing R software.',\n",
      "  'events_url': 'https://api.github.com/orgs/RConsortium/events',\n",
      "  'hooks_url': 'https://api.github.com/orgs/RConsortium/hooks',\n",
      "  'id': 15366137,\n",
      "  'issues_url': 'https://api.github.com/orgs/RConsortium/issues',\n",
      "  'login': 'RConsortium',\n",
      "  'members_url': 'https://api.github.com/orgs/RConsortium/members{/member}',\n",
      "  'node_id': 'MDEyOk9yZ2FuaXphdGlvbjE1MzY2MTM3',\n",
      "  'public_members_url': 'https://api.github.com/orgs/RConsortium/public_members{/member}',\n",
      "  'repos_url': 'https://api.github.com/orgs/RConsortium/repos',\n",
      "  'url': 'https://api.github.com/orgs/RConsortium'},\n",
      " {'avatar_url': 'https://avatars.githubusercontent.com/u/22032646?v=4',\n",
      "  'description': 'The tidyverse is a collection of R packages that share '\n",
      "                 'common principles and are designed to work together '\n",
      "                 'seamlessly',\n",
      "  'events_url': 'https://api.github.com/orgs/tidyverse/events',\n",
      "  'hooks_url': 'https://api.github.com/orgs/tidyverse/hooks',\n",
      "  'id': 22032646,\n",
      "  'issues_url': 'https://api.github.com/orgs/tidyverse/issues',\n",
      "  'login': 'tidyverse',\n",
      "  'members_url': 'https://api.github.com/orgs/tidyverse/members{/member}',\n",
      "  'node_id': 'MDEyOk9yZ2FuaXphdGlvbjIyMDMyNjQ2',\n",
      "  'public_members_url': 'https://api.github.com/orgs/tidyverse/public_members{/member}',\n",
      "  'repos_url': 'https://api.github.com/orgs/tidyverse/repos',\n",
      "  'url': 'https://api.github.com/orgs/tidyverse'},\n",
      " {'avatar_url': 'https://avatars.githubusercontent.com/u/22618716?v=4',\n",
      "  'description': '',\n",
      "  'events_url': 'https://api.github.com/orgs/r-lib/events',\n",
      "  'hooks_url': 'https://api.github.com/orgs/r-lib/hooks',\n",
      "  'id': 22618716,\n",
      "  'issues_url': 'https://api.github.com/orgs/r-lib/issues',\n",
      "  'login': 'r-lib',\n",
      "  'members_url': 'https://api.github.com/orgs/r-lib/members{/member}',\n",
      "  'node_id': 'MDEyOk9yZ2FuaXphdGlvbjIyNjE4NzE2',\n",
      "  'public_members_url': 'https://api.github.com/orgs/r-lib/public_members{/member}',\n",
      "  'repos_url': 'https://api.github.com/orgs/r-lib/repos',\n",
      "  'url': 'https://api.github.com/orgs/r-lib'},\n",
      " {'avatar_url': 'https://avatars.githubusercontent.com/u/34165516?v=4',\n",
      "  'description': '',\n",
      "  'events_url': 'https://api.github.com/orgs/rstudio-education/events',\n",
      "  'hooks_url': 'https://api.github.com/orgs/rstudio-education/hooks',\n",
      "  'id': 34165516,\n",
      "  'issues_url': 'https://api.github.com/orgs/rstudio-education/issues',\n",
      "  'login': 'rstudio-education',\n",
      "  'members_url': 'https://api.github.com/orgs/rstudio-education/members{/member}',\n",
      "  'node_id': 'MDEyOk9yZ2FuaXphdGlvbjM0MTY1NTE2',\n",
      "  'public_members_url': 'https://api.github.com/orgs/rstudio-education/public_members{/member}',\n",
      "  'repos_url': 'https://api.github.com/orgs/rstudio-education/repos',\n",
      "  'url': 'https://api.github.com/orgs/rstudio-education'}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d1ae9f1-b81a-40e5-b37f-74c99bfcac9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9597fdae-c7c6-4770-b84a-5fc9236295c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "WebBaseLoader(web_path='https://URL1')\n",
    "WebBaseLoader(web_paths=['URL1','URL2','URL3']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b1bab02-e915-4f73-a49c-a57444d050b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.google.com', 'title': 'Google', 'language': 'en-IN'}, page_content='GoogleSearch Images Maps Play YouTube News Gmail Drive More »Web History | Settings | Sign in\\xa0Advanced searchGoogle offered in:  हिन्दी বাংলা తెలుగు मराठी தமிழ் ગુજરાતી ಕನ್ನಡ മലയാളം ਪੰਜਾਬੀ AdvertisingBusiness SolutionsAbout GoogleGoogle.co.in© 2025 - Privacy - Terms')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_loader = WebBaseLoader('https://www.google.com')\n",
    "web_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "617f8826-a8d0-4731-9d55-0eef1f3a60ff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.indiatoday.in/', 'title': 'Latest News, Breaking News Today - Entertainment, Cricket, Business, Politics - India Today', 'description': 'Check out the latest news from India and around the world. Latest India news on Bollywood, Politics, Business, Cricket, Technology and Travel.', 'language': 'en'}, page_content='Latest News, Breaking News Today - Entertainment, Cricket, Business, Politics - India Today India TodayAaj TakGNTTVLallantopBusiness TodayBanglaMalayalamNortheastBT BazaarHarper\\'s BazaarSports TakCrime TakAstro TakGamingBrides TodayCosmopolitanKisan TakIshq FMIndia Today HindiReader’s DigestIndia TodayAaj TakGNTTVLallantopBusiness TodayBanglaMalayalamNortheastBT BazaarHarper\\'s BazaarSports TakMagazineLive TVSearchSEARCHSIGN INEdition ININUSHome TVLive TVPrimetimeMagazineLatest EditionInsightBest CollegesElection HubElectionsBihar AssemblyBihar ConstituenciesBihar Poll ScheduleGround ReportLife+StyleIndiaSouthGlobalAll World NewsUS NewsCanada NewsUK NewsChina NewsIndians AbroadBusinessAll SportsWomen\\'s World CupWomen\\'s World Cup ScheduleWomen\\'s World Cup Points TableTennisCricketFootballSports TodayTechnologyShowbuzzEntertainmentBollywoodHollywoodTelevisionOTTLatest ReviewsNewspresso SpecialsPodcastsSunday SpecialHistory of ItNewsMoDIUOpinionVideosShort VideosFact Check Other NewsEducationIt\\'s ViralScienceHealthAutoLaw TodayEnvironmentCitiesWeatherWeb StoriesHoroscopesDownload AppFollow Us On:   Green CrackersPankaj DheerKBC 17Delhi AQIPrashant KishorNitish KumarBihar Election 2025EPFODonald TrumpPM ModiWomen\\'s World CupBihar Election ConstituenciesRahul GandhiShare MarketBigg BossAdvertisement   News Fresh violence along Afghan border leaves a dozen dead. Pak dials Qatar, SaudiAfghanistan-Pakistan conflict: With the conflict showing no signs of slowing down and Afghanistan denying entry to Pakistani ministers for talks, Islamabad has asked Qatar and Saudi Arabia to act as mediators.India NewsSex abuse, gangsters, bribes: Twin cop suicides expose rot in Haryana PoliceHaryana Cop Suicide: The saga, which started with the suicide of senior IPS officer Y Puran Kumar, took a new twist this week after an assistant sub-inspector (ASI) allegedly shot himself and blamed the former of corruption.advertisementGlobalFBI\\'s Ashely Tellis Files: Manila envelope, gift bags, dinners with the ChineseCourt records show Ashley Tellis\\'s alleged trail of classified material. Surveillance video reportedly captured him leaving both the State Department and a Defense Department facility after printing reams of restricted files. The FBI also tracked his private meetings with Chinese officials.TelevisionActor Pankaj Dheer, the iconic Karn of Mahabharat, dies after cancer battleVeteran actor Pankaj Dheer, best known for playing Karna in BR Chopra\\'s iconic \\'Mahabharat\\', died on October 15. The actor had reportedly been battling cancer, which relapsed a few months ago despite undergoing major surgery.Your daily capsule for quick news Actor Pankaj Dheer, the iconic Karn of Mahabharat, dies after cancer battleMore >> \\n\\n\\nGlobalPakistan\\'s Lal Masjid momentWill the crackdown on the Tehreek-e-Labbaik spark off a new wave of violence? Assembly ElectionsJDU names 57 candidates in 1st Bihar list, snipes seats eyed by Chirag PaswanNitish Kumar\\'s JDU has released first candidate list, with 57 names, for the upcoming Bihar assembly elections. India NewsWhy Ashley Tellis arrest by FBI has sparked political war in IndiaThe arrest of Indian-American geopolitical strategist Ashley Tellis has ignited a fierce political battle in India. He is charged with unlawfully retaining classified US defence documents and meeting with Chinese officials. The BJP alleged that he was part of the \"anti-India\" force and \"celebrated by the Opposition\". advertisementIndia NewsTamil Nadu to table bill seeking ban on Hindi hoardings, movies, songs: SourcesTamil Nadu plans to table a bill today banning Hindi hoardings, boards, movies, and songs, said sources. Officials stressed the move will comply with the Constitution. BJP called it absurd and a political diversion.\\r\\n TRENDING TOPICSTarot Card Predictions October 15Love HoroscopeHoroscope TodayDhanteras 2025 DateWeekly Love HoroscopeChinese Weekly HoroscopeWeekly Numerology HoroscopeWeekly Horoscope Chinese Monthly HoroscopeWomen\\'s World CupWomen\\'s World CupWomen\\'s World Cup: India fined for slow over-rate against AustraliaENGW vs PAKW Live: Sadia Iqbal wrecks England after Fatima Sana shinesIndia women\\'s team visits Mahakaleshwar Temple after twin World Cup lossesWomen\\'s World Cup: Colombo rain keeps SL winless, adds to New Zealand\\'s woesIND vs AUS: How India blunted out even before Australia clashMatch ScheduleTeams Player StatsMatch Report LIVE SCORES \\n\\nWomen\\'s World Cup 2025 Points table \\nTeamsPLDWINNRRPTSAUS-W43+1.3537ENG-W33+1.8646SA-W43-0.6186IND-W42+0.6824\\nView Full Points Table\\nNewsVideosVisualsCelebritiesSunjay Kapur\\'s wife Priya shares tribute on his birth anniversary, talks legacyPriya Sachdev Kapur penned a heartfelt tribute to her late husband, Sunjay Kapur, on his 54th birth anniversary, recalling his legacy as a devoted family man. Sharing a touching video and verses from the Bhagavad Gita, Priya remembered Sunjay\\'s kindness, strength, and enduring love, saying, \"Some souls don\\'t depart; they expand.\"Global\\'Give Shehbaz Sharif a Nobel for bootlicking\\': Pak PM roasted for Trump flatteryAt the Gaza Summit in Egypt, Shehbaz Sharif may have impressed Trump with his effusive praise, but the Pakistan PM also earned a flood of online mockery and the \\'bootlicker\\' tag. India NewsAshley Tellis: From helping broker civil nuclear deal to rabid India detractorThe arrest of US foreign policy expert Ashley Tellis marks a stunning fall for a man once hailed as one of the key architects of the India-US nuclear deal. In recent years, Tellis has been one of the sharpest critics of the Narendra Modi-led government. EatThe tragic story of the brilliant Soan PapdiSohan Papdi, once a royal Indian sweet, has become a Diwali meme. This Diwali, it seeks a revival as a symbol of heritage and tradition. advertisementLATEST STORIESBengaluru man shares text from LinkedIn user asking him to be a proxy in interviews5 easy DIY body scrubs for glowing skin with kitchen ingredientsFresh violence along Afghan border leaves a dozen dead. Pak dials Qatar, SaudiHigh Court declares IAS Officer Anilkumar Pawar\\'s ED arrest illegal,\\xa0orders\\xa0releaseDoctor claims fundamental right to use WhatsApp, court says no, try Arattai insteadBengaluru doctor arrested for wife\\'s murder using anaesthetic drug PropofolBeyond the numbers: The untold costs of India\\'s ethanol breakthroughBreathing trouble already? Diwali smog could make it worse. Here\\'s how to cope7 simple skin care tips to stay glowing this festive seasonSensex ends 575 points higher, Nifty above 25,300; Bajaj Finance up 4%The LowdownBeyond the numbers: The untold costs of India\\'s ethanol breakthroughIndia\\'s ethanol blending program achieved 20% blending in petrol five years ahead of schedule, driving major foreign exchange savings, farmer payouts, and emission cuts, while facing challenges around water use, food security, and vehicle impacts.Car NewsNew Hyundai Venue spotted and it looks drastically differentThe next-generation Hyundai Venue has been revealed undisguised, showcasing a bold new design, upgraded tech-rich interior, and advanced features ahead of its highly anticipated India debut next month. NewsAmazon plans major layoffs, up to 15% of HR staff and more could be firedAmazon is preparing to cut up to 15 per cent of staff in its HR division as part of a broader AI-driven restructuring. While the exact number of jobs affected is unknown, the Fortune report says other departments are also in the radar.  FeaturephiliaKBC\\'s viral kid and the six-pocket syndrome: A mirror to modern parentingA KBC contestant\\'s confident interruption has ignited a debate on modern parenting styles in India. Experts link this behaviour to \\'Six-Pocket Syndrome,\\' highlighting the risks of overindulgence and entitlement among children. CelebritiesBreaking the beat: When dancers get the Bollywood act rightThey started with rhythm in their veins, not scripts in their hands. Today, they\\'re proving that the dance floor can be a gateway to stardom. The story explores how stars like Raghav Juyal, Nora Fatehi, and others have emerged as global icons.  StandpointPakistan\\'s youth swipe right and conservatives lose sleep over Lazawal Ishq\\'Lazawal Ishq\\', Pakistan\\'s first online dating show, has sparked a culture clash, pitting a modern, digital generation against old-school morality. GlobalBody returned by Hamas does not match any of the hostages, claims IsraelOn Tuesday, four bodies were handed over to Israel after the Jewish nation threatened to reduce humanitarian aid into Gaza for what it called the militant group\\'s violation of its agreement to transfer remains under the US-brokered ceasefire deal reached last week. Madhya PradeshVideo: Accused of being anti-Sanatana, Muslim woman cop shouts Jai Shri RamA disagreement between police officer Hina Khan and lawyer Anil Mishra in Gwalior over a religious event at a Hanuman temple led to slogans, accusations, and a street protest. Trending NewsXiaomi electric car bursts into flames in China, driver burnt alive insideA Xiaomi electric car caught fire after a sudden power failure, trapping the driver inside and burning him alive. The incident has raised urgent questions about electric vehicle safety and emergency escape features.Women\\'s World CupWomen\\'s World Cup: India fined for slow over-rate against AustraliaIndia have been fined five percent of their match fee for maintaining a slow over-rate during their ICC Women\\'s World Cup group-stage match against Australia in Visakhapatnam on Sunday (October 12). Live blogLive: RJD\\'s Tejashwi Yadav files nomination from Raghopur amid seat-sharing dramaAs the deadline for nominations nears, the NDA\\'s seat-sharing arithmetic appears increasingly strained, with public unity masking deep internal dissatisfaction. JDU releases the first candidate list today with 57 names. The BJP on Tuesday announced its first list of 71 candidates, including Deputy Chief Ministers Samrat Choudhary and Vijay Sinha. The 243-member Bihar Assembly polls are scheduled for November 6 and 11, with counting on November 14. Follow indiatoday.in for the latest updates. US NewsMexican cartels enact Narcos, put bounties on ICE, border officers in ChicagoMexican drug cartels are enacting the Netflix web series \\'Narcos\\' in real life. They are offering bounties of up to $50,000 to Chicago gangs to kill senior US immigration officials, according to the Department of Homeland Security. Homeland Security Secretary Kristi Noem condemned the threats, calling them an \"organised campaign of terror\" and vowed that agents would not back down. India Today PodcastsNews at 215th October: Bus Tragedy in Rajasthan Kills 21, India-Mongolia Sign Pacts Elevating Ties & JD(U) Releases Candidate List6:41News at 7October 14- BJP Unveils First List for Bihar Polls; Google Announces $15 Billion AI Hub in Visakhapatnam; Delhi CM Announces Rs110 Billion Water Bill Waiver Till 20266:52Connecting RodTales from the Road: Adventures, Mishaps and Madness Behind the Wheel | Connecting Rod | Episode 2052:25News at 7October 13: Canada FM in India amid thawing ties; US Envoy meets PM Modi & Bypolls announced for 5 assembly seats7:17NewsOctober 13- Hamas begins releasing Israeli hostages; Modi to skip Gaza Summit; Lalu Yadav, family to face trial in \\'hotel\\' scam6:38UnPolitics with PreetiPushkar Singh Dhami Exclusive Podcast with Preeti Choudhry | UnPolitics | Ep 1332:46News at 7October 10: Nobel Peace Prize for Machado; Afghan FM vows no \\'misuse of soil\\'; Meghalaya CM rejects Manipur division6:24Nothing But The TruthWho Will Win Bihar Polls? 10 X Factors | Nothing But The Truth S2 | Ep 11052:49ANCHORSSHOWSRajdeep SardesaiGaurav C SawantPreeti ChoudhryAkshita NandagopalSuyesha SavantMonday to Friday at 09:00 pmMonday to Friday at 08:00 pmMonday to Friday at 10:00 pmMonday to Friday at 07:00 pmMonday to Friday at 05:00 pmMonday to Friday at 06:00 pm\\n\\nadvertisement\\n\\n\\n \\n\\n\\n \\n\\n\\n\\n\\n\\nWHAT’S HAPPENING IN TV\\n\\n\\n Magazine Election Hub\\n\\n\\n\\n\\n\\n\\n\\n\\n Ground Report Life+Style India South\\n\\n GlobalMoreVideosTrendingNews TodayTo The Point5ive LiveIndia First00:22:29MarketQ2 earnings lift IT stocks, but should investors join the rally?00:04:07GlobalDurand Line on Fire: Pakistan & Taliban in Fierce Border Clash, Heavy Weaponry Used00:02:58India NewsNDA Rift Explodes In Bihar: JDU Objects To Giving Stronghold Seats Like Sonbarsa, Rajgir To Chirag00:01:08India NewsIMF Boosts India\\'s Growth Forecast To 6.6% Despite Trump\\'s Tariff PressureView All\\n\\nadvertisement\\n\\n \\n\\n\\n \\n\\n\\n\\nRegional cinemaVishal takes over Magudam after creative differences with director Ravi ArasuActor Vishal took over \\'Magudam\\' as a director amid creative differences with Ravi Arasu. The film features Dushara Vijayan, Anjali, and Yogi Babu and is produced by Super Good Films. HollywoodLaurence Fishburne wants to play Professor X in Marvel\\'s new X-Men instalmentActor Laurence Fishburne has expressed his interest in portraying Professor Charles Xavier in the next \\'X-Men\\' film. His proposal comes amid speculations about the future of the Marvel Cinematic Universe superhero franchise. \\nAdvertisement\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n \\nTrending Videos00:22:29MarketQ2 earnings lift IT stocks, but should investors join the rally?00:04:07GlobalDurand Line on Fire: Pakistan & Taliban in Fierce Border Clash, Heavy Weaponry Used00:02:58India NewsNDA Rift Explodes In Bihar: JDU Objects To Giving Stronghold Seats Like Sonbarsa, Rajgir To Chirag00:01:08India NewsIMF Boosts India\\'s Growth Forecast To 6.6% Despite Trump\\'s Tariff Pressure00:01:10NewsMoWhere is the Asia Cup & how can India get it back?\\nAdvertisement\\n\\n \\n\\n\\n      \\n\\n\\xa0\\n\\n  Follow Us On:                      Advertisement\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0PUBLICATIONSIndia TodayBusiness TodayIndia Today-HindiTIME TELEVISIONIndia Today TVAaj Tak Good News TodayEVENTSAgenda AajTakIndia Today ConclaveSahitya AajTak RADIOIshq FMAajTak RadioGAMINGIndia Today Gaming World Esports CupUSEFUL LINKSPress ReleaseSitemapNewsNewsletter Privacy PolicyCorrection PolicyLMIL DocumentsPRINTINGThomson PressWELFARECare TodayDISTRIBUTIONRate Card    SYNDICATIONS  Headlines Today        WEBSITES  India Today India Today Malayalam India Today NE Business Today DailyO AajTak Lallantop Bangla GNTTV iChowk Reader’s Digest Cosmopolitan    EDUCATION  Vasant Valley Best Colleges Best Universities          Download App              ABOUT US CONTACT US TERMS AND CONDITIONS ARCHIVES    Copyright © 2025 Living Media India Limited. For reprint rights: Syndications Today ')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL=\"https://www.indiatoday.in/\"\n",
    "web_loader = WebBaseLoader(URL)\n",
    "web_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba0b70cb-dd35-4ffe-b33a-474edbef084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "URL=\"https://www.google.com\"\n",
    "india_today_web = requests.get(URL).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d5a4ce9-2742-42f9-8c91-f2a1eab24184",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testindia.html','w') as wobj:\n",
    "    wobj.write(india_today_web.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9819f662-0082-4add-9ac7-59e294426de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.wikipedia.WikipediaLoader at 0x1744b0fb230>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "WikipediaLoader(query='what is langchain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5dfbfb92-f78c-4cf6-8fe8-7f264765255a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'title': 'Vector database', 'summary': 'A vector database, vector store or vector search engine is a database that uses the vector space model to store vectors (fixed-length lists of numbers) along with other data items. Vector databases typically implement one or more approximate nearest neighbor algorithms, so that one can search the database with a query vector to retrieve the closest matching database records.\\nVectors are mathematical representations of data in a high-dimensional space. In this space, each dimension corresponds to a feature of the data, with the number of dimensions ranging from a few hundred to tens of thousands, depending on the complexity of the data being represented. A vector\\'s position in this space represents its characteristics. Words, phrases, or entire documents, as well as images, audio, and other types of data, can all be vectorized.\\nThese feature vectors may be computed from the raw data using machine learning methods such as feature extraction algorithms, word embeddings or deep learning networks. The goal is that semantically similar data items receive feature vectors close to each other.\\nVector databases can be used for similarity search, semantic search, multi-modal search, recommendations engines, large language models (LLMs), object detection,  etc.\\nVector databases are also often used to implement retrieval-augmented generation (RAG), a method to improve domain-specific responses of large language models. The retrieval component of a RAG can be any search system, but is most often implemented as a vector database. Text documents describing the domain of interest are collected, and for each document or document section, a feature vector (known as an \"embedding\") is computed, typically using a deep learning network, and stored in a vector database. Given a user prompt, the feature vector of the prompt is computed, and the database is queried to retrieve the most relevant documents. These are then automatically added into the context window of the large language model, and the large language model proceeds to create a response to the prompt given this context.', 'source': 'https://en.wikipedia.org/wiki/Vector_database'}, page_content='A vector database, vector store or vector search engine is a database that uses the vector space model to store vectors (fixed-length lists of numbers) along with other data items. Vector databases typically implement one or more approximate nearest neighbor algorithms, so that one can search the database with a query vector to retrieve the closest matching database records.\\nVectors are mathematical representations of data in a high-dimensional space. In this space, each dimension corresponds to a feature of the data, with the number of dimensions ranging from a few hundred to tens of thousands, depending on the complexity of the data being represented. A vector\\'s position in this space represents its characteristics. Words, phrases, or entire documents, as well as images, audio, and other types of data, can all be vectorized.\\nThese feature vectors may be computed from the raw data using machine learning methods such as feature extraction algorithms, word embeddings or deep learning networks. The goal is that semantically similar data items receive feature vectors close to each other.\\nVector databases can be used for similarity search, semantic search, multi-modal search, recommendations engines, large language models (LLMs), object detection,  etc.\\nVector databases are also often used to implement retrieval-augmented generation (RAG), a method to improve domain-specific responses of large language models. The retrieval component of a RAG can be any search system, but is most often implemented as a vector database. Text documents describing the domain of interest are collected, and for each document or document section, a feature vector (known as an \"embedding\") is computed, typically using a deep learning network, and stored in a vector database. Given a user prompt, the feature vector of the prompt is computed, and the database is queried to retrieve the most relevant documents. These are then automatically added into the context window of the large language model, and the large language model proceeds to create a response to the prompt given this context.\\n\\n\\n== Techniques ==\\nThe most important techniques for similarity search on high-dimensional vectors include:\\n\\nHierarchical Navigable Small World (HNSW) graphs\\nLocality-sensitive Hashing (LSH) and Sketching\\nProduct Quantization (PQ)\\nInverted Files\\nand combinations of these techniques.\\nIn recent benchmarks, HNSW-based implementations have been among the best performers. Conferences such as the International Conference on Similarity Search and Applications, SISAP and the Conference on Neural Information Processing Systems (NeurIPS) host competitions on vector search in large databases.\\n\\n\\n== Implementations ==\\n\\n\\n== See also ==\\nCurse of dimensionality – Difficulties arising when analyzing data with many aspects (\"dimensions\")\\nMachine learning – Study of algorithms that improve automatically through experience\\nNearest neighbor search – Optimization problem in computer science\\nRecommender system – System to predict users\\' preferences\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nSawers, Paul (2024-04-20). \"Why vector databases are having a moment as the AI hype cycle peaks\". TechCrunch. Retrieved 2024-04-23.'), Document(metadata={'title': 'Retrieval-augmented generation', 'summary': 'Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM\\'s pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\\nRAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don\\'t exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\\nRAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\\nThe term RAG was first introduced in a 2020 research paper.', 'source': 'https://en.wikipedia.org/wiki/Retrieval-augmented_generation'}, page_content='Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM\\'s pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\\nRAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don\\'t exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\\nRAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\\nThe term RAG was first introduced in a 2020 research paper.\\n\\n\\n== RAG and LLM Limitations ==\\nLLMs can provide incorrect information. For example, when Google first demonstrated its LLM tool \"Google Bard\", the LLM provided incorrect information about the James Webb Space Telescope. This error contributed to a $100 billion decline in the company’s stock value. RAG is used to prevent these errors, but it does not solve all the problems. For example, LLMs can generate misinformation even when pulling from factually correct sources if they misinterpret the context. MIT Technology Review gives the example of an AI-generated response stating, \"The United States has had one Muslim president, Barack Hussein Obama.\" The model retrieved this from an academic book rhetorically titled Barack Hussein Obama: America’s First Muslim President? The LLM did not \"know\" or \"understand\" the context of the title, generating a false statement.\\nLLMs with RAG are programmed to prioritize new information. This technique has been called \"prompt stuffing.\" Without prompt stuffing, the LLM\\'s input is generated by a user; with prompt stuffing, additional relevant context is added to this input to guide the model’s response. This approach provides the LLM with key information early in the prompt, encouraging it to prioritize the supplied data over pre-existing training knowledge.\\n\\n\\n== Process ==\\nRetrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating an information-retrieval mechanism that allows models to access and utilize additional data beyond their original training set. Ars Technica notes that \"when new information becomes available, rather than having to retrain the model, all that’s needed is to augment the model’s external knowledge base with the updated information\" (\"augmentation\"). IBM states that \"in the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant\".\\n\\n\\n=== RAG key stages ===\\n\\nTypically, the data to be referenced is converted into LLM embeddings, numerical representations in the form of a large vector space. RAG can be used on unstructured (usually text), semi-structured, or structured data (for example knowledge graphs). These embeddings are then stored in a vector database to allow for document retrieval.\\nGiven a user query, a document retriever is first c'), Document(metadata={'title': 'Model Context Protocol', 'summary': 'The Model Context Protocol (MCP) is an open standard, open-source framework introduced by Anthropic in November 2024 to standardize the way artificial intelligence (AI) systems like large language models (LLMs) integrate and share data with external tools, systems, and data sources. MCP provides a universal interface for reading files, executing functions, and handling contextual prompts. Following its announcement, the protocol was adopted by major AI providers, including OpenAI and Google DeepMind.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Model_Context_Protocol'}, page_content='The Model Context Protocol (MCP) is an open standard, open-source framework introduced by Anthropic in November 2024 to standardize the way artificial intelligence (AI) systems like large language models (LLMs) integrate and share data with external tools, systems, and data sources. MCP provides a universal interface for reading files, executing functions, and handling contextual prompts. Following its announcement, the protocol was adopted by major AI providers, including OpenAI and Google DeepMind.\\n\\n\\n== Background ==\\nThe protocol was announced by Anthropic in November 2024 as an open standard for connecting AI assistants to data systems such as content repositories, business management tools, and development environments. It aims to address the challenge of information silos and legacy systems. Before MCP, developers often had to build custom connectors for each data source or tool, resulting in what Anthropic described as an \"N×M\" data integration problem.\\nEarlier stop-gap approaches—such as OpenAI\\'s 2023 \"function-calling\" API and the ChatGPT plug-in framework—solved similar problems but required vendor-specific connectors. MCP\\'s authors note that the protocol deliberately re-uses the message-flow ideas of the Language Server Protocol (LSP) and is transported over JSON-RPC 2.0. MCP formally specifies stdio and HTTP (optionally with SSE) as its standard transport mechanisms.\\n\\n\\n== Features ==\\nMCP defines a standardized framework for integrating AI systems with external data sources and tools. It includes specifications for data ingestion and transformation, contextual metadata tagging, and AI interoperability across different platforms. The protocol also supports secure, bidirectional connections between data sources and AI-powered tools.\\nMCP enables developers to expose their data via MCP servers or to develop AI applications—referred to as MCP clients—that connect to these servers. Key components of the protocol include a formal protocol specification and software development kits (SDKs), local MCP server support in Claude Desktop applications, and an open-source repository of MCP server implementations.\\n\\n\\n== Applications ==\\nIn the field of natural language data access, MCP enables applications such as AI2SQL to bridge language models with structured databases, allowing plain-language queries.\\nThe protocol is used in AI-assisted software development tools. Integrated development environments (IDEs), coding platforms such as Replit, and code intelligence tools like Sourcegraph have adopted MCP to grant AI coding assistants real-time access to project context.\\n\\n\\n== Implementation ==\\nThe protocol was released with software development kits (SDKs) in programming languages including Python, TypeScript, C# and Java. Anthropic maintains an open-source repository of reference MCP server implementations for popular enterprise systems including Google Drive, Slack, GitHub, Git, Postgres, Puppeteer and Stripe. Developers can create custom MCP servers to connect proprietary systems or specialized data sources to AI systems.\\nThe protocol\\'s open standard allows organizations to build tailored connections while maintaining compatibility with the broader MCP ecosystem. AI systems can then leverage these custom connections to provide domain-specific assistance while respecting data access permissions.\\n\\n\\n== Adoption ==\\nIn March 2025, OpenAI officially adopted the MCP, following a decision to integrate the standard across its products, including the ChatGPT desktop app, OpenAI\\'s Agents SDK, and the Responses API.\\nMCP can be integrated with Microsoft Semantic Kernel, and Azure OpenAI. MCP servers can be deployed to Cloudflare.\\nDemis Hassabis, CEO of Google DeepMind, confirmed in April 2025 MCP support in the upcoming Gemini models and related infrastructure.\\n\\n\\n== Reception ==\\nThe Verge reported that MCP addresses a growing demand for AI agents that are contextually aware and capable of securely pulling from diverse sources. The protocol\\'s rap'), Document(metadata={'title': 'Intelligent agent', 'summary': 'In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. AI textbooks define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\\nA specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods.\\nIntelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.\\nIntelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion. For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior. Similarly, an evolutionary algorithm\\'s behavior is guided by a fitness function.\\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\\nIntelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Intelligent_agent'}, page_content='In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. AI textbooks define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\\nA specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods.\\nIntelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.\\nIntelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion. For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior. Similarly, an evolutionary algorithm\\'s behavior is guided by a fitness function.\\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\\nIntelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".\\n\\n\\n== Intelligent agents as the foundation of AI ==\\n\\nThe concept of intelligent agents provides a foundational lens through which to define and understand artificial intelligence. For instance, the influential textbook Artificial Intelligence: A Modern Approach (Russell & Norvig) describes:\\n\\nAgent: Anything that perceives its environment (using sensors) and acts upon it (using actuators). E.g., a robot with cameras and wheels, or a software program that reads data and makes recommendations.\\nRational Agent: An agent that strives to achieve the *best possible outcome* based on its knowledge and past experiences. \"Best\" is defined by a performance measure – a way of evaluating how well the agent is doing.\\nArtificial Intelligence (as a field): The study and creation of these rational agents.\\nOther researchers and definitions build upon this foundation. Padgham & Winikoff emphasize that intelligent agents should react to changes in their environment in a timely way, proactively pursue goals, and be flexible and robust (able to handle unexpected situations). Some also suggest that ideal agents should be \"rational\" in the economic sense (making optimal choices) and capable of complex reasoning, like having beliefs, desires, and intentions (BDI model). Kaplan and Haenlein offer a similar definition, focusing on a system\\'s ability to understand external data, learn from that data, and use what is learned to achieve goals through flexible adaptation.\\nDefining AI in terms of intelligent agents offers several key advantages:\\n\\nAvoids Philosophical Debates: It sidesteps arguments about whether AI is \"truly\" intelligent or conscious, like those raised by the Turing test or Searle\\'s Chinese Room. It focuses on behavior and goal achievement, not on replicating human thought.\\nObjective Testing: It provides a clear, scientific way to evaluate AI systems. Researchers can compare different approaches by measuring how well they maximize a specific \"')]\n"
     ]
    }
   ],
   "source": [
    "loader = WikipediaLoader(query='what is langchain')\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33e9f607-f85b-4db5-84e9-48f5e86015cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: load the data\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4643c8-0ac5-464e-83b2-63324bd22fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader_Name(args) ->loader_obj.load() ->result //data\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54734603-cc61-4797-b7e3-bde64da01daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_text_splitters.character.CharacterTextSplitter at 0x17445b97380>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: load the data\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader('my_docs.txt')\n",
    "docs = loader.load()\n",
    "\n",
    "# Step 2: split input data into multiple chunks\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "CharacterTextSplitter(chunk_size=100,chunk_overlap=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b561c76-7b04-483b-9fca-7557df67c249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).'),\n",
       " Document(metadata={'source': 'my_docs.txt'}, page_content='LangChain simplifies every stage of the LLM application lifecycle:'),\n",
       " Document(metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\")]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = CharacterTextSplitter(chunk_size=100,chunk_overlap=10)\n",
    "obj.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9314b692-ea37-4e73-83e8-b152e2103335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models'),\n",
       " Document(metadata={'source': 'my_docs.txt'}, page_content='models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n\\nDevelopment:'),\n",
       " Document(metadata={'source': 'my_docs.txt'}, page_content=\"Build your applications using LangChain's open-source components and third-party integrations. Use\"),\n",
       " Document(metadata={'source': 'my_docs.txt'}, page_content='Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop'),\n",
       " Document(metadata={'source': 'my_docs.txt'}, page_content='support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so'),\n",
       " Document(metadata={'source': 'my_docs.txt'}, page_content='so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph'),\n",
       " Document(metadata={'source': 'my_docs.txt'}, page_content='LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = CharacterTextSplitter(chunk_size=100,chunk_overlap=10,separator=' ')\n",
    "obj.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e91bf797-bd2b-47b1-9acd-5fb36bdf9a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg='this is a test document for sample data split into multiple values'\n",
    "len(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1a35772-b5ac-4828-a814-b0bf900e7ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is a',\n",
       " 'a test',\n",
       " 'document',\n",
       " 'for sample',\n",
       " 'data split',\n",
       " 'into',\n",
       " 'multiple',\n",
       " 'values']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = CharacterTextSplitter(chunk_size=10,chunk_overlap=3,separator=' ')\n",
    "# obj.split_documents() # loaded from data_loder object(from file source)\n",
    "obj.split_text(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "10827119-0673-4190-a85f-8facf99809a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is a test document for sample data split into multiple values']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = CharacterTextSplitter(chunk_size=10,chunk_overlap=3)\n",
    "# obj.split_documents() # loaded from data_loder object(from file source)\n",
    "obj.split_text(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de76d6-c94a-48da-a191-1cfa9a149a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: load the data\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader('my_docs.txt')\n",
    "docs = loader.load()\n",
    "\n",
    "# Step 2: split input data into multiple chunks\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "obj = CharacterTextSplitter(chunk_size=100,chunk_overlap=10)\n",
    "obj.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98a1f7-ca75-4837-bc50-ab9803ac9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Embedding\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d318b-b1a5-42e5-9021-e9d602c707e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "llm_obj = OllamaLLM(model='gemma2:2b')\n",
    "result1 = llm_obj.invoke('explain about langchain')\n",
    "## End_user <->llm <-->DB\n",
    "## Vs\n",
    "##\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "OllamaEmbeddings(model='ModelName') ->embedding_object\n",
    "embedding_object.method('data') ->vector_result_list\n",
    "[Docs] -->[splited]-->[embedded] -->[vectors] --->[DataBase(vectorDataBase)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d72af4f-7ec2-470f-89b7-20fb26220567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_17804\\2560455962.py:2: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding_obj = OllamaEmbeddings(model='gemma2:2b')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.997397780418396"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embedding_obj = OllamaEmbeddings(model='gemma2:2b')\n",
    "r = embedding_obj.embed_query('Hello')\n",
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9dafbc18-d134-4a76-9d5e-0f6d2af6b1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7023508548736572"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = embedding_obj.embed_query('Hi')\n",
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b2cf8f48-49f4-4d77-9aa8-3918cc994a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.918656349182129"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = embedding_obj.embed_query('fruits')\n",
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f097bd69-747c-4139-9928-781e4df9aad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.398831367492676"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = embedding_obj.embed_query('apple')\n",
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d5a08fe6-94f6-484d-b624-e0cbc94a5d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.900454521179199"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = embedding_obj.embed_query('banana')\n",
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c2443-6f22-49b4-a447-f828d1dc5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple juice  -> [1.34,0.23...]\n",
    "dosa bfst    -> [3.12,...]\n",
    "banana fruits -> [1.35,..]\n",
    "text book     -> [8.233]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1419f1ad-4a5a-49ba-a54e-59fb9daaec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAISS\n",
    " - vectorDB\n",
    " - in memory based - processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f74c3f-42da-4143-b2ef-79fa8a146f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step-1 - load the data from datasource \n",
    "\n",
    "## Step-2 - split the data\n",
    "\n",
    "## Step-3 - Embedding\n",
    "\n",
    "## Step-4 - Stores to DB\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(docs,embedding_object)\n",
    "\n",
    "## Step-5 - do query - similarity search\n",
    "result = db.similarity_search('User Input Query')\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a7ca774-bb17-40cb-8f6d-57acc3944f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cbf2067-9a4b-4f84-8336-d49760b4e7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_456\\2147717799.py:10: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model='gemma2:2b')\n"
     ]
    }
   ],
   "source": [
    "## Step-1 - load the data from datasource \n",
    "loader = TextLoader('my_docs.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "## Step-2 - Split \n",
    "text_splitter = CharacterTextSplitter(chunk_size=100,chunk_overlap=10)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "## Step-3 - Embeddings\n",
    "embeddings = OllamaEmbeddings(model='gemma2:2b')\n",
    "\n",
    "## Step-4 \n",
    "db = FAISS.from_documents(docs,embeddings)\n",
    "\n",
    "## Step-5 \n",
    "r = db.similarity_search('what is langchain?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868814d3-6e5f-4fc2-af8f-2060fe1e6da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='7879531e-566f-415d-ac43-6460ff43bf79', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).'), Document(id='1f81fe67-f0af-466b-9567-7a857365c02c', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\"), Document(id='d4b774f4-eb42-494a-857c-cf214fcb0594', metadata={'source': 'my_docs.txt'}, page_content='LangChain simplifies every stage of the LLM application lifecycle:')]\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a4e9276-4c1c-4db2-ac34-88802b793716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\n",
      "Productionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\n",
      "Deployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\n"
     ]
    }
   ],
   "source": [
    "print(r[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5608c994-d0fb-4591-b222-dab391d22f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############  End of Day1 Session ###########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
