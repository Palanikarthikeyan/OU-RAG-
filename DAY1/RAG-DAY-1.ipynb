{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99818893-ea79-420b-8f74-5ac7fcb03bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Python\n",
    " - class and object\n",
    " python.org ----> python -- develop python code - pip install numpy pandas matplotlib scikilearn ..\n",
    " ------------------------------\n",
    "anaconda python/download --> python + DE+ML+DL libs \n",
    "--------------------------------\n",
    "\n",
    "CProgram ==> CPython --- GIL is enabled (Global Interpreter Lock) //mutext -lock and unlock\n",
    "Vs\n",
    "Ipython - GIL is disabled\n",
    "Jypthon - GIL is disabled \n",
    "-- create threads + synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f246fd9-c3fb-4691-b65d-c73dcd56e116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x7ffb5ce2b4c8\n",
      "0x7ffb5ce2b4c8\n",
      "0x7ffb5ce2b4c8\n",
      "0x7ffb5ce2b4c8\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "j = 4+6\n",
    "k = 3+7\n",
    "print(hex(id(10)))\n",
    "print(hex(id(i)))\n",
    "print(hex(id(j)))\n",
    "print(hex(id(k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c079ea75-795e-4794-b162-9abb144d5f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1\n"
     ]
    }
   ],
   "source": [
    "def display(a):\n",
    "    print(a)\n",
    "\n",
    "display('data1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d950dff1-e886-4825-ad02-75727a85d3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.box"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class box:\n",
    "    def display():\n",
    "        print('display block')\n",
    "\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d3ed783-a9a4-4bbc-9745-562406370776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.box at 0x1cd1f22cc20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b88e9c8a-2029-4dd3-88c3-3f491d84e4a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "box.display() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m obj1 \u001b[38;5;241m=\u001b[39m box()\n\u001b[1;32m----> 2\u001b[0m obj1\u001b[38;5;241m.\u001b[39mdisplay()\n",
      "\u001b[1;31mTypeError\u001b[0m: box.display() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "obj1 = box()\n",
    "obj1.display() --> display(obj1)\n",
    "TypeError: box.display() takes 0 positional arguments but 1 was given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3e8ae58-15c3-4918-8578-ade6b27e548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self= <__main__.box object at 0x000001CD217723C0>\n",
      "self= <__main__.box object at 0x000001CD2177D950>\n"
     ]
    }
   ],
   "source": [
    "class box:\n",
    "    def display(self):\n",
    "        print(\"self=\",self)\n",
    "\n",
    "obj1 = box()\n",
    "obj1.display() # display(obj1)\n",
    "\n",
    "obj2 = box()\n",
    "obj2.display() # display(obj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844069e-6491-4107-8b22-67cf971be731",
   "metadata": {},
   "outputs": [],
   "source": [
    "file: ab.py                                            file: p1.py\n",
    "----------------                                    ================\n",
    "def embedding():                                   import ab\n",
    "    return [1.0,2.0]                               ab.embedding()\n",
    "\n",
    "class OpenAI:                                      myobj = ab.OpenAI()\n",
    "    def vector_emb(self):                          myobj.vector_emb() \n",
    "        ...                                            file: p2.py\n",
    "        return [10.24,30.3,344.55]                    ==============\n",
    "---------------------------------------                from ab import embedding,OpenAI\n",
    "                                                       result = embedding()\n",
    "                                                       myobj = OpenAI()\n",
    "                                                       myobj.vector_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d19a27-edad-468d-9c73-e55aabb0595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP Terms\n",
    "==========\n",
    "    1. Corpus - Paragraph \n",
    "    2. Documents - Sentence \n",
    "    3. Vocabulary - unique words\n",
    "\n",
    "Hello Good morning had your food  ----------------->  French /..../.../... (Google Translate) \n",
    "\n",
    "\"I like to drink apple juice\n",
    "my friend likes to drink mango juice\" //Corpus \n",
    "    |\n",
    "    I like to drink apple juice - Doc1\n",
    "    my friend likes to drink mango juice - Doc2\n",
    "    |\n",
    "    i like to drink apple juice   - Doc1\n",
    "    my friend likes to drink mango juice - Doc2\n",
    "    |\n",
    "    i like to drink apple juice my friend likes mango //unique words \n",
    "\n",
    "Step 1: Tokenization = NLP Text processing \n",
    "Step 2: Text processing - BOW,ngrams\n",
    "Step 3: Text processing -  word2vector - Deeplearning\n",
    "Step 4: Neural network\n",
    "Step 5: Word embedding\n",
    "Step 6: Transformer \n",
    "Step 7: BERT \n",
    "\n",
    "[DataSet] ---> [Text processing]->[Stemming/lemmatization and stop words] ->[vectors]\n",
    "               =========================================================\n",
    "\n",
    "Stemming - reducing root word => eating Vs eat\n",
    "                                 history =>histori\n",
    "lemmitazation - lemma \n",
    "    |->Parts of Speech (POS)\n",
    "\n",
    "One Hot Encoding\n",
    "===================\n",
    "    D1 - the food good bad \n",
    "\n",
    "    the -  1 0 0 0\n",
    "    food - 0 1 0 0\n",
    "    good - 0 0 1 0\n",
    "    bad  - 0 0 0 1\n",
    "\n",
    "    BOW + frequency\n",
    "\n",
    "    ngrams \n",
    "    n=1\n",
    "    the => 1 0 0 0 \n",
    "    food => 0 1 0 0\n",
    "\n",
    "    n=2\n",
    "    the food => 1 0 0\n",
    "\n",
    "    n=3\n",
    "    the food good => 1 0\n",
    "\n",
    "    n=4\n",
    "    the food good bad => 1\n",
    "    ...                  ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67d78951-06b8-4783-9c62-4f71866924a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97947b-9c02-43f4-be87-c3745c84c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "662fc306-bdb2-4e18-9e55-7efa6be9a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '''Hello welcome,to Gen AI NLP Lecture\n",
    "Please do activity ! to become expert in NLP'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "010cda44-8f0c-468d-ba50-4500fa882ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello welcome,to gen ai nlp lecture\\nplease do activity ! to become expert in nlp'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dd323f4-edc2-47e9-8d36-2b62ad153053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello welcome,to gen ai nlp lecture\\nplease do activity ! to become expert in nlp'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = corpus.lower()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53269ab-ec08-4f7e-9e8a-fac56d016493",
   "metadata": {},
   "outputs": [],
   "source": [
    "LookupError\n",
    "...\n",
    ">>> import nltk\n",
    ">>> nltk.download('<tokenName>')(ex: nltk.download('punkt_tab'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8788250-6aaa-4c1a-aa8d-63c9d3d1d91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function nltk.tokenize.word_tokenize(text, language='english', preserve_line=False)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78a45fed-cb25-45d2-8ca1-13b5878f70df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'gen',\n",
       " 'ai',\n",
       " 'nlp',\n",
       " 'lecture',\n",
       " 'please',\n",
       " 'do',\n",
       " 'activity',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'nlp']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1462832-7f43-4ebf-9239-cf441cf4c6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello welcome,to gen ai nlp lecture\\nplease do activity !',\n",
       " 'to become expert in nlp']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34f77925-4ff3-4b74-abf6-db7bdfc8f350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello arun,how are you doing?',\n",
       " \"ok. i am learning nlp activities's code ab'c data1.data2\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg=\"hello arun,how are you doing? ok. i am learning nlp activities's code ab'c data1.data2\"\n",
    "sent_tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c5a71bf-3f3d-4230-962b-500916b086ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'arun',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'ok.',\n",
       " 'i',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'nlp',\n",
       " 'activities',\n",
       " \"'s\",\n",
       " 'code',\n",
       " 'ab',\n",
       " \"'\",\n",
       " 'c',\n",
       " 'data1.data2']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0e4eaee-2cee-48ce-8579-f0bbba778e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'arun',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'ok',\n",
       " '.',\n",
       " 'i',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'nlp',\n",
       " 'activities',\n",
       " \"'\",\n",
       " 's',\n",
       " 'code',\n",
       " 'ab',\n",
       " \"'\",\n",
       " 'c',\n",
       " 'data1',\n",
       " '.',\n",
       " 'data2']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6428b368-9124-4f81-8145-0f40585584cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming - stem - reduce root word\n",
    "from nltk.stem import PorterStemmer\n",
    "obj_stemming = PorterStemmer()\n",
    "obj_stemming.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e51db10-88a9-4f2a-8455-456c31a2f379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_stemming.stem('history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16a741cf-11f4-43a9-a550-f1d6bb098c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_stemming.stem('Congratulations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc07c744-87ab-4357-b63d-860cffb00f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"eating\",\"eats\",\"eatern\",\"writing\",\"writes\",\n",
    "         \"programming\",\"program\",\"history\",\n",
    "         \"finally\",\"finalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae4abfa5-6f0d-422e-8f03-925bee3130c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating =====> eat\n",
      "eats =====> eat\n",
      "eatern =====> eatern\n",
      "writing =====> write\n",
      "writes =====> write\n",
      "programming =====> program\n",
      "program =====> program\n",
      "history =====> histori\n",
      "finally =====> final\n",
      "finalized =====> final\n"
     ]
    }
   ],
   "source": [
    "'''each and every word apply to stemming word'''\n",
    "\n",
    "for var in words:\n",
    "    print(var+\" =====> \"+obj_stemming.stem(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cad371b-36d3-464f-8d10-0a7def446279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regx Stemming\n",
    "from nltk.stem import RegexpStemmer\n",
    "# stem_obj = RegexpStemmer('RegxPattern') <= Constructor\n",
    "\n",
    "reg_obj = RegexpStemmer('ing$|s$')\n",
    "reg_obj.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f19f8c1c-bc25-4a68-8faa-daf38cea0d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'service'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_obj.stem('service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93aab2f0-86f4-412e-9871-1e43ffb955f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'service'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_obj.stem('services')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0311e929-0264-4cce-914a-3abecc814a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'systemd'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_obj.stem('systemd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04d41d58-d99f-49e4-aec1-2599ee8c0c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stem = SnowballStemmer('english')\n",
    "snowball_stem.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30de6c17-e8da-4914-a87a-f9ab7eb7fa75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(SnowballStemmer)\n",
    "snowball_stem.stem('history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3724c971-5283-42c1-94d5-809cc7c90a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'fair')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_stemming = PorterStemmer()\n",
    "snowball_stem = SnowballStemmer('english')\n",
    "\n",
    "obj_stemming.stem(\"fairly\"),snowball_stem.stem(\"fairly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46b28331-90b5-420b-aa66-90cceee85d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairly'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "lemma.lemmatize('fairly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba818853-cdc0-47d7-97bd-e68d86b3167d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eating'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1689369-681b-4db2-9392-fd61bcdf46aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('eating',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "448ecae9-61de-4fb1-bd61-8c35a99cd194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairly'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('fairly',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "284ce031-55b7-4d3a-a814-cd922e33b5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating =====> eat\n",
      "eats =====> eat\n",
      "eatern =====> eatern\n",
      "writing =====> write\n",
      "writes =====> write\n",
      "programming =====> program\n",
      "program =====> program\n",
      "history =====> histori\n",
      "finally =====> final\n",
      "finalized =====> final\n"
     ]
    }
   ],
   "source": [
    "words = [\"eating\",\"eats\",\"eatern\",\"writing\",\"writes\",\n",
    "         \"programming\",\"program\",\"history\",\n",
    "         \"finally\",\"finalized\"]\n",
    "\n",
    "for var in words:\n",
    "    print(var+\" =====> \"+obj_stemming.stem(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5822b979-d45f-4fb4-897e-8780efcfdab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating----->eat\n",
      "eats----->eat\n",
      "eatern----->eatern\n",
      "writing----->write\n",
      "writes----->write\n",
      "programming----->program\n",
      "program----->program\n",
      "history----->history\n",
      "finally----->finally\n",
      "finalized----->finalize\n"
     ]
    }
   ],
   "source": [
    "for var in words:\n",
    "    print(var+'----->'+lemma.lemmatize(var,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ae93eeb-e0e7-479c-a406-d86257bf2580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\karth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ae8d3be-eb9f-469b-a06a-98e64e7da070",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4938a6e4-21d9-4089-a99f-6f7326fa7244",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "224bb0df-2cbd-47f8-a688-49b2690587e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package nltk.corpus in nltk:\n",
      "\n",
      "NAME\n",
      "    nltk.corpus\n",
      "\n",
      "DESCRIPTION\n",
      "    NLTK corpus readers.  The modules in this package provide functions\n",
      "    that can be used to read corpus files in a variety of formats.  These\n",
      "    functions can be used to read both the corpus files that are\n",
      "    distributed in the NLTK corpus package, and corpus files that are part\n",
      "    of external corpora.\n",
      "\n",
      "    Available Corpora\n",
      "    =================\n",
      "\n",
      "    Please see https://www.nltk.org/nltk_data/ for a complete list.\n",
      "    Install corpora using nltk.download().\n",
      "\n",
      "    Corpus Reader Functions\n",
      "    =======================\n",
      "    Each corpus module defines one or more \"corpus reader functions\",\n",
      "    which can be used to read documents from that corpus.  These functions\n",
      "    take an argument, ``item``, which is used to indicate which document\n",
      "    should be read from the corpus:\n",
      "\n",
      "    - If ``item`` is one of the unique identifiers listed in the corpus\n",
      "      module's ``items`` variable, then the corresponding document will\n",
      "      be loaded from the NLTK corpus package.\n",
      "    - If ``item`` is a filename, then that file will be read.\n",
      "\n",
      "    Additionally, corpus reader functions can be given lists of item\n",
      "    names; in which case, they will return a concatenation of the\n",
      "    corresponding documents.\n",
      "\n",
      "    Corpus reader functions are named based on the type of information\n",
      "    they return.  Some common examples, and their return types, are:\n",
      "\n",
      "    - words(): list of str\n",
      "    - sents(): list of (list of str)\n",
      "    - paras(): list of (list of (list of str))\n",
      "    - tagged_words(): list of (str,str) tuple\n",
      "    - tagged_sents(): list of (list of (str,str))\n",
      "    - tagged_paras(): list of (list of (list of (str,str)))\n",
      "    - chunked_sents(): list of (Tree w/ (str,str) leaves)\n",
      "    - parsed_sents(): list of (Tree with str leaves)\n",
      "    - parsed_paras(): list of (list of (Tree with str leaves))\n",
      "    - xml(): A single xml ElementTree\n",
      "    - raw(): unprocessed corpus contents\n",
      "\n",
      "    For example, to read a list of the words in the Brown Corpus, use\n",
      "    ``nltk.corpus.brown.words()``:\n",
      "\n",
      "        >>> from nltk.corpus import brown\n",
      "        >>> print(\", \".join(brown.words())) # doctest: +ELLIPSIS\n",
      "        The, Fulton, County, Grand, Jury, said, ...\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    europarl_raw\n",
      "    reader (package)\n",
      "    util\n",
      "\n",
      "FUNCTIONS\n",
      "    demo()\n",
      "        # FIXME:  override any imported demo from various corpora, see https://github.com/nltk/nltk/issues/2116\n",
      "\n",
      "DATA\n",
      "    __annotations__ = {'abc': <class 'nltk.corpus.reader.plaintext.Plainte...\n",
      "    abc = <PlaintextCorpusReader in '.../corpora/abc' (not loaded yet)>\n",
      "    alpino = <AlpinoCorpusReader in '.../corpora/alpino' (not loaded yet)>\n",
      "    bcp47 = <BCP47CorpusReader in '.../corpora/bcp47' (not loaded yet)>\n",
      "    brown = <CategorizedTaggedCorpusReader in '.../corpora/brown' (not loa...\n",
      "    cess_cat = <BracketParseCorpusReader in '.../corpora/cess_cat' (not lo...\n",
      "    cess_esp = <BracketParseCorpusReader in '.../corpora/cess_esp' (not lo...\n",
      "    cmudict = <CMUDictCorpusReader in '.../corpora/cmudict' (not loaded ye...\n",
      "    comparative_sentences = <ComparativeSentencesCorpusReader in '.../corp...\n",
      "    comtrans = <AlignedCorpusReader in '.../corpora/comtrans' (not loaded ...\n",
      "    conll2000 = <ConllChunkCorpusReader in '.../corpora/conll2000' (not lo...\n",
      "    conll2002 = <ConllChunkCorpusReader in '.../corpora/conll2002' (not lo...\n",
      "    conll2007 = <DependencyCorpusReader in '.../corpora/conll2007' (not lo...\n",
      "    crubadan = <CrubadanCorpusReader in '.../corpora/crubadan' (not loaded...\n",
      "    dependency_treebank = <DependencyCorpusReader in '.../corpora/dependen...\n",
      "    extended_omw = <CorpusReader in '.../corpora/extended_omw' (not loaded...\n",
      "    floresta = <BracketParseCorpusReader in '.../corpora/floresta' (not lo...\n",
      "    framenet = <FramenetCorpusReader in '.../corpora/framenet_v17' (not lo...\n",
      "    framenet15 = <FramenetCorpusReader in '.../corpora/framenet_v15' (not ...\n",
      "    gazetteers = <WordListCorpusReader in '.../corpora/gazetteers' (not lo...\n",
      "    genesis = <PlaintextCorpusReader in '.../corpora/genesis' (not loaded ...\n",
      "    gutenberg = <PlaintextCorpusReader in '.../corpora/gutenberg' (not loa...\n",
      "    ieer = <IEERCorpusReader in '.../corpora/ieer' (not loaded yet)>\n",
      "    inaugural = <PlaintextCorpusReader in '.../corpora/inaugural' (not loa...\n",
      "    indian = <IndianCorpusReader in '.../corpora/indian' (not loaded yet)>\n",
      "    jeita = <ChasenCorpusReader in '.../corpora/jeita' (not loaded yet)>\n",
      "    knbc = <KNBCorpusReader in '.../corpora/knbc/corpus1' (not loaded yet)...\n",
      "    lin_thesaurus = <LinThesaurusCorpusReader in '.../corpora/lin_thesauru...\n",
      "    mac_morpho = <MacMorphoCorpusReader in '.../corpora/mac_morpho' (not l...\n",
      "    machado = <PortugueseCategorizedPlaintextCorpusReader in '.../corpora/...\n",
      "    masc_tagged = <CategorizedTaggedCorpusReader in '.../corpora/masc_tagg...\n",
      "    movie_reviews = <CategorizedPlaintextCorpusReader in '.../corpora/movi...\n",
      "    multext_east = <MTECorpusReader in '.../corpora/mte_teip5' (not loaded...\n",
      "    names = <WordListCorpusReader in '.../corpora/names' (not loaded yet)>\n",
      "    nombank = <NombankCorpusReader in '.../corpora/nombank.1.0' (not loade...\n",
      "    nombank_ptb = <NombankCorpusReader in '.../corpora/nombank.1.0' (not l...\n",
      "    nonbreaking_prefixes = <NonbreakingPrefixesCorpusReader in '.../corpor...\n",
      "    nps_chat = <NPSChatCorpusReader in '.../corpora/nps_chat' (not loaded ...\n",
      "    opinion_lexicon = <OpinionLexiconCorpusReader in '.../corpora/opinion_...\n",
      "    perluniprops = <UnicharsCorpusReader in '.../corpora/perluniprops' (no...\n",
      "    ppattach = <PPAttachmentCorpusReader in '.../corpora/ppattach' (not lo...\n",
      "    product_reviews_1 = <ReviewsCorpusReader in '.../corpora/product_revie...\n",
      "    product_reviews_2 = <ReviewsCorpusReader in '.../corpora/product_revie...\n",
      "    propbank = <PropbankCorpusReader in '.../corpora/propbank' (not loaded...\n",
      "    propbank_ptb = <PropbankCorpusReader in '.../corpora/propbank' (not lo...\n",
      "    pros_cons = <ProsConsCorpusReader in '.../corpora/pros_cons' (not load...\n",
      "    ptb = <CategorizedBracketParseCorpusReader in '.../corpora/ptb' (not l...\n",
      "    qc = <StringCategoryCorpusReader in '.../corpora/qc' (not loaded yet)>\n",
      "    reuters = <CategorizedPlaintextCorpusReader in '.../corpora/reuters' (...\n",
      "    rte = <RTECorpusReader in '.../corpora/rte' (not loaded yet)>\n",
      "    semcor = <SemcorCorpusReader in '.../corpora/semcor' (not loaded yet)>\n",
      "    senseval = <SensevalCorpusReader in '.../corpora/senseval' (not loaded...\n",
      "    sentence_polarity = <CategorizedSentencesCorpusReader in '.../corpora/...\n",
      "    sentiwordnet = <SentiWordNetCorpusReader in '.../corpora/sentiwordnet'...\n",
      "    shakespeare = <XMLCorpusReader in '.../corpora/shakespeare' (not loade...\n",
      "    sinica_treebank = <SinicaTreebankCorpusReader in '.../corpora/sinica_t...\n",
      "    state_union = <PlaintextCorpusReader in '.../corpora/state_union' (not...\n",
      "    stopwords = <WordListCorpusReader in 'C:\\\\Users\\\\karth\\\\AppData\\\\Roami...\n",
      "    subjectivity = <CategorizedSentencesCorpusReader in '.../corpora/subje...\n",
      "    swadesh = <SwadeshCorpusReader in '.../corpora/swadesh' (not loaded ye...\n",
      "    swadesh110 = <PanlexSwadeshCorpusReader in '.../corpora/panlex_swadesh...\n",
      "    swadesh207 = <PanlexSwadeshCorpusReader in '.../corpora/panlex_swadesh...\n",
      "    switchboard = <SwitchboardCorpusReader in '.../corpora/switchboard' (n...\n",
      "    timit = <TimitCorpusReader in '.../corpora/timit' (not loaded yet)>\n",
      "    timit_tagged = <TimitTaggedCorpusReader in '.../corpora/timit' (not lo...\n",
      "    toolbox = <ToolboxCorpusReader in '.../corpora/toolbox' (not loaded ye...\n",
      "    treebank = <BracketParseCorpusReader in '.../corpora/treebank/combined...\n",
      "    treebank_chunk = <ChunkedCorpusReader in '.../corpora/treebank/tagged'...\n",
      "    treebank_raw = <PlaintextCorpusReader in '.../corpora/treebank/raw' (n...\n",
      "    twitter_samples = <TwitterCorpusReader in '.../corpora/twitter_samples...\n",
      "    udhr = <UdhrCorpusReader in '.../corpora/udhr' (not loaded yet)>\n",
      "    udhr2 = <PlaintextCorpusReader in '.../corpora/udhr2' (not loaded yet)...\n",
      "    universal_treebanks = <ConllCorpusReader in '.../corpora/universal_tre...\n",
      "    verbnet = <VerbnetCorpusReader in '.../corpora/verbnet' (not loaded ye...\n",
      "    webtext = <PlaintextCorpusReader in '.../corpora/webtext' (not loaded ...\n",
      "    wordnet = <WordNetCorpusReader in 'C:\\\\Users\\\\karth\\\\AppDa...aming\\\\nl...\n",
      "    wordnet2021 = <WordNetCorpusReader in '.../corpora/wordnet2021' (not l...\n",
      "    wordnet2022 = <WordNetCorpusReader in '.../corpora/wordnet2022' (not l...\n",
      "    wordnet31 = <WordNetCorpusReader in '.../corpora/wordnet31' (not loade...\n",
      "    wordnet_ic = <WordNetICCorpusReader in '.../corpora/wordnet_ic' (not l...\n",
      "    words = <WordListCorpusReader in '.../corpora/words' (not loaded yet)>\n",
      "\n",
      "FILE\n",
      "    c:\\programdata\\anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#help(nltk.corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "461b45b8-6911-473d-9aa1-1546815418e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ',\n",
       " 'إذا',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'أف',\n",
       " 'أقل',\n",
       " 'أكثر',\n",
       " 'ألا',\n",
       " 'إلا',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللاتي',\n",
       " 'اللائي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أما',\n",
       " 'إما',\n",
       " 'أن',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'أنا',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'أنى',\n",
       " 'أنى',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'أو',\n",
       " 'أولاء',\n",
       " 'أولئك',\n",
       " 'أوه',\n",
       " 'آي',\n",
       " 'أي',\n",
       " 'أيها',\n",
       " 'إي',\n",
       " 'أين',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'إيه',\n",
       " 'بخ',\n",
       " 'بس',\n",
       " 'بعد',\n",
       " 'بعض',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بين',\n",
       " 'بيد',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'ثم',\n",
       " 'ثمة',\n",
       " 'حاشا',\n",
       " 'حبذا',\n",
       " 'حتى',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'خلا',\n",
       " 'دون',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ريث',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'شتان',\n",
       " 'عدا',\n",
       " 'عسى',\n",
       " 'عل',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'غير',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فلا',\n",
       " 'فمن',\n",
       " 'في',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'قد',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كم',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'لا',\n",
       " 'لاسيما',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'لئن',\n",
       " 'ليت',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'ما',\n",
       " 'ماذا',\n",
       " 'متى',\n",
       " 'مذ',\n",
       " 'مع',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'منذ',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'ها',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاهنا',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هؤلاء',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهو',\n",
       " 'يا',\n",
       " 'أبٌ',\n",
       " 'أخٌ',\n",
       " 'حمٌ',\n",
       " 'فو',\n",
       " 'أنتِ',\n",
       " 'يناير',\n",
       " 'فبراير',\n",
       " 'مارس',\n",
       " 'أبريل',\n",
       " 'مايو',\n",
       " 'يونيو',\n",
       " 'يوليو',\n",
       " 'أغسطس',\n",
       " 'سبتمبر',\n",
       " 'أكتوبر',\n",
       " 'نوفمبر',\n",
       " 'ديسمبر',\n",
       " 'جانفي',\n",
       " 'فيفري',\n",
       " 'مارس',\n",
       " 'أفريل',\n",
       " 'ماي',\n",
       " 'جوان',\n",
       " 'جويلية',\n",
       " 'أوت',\n",
       " 'كانون',\n",
       " 'شباط',\n",
       " 'آذار',\n",
       " 'نيسان',\n",
       " 'أيار',\n",
       " 'حزيران',\n",
       " 'تموز',\n",
       " 'آب',\n",
       " 'أيلول',\n",
       " 'تشرين',\n",
       " 'دولار',\n",
       " 'دينار',\n",
       " 'ريال',\n",
       " 'درهم',\n",
       " 'ليرة',\n",
       " 'جنيه',\n",
       " 'قرش',\n",
       " 'مليم',\n",
       " 'فلس',\n",
       " 'هللة',\n",
       " 'سنتيم',\n",
       " 'يورو',\n",
       " 'ين',\n",
       " 'يوان',\n",
       " 'شيكل',\n",
       " 'واحد',\n",
       " 'اثنان',\n",
       " 'ثلاثة',\n",
       " 'أربعة',\n",
       " 'خمسة',\n",
       " 'ستة',\n",
       " 'سبعة',\n",
       " 'ثمانية',\n",
       " 'تسعة',\n",
       " 'عشرة',\n",
       " 'أحد',\n",
       " 'اثنا',\n",
       " 'اثني',\n",
       " 'إحدى',\n",
       " 'ثلاث',\n",
       " 'أربع',\n",
       " 'خمس',\n",
       " 'ست',\n",
       " 'سبع',\n",
       " 'ثماني',\n",
       " 'تسع',\n",
       " 'عشر',\n",
       " 'ثمان',\n",
       " 'سبت',\n",
       " 'أحد',\n",
       " 'اثنين',\n",
       " 'ثلاثاء',\n",
       " 'أربعاء',\n",
       " 'خميس',\n",
       " 'جمعة',\n",
       " 'أول',\n",
       " 'ثان',\n",
       " 'ثاني',\n",
       " 'ثالث',\n",
       " 'رابع',\n",
       " 'خامس',\n",
       " 'سادس',\n",
       " 'سابع',\n",
       " 'ثامن',\n",
       " 'تاسع',\n",
       " 'عاشر',\n",
       " 'حادي',\n",
       " 'أ',\n",
       " 'ب',\n",
       " 'ت',\n",
       " 'ث',\n",
       " 'ج',\n",
       " 'ح',\n",
       " 'خ',\n",
       " 'د',\n",
       " 'ذ',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'س',\n",
       " 'ش',\n",
       " 'ص',\n",
       " 'ض',\n",
       " 'ط',\n",
       " 'ظ',\n",
       " 'ع',\n",
       " 'غ',\n",
       " 'ف',\n",
       " 'ق',\n",
       " 'ك',\n",
       " 'ل',\n",
       " 'م',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ي',\n",
       " 'ء',\n",
       " 'ى',\n",
       " 'آ',\n",
       " 'ؤ',\n",
       " 'ئ',\n",
       " 'أ',\n",
       " 'ة',\n",
       " 'ألف',\n",
       " 'باء',\n",
       " 'تاء',\n",
       " 'ثاء',\n",
       " 'جيم',\n",
       " 'حاء',\n",
       " 'خاء',\n",
       " 'دال',\n",
       " 'ذال',\n",
       " 'راء',\n",
       " 'زاي',\n",
       " 'سين',\n",
       " 'شين',\n",
       " 'صاد',\n",
       " 'ضاد',\n",
       " 'طاء',\n",
       " 'ظاء',\n",
       " 'عين',\n",
       " 'غين',\n",
       " 'فاء',\n",
       " 'قاف',\n",
       " 'كاف',\n",
       " 'لام',\n",
       " 'ميم',\n",
       " 'نون',\n",
       " 'هاء',\n",
       " 'واو',\n",
       " 'ياء',\n",
       " 'همزة',\n",
       " 'ي',\n",
       " 'نا',\n",
       " 'ك',\n",
       " 'كن',\n",
       " 'ه',\n",
       " 'إياه',\n",
       " 'إياها',\n",
       " 'إياهما',\n",
       " 'إياهم',\n",
       " 'إياهن',\n",
       " 'إياك',\n",
       " 'إياكما',\n",
       " 'إياكم',\n",
       " 'إياك',\n",
       " 'إياكن',\n",
       " 'إياي',\n",
       " 'إيانا',\n",
       " 'أولالك',\n",
       " 'تانِ',\n",
       " 'تانِك',\n",
       " 'تِه',\n",
       " 'تِي',\n",
       " 'تَيْنِ',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'ذانِ',\n",
       " 'ذِه',\n",
       " 'ذِي',\n",
       " 'ذَيْنِ',\n",
       " 'هَؤلاء',\n",
       " 'هَاتانِ',\n",
       " 'هَاتِه',\n",
       " 'هَاتِي',\n",
       " 'هَاتَيْنِ',\n",
       " 'هَذا',\n",
       " 'هَذانِ',\n",
       " 'هَذِه',\n",
       " 'هَذِي',\n",
       " 'هَذَيْنِ',\n",
       " 'الألى',\n",
       " 'الألاء',\n",
       " 'أل',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'ذيت',\n",
       " 'كأيّ',\n",
       " 'كأيّن',\n",
       " 'بضع',\n",
       " 'فلان',\n",
       " 'وا',\n",
       " 'آمينَ',\n",
       " 'آهِ',\n",
       " 'آهٍ',\n",
       " 'آهاً',\n",
       " 'أُفٍّ',\n",
       " 'أُفٍّ',\n",
       " 'أفٍّ',\n",
       " 'أمامك',\n",
       " 'أمامكَ',\n",
       " 'أوّهْ',\n",
       " 'إلَيْكَ',\n",
       " 'إلَيْكَ',\n",
       " 'إليكَ',\n",
       " 'إليكنّ',\n",
       " 'إيهٍ',\n",
       " 'بخٍ',\n",
       " 'بسّ',\n",
       " 'بَسْ',\n",
       " 'بطآن',\n",
       " 'بَلْهَ',\n",
       " 'حاي',\n",
       " 'حَذارِ',\n",
       " 'حيَّ',\n",
       " 'حيَّ',\n",
       " 'دونك',\n",
       " 'رويدك',\n",
       " 'سرعان',\n",
       " 'شتانَ',\n",
       " 'شَتَّانَ',\n",
       " 'صهْ',\n",
       " 'صهٍ',\n",
       " 'طاق',\n",
       " 'طَق',\n",
       " 'عَدَسْ',\n",
       " 'كِخ',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانكم',\n",
       " 'مكانكما',\n",
       " 'مكانكنّ',\n",
       " 'نَخْ',\n",
       " 'هاكَ',\n",
       " 'هَجْ',\n",
       " 'هلم',\n",
       " 'هيّا',\n",
       " 'هَيْهات',\n",
       " 'وا',\n",
       " 'واهاً',\n",
       " 'وراءَك',\n",
       " 'وُشْكَانَ',\n",
       " 'وَيْ',\n",
       " 'يفعلان',\n",
       " 'تفعلان',\n",
       " 'يفعلون',\n",
       " 'تفعلون',\n",
       " 'تفعلين',\n",
       " 'اتخذ',\n",
       " 'ألفى',\n",
       " 'تخذ',\n",
       " 'ترك',\n",
       " 'تعلَّم',\n",
       " 'جعل',\n",
       " 'حجا',\n",
       " 'حبيب',\n",
       " 'خال',\n",
       " 'حسب',\n",
       " 'خال',\n",
       " 'درى',\n",
       " 'رأى',\n",
       " 'زعم',\n",
       " 'صبر',\n",
       " 'ظنَّ',\n",
       " 'عدَّ',\n",
       " 'علم',\n",
       " 'غادر',\n",
       " 'ذهب',\n",
       " 'وجد',\n",
       " 'ورد',\n",
       " 'وهب',\n",
       " 'أسكن',\n",
       " 'أطعم',\n",
       " 'أعطى',\n",
       " 'رزق',\n",
       " 'زود',\n",
       " 'سقى',\n",
       " 'كسا',\n",
       " 'أخبر',\n",
       " 'أرى',\n",
       " 'أعلم',\n",
       " 'أنبأ',\n",
       " 'حدَث',\n",
       " 'خبَّر',\n",
       " 'نبَّا',\n",
       " 'أفعل به',\n",
       " 'ما أفعله',\n",
       " 'بئس',\n",
       " 'ساء',\n",
       " 'طالما',\n",
       " 'قلما',\n",
       " 'لات',\n",
       " 'لكنَّ',\n",
       " 'ءَ',\n",
       " 'أجل',\n",
       " 'إذاً',\n",
       " 'أمّا',\n",
       " 'إمّا',\n",
       " 'إنَّ',\n",
       " 'أنًّ',\n",
       " 'أى',\n",
       " 'إى',\n",
       " 'أيا',\n",
       " 'ب',\n",
       " 'ثمَّ',\n",
       " 'جلل',\n",
       " 'جير',\n",
       " 'رُبَّ',\n",
       " 'س',\n",
       " 'علًّ',\n",
       " 'ف',\n",
       " 'كأنّ',\n",
       " 'كلَّا',\n",
       " 'كى',\n",
       " 'ل',\n",
       " 'لات',\n",
       " 'لعلَّ',\n",
       " 'لكنَّ',\n",
       " 'لكنَّ',\n",
       " 'م',\n",
       " 'نَّ',\n",
       " 'هلّا',\n",
       " 'وا',\n",
       " 'أل',\n",
       " 'إلّا',\n",
       " 'ت',\n",
       " 'ك',\n",
       " 'لمّا',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ا',\n",
       " 'ي',\n",
       " 'تجاه',\n",
       " 'تلقاء',\n",
       " 'جميع',\n",
       " 'حسب',\n",
       " 'سبحان',\n",
       " 'شبه',\n",
       " 'لعمر',\n",
       " 'مثل',\n",
       " 'معاذ',\n",
       " 'أبو',\n",
       " 'أخو',\n",
       " 'حمو',\n",
       " 'فو',\n",
       " 'مئة',\n",
       " 'مئتان',\n",
       " 'ثلاثمئة',\n",
       " 'أربعمئة',\n",
       " 'خمسمئة',\n",
       " 'ستمئة',\n",
       " 'سبعمئة',\n",
       " 'ثمنمئة',\n",
       " 'تسعمئة',\n",
       " 'مائة',\n",
       " 'ثلاثمائة',\n",
       " 'أربعمائة',\n",
       " 'خمسمائة',\n",
       " 'ستمائة',\n",
       " 'سبعمائة',\n",
       " 'ثمانمئة',\n",
       " 'تسعمائة',\n",
       " 'عشرون',\n",
       " 'ثلاثون',\n",
       " 'اربعون',\n",
       " 'خمسون',\n",
       " 'ستون',\n",
       " 'سبعون',\n",
       " 'ثمانون',\n",
       " 'تسعون',\n",
       " 'عشرين',\n",
       " 'ثلاثين',\n",
       " 'اربعين',\n",
       " 'خمسين',\n",
       " 'ستين',\n",
       " 'سبعين',\n",
       " 'ثمانين',\n",
       " 'تسعين',\n",
       " 'بضع',\n",
       " 'نيف',\n",
       " 'أجمع',\n",
       " 'جميع',\n",
       " 'عامة',\n",
       " 'عين',\n",
       " 'نفس',\n",
       " 'لا سيما',\n",
       " 'أصلا',\n",
       " 'أهلا',\n",
       " 'أيضا',\n",
       " 'بؤسا',\n",
       " 'بعدا',\n",
       " 'بغتة',\n",
       " 'تعسا',\n",
       " 'حقا',\n",
       " 'حمدا',\n",
       " 'خلافا',\n",
       " 'خاصة',\n",
       " 'دواليك',\n",
       " 'سحقا',\n",
       " 'سرا',\n",
       " 'سمعا',\n",
       " 'صبرا',\n",
       " 'صدقا',\n",
       " 'صراحة',\n",
       " 'طرا',\n",
       " 'عجبا',\n",
       " 'عيانا',\n",
       " 'غالبا',\n",
       " 'فرادى',\n",
       " 'فضلا',\n",
       " 'قاطبة',\n",
       " 'كثيرا',\n",
       " 'لبيك',\n",
       " 'معاذ',\n",
       " 'أبدا',\n",
       " 'إزاء',\n",
       " 'أصلا',\n",
       " 'الآن',\n",
       " 'أمد',\n",
       " 'أمس',\n",
       " 'آنفا',\n",
       " 'آناء',\n",
       " 'أنّى',\n",
       " 'أول',\n",
       " 'أيّان',\n",
       " 'تارة',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'حقا',\n",
       " 'صباح',\n",
       " 'مساء',\n",
       " 'ضحوة',\n",
       " 'عوض',\n",
       " 'غدا',\n",
       " 'غداة',\n",
       " 'قطّ',\n",
       " 'كلّما',\n",
       " 'لدن',\n",
       " 'لمّا',\n",
       " 'مرّة',\n",
       " 'قبل',\n",
       " 'خلف',\n",
       " 'أمام',\n",
       " 'فوق',\n",
       " 'تحت',\n",
       " 'يمين',\n",
       " 'شمال',\n",
       " 'ارتدّ',\n",
       " 'استحال',\n",
       " 'أصبح',\n",
       " 'أضحى',\n",
       " 'آض',\n",
       " 'أمسى',\n",
       " 'انقلب',\n",
       " 'بات',\n",
       " 'تبدّل',\n",
       " 'تحوّل',\n",
       " 'حار',\n",
       " 'رجع',\n",
       " 'راح',\n",
       " 'صار',\n",
       " 'ظلّ',\n",
       " 'عاد',\n",
       " 'غدا',\n",
       " 'كان',\n",
       " 'ما انفك',\n",
       " 'ما برح',\n",
       " 'مادام',\n",
       " 'مازال',\n",
       " 'مافتئ',\n",
       " 'ابتدأ',\n",
       " 'أخذ',\n",
       " 'اخلولق',\n",
       " 'أقبل',\n",
       " 'انبرى',\n",
       " 'أنشأ',\n",
       " 'أوشك',\n",
       " 'جعل',\n",
       " 'حرى',\n",
       " 'شرع',\n",
       " 'طفق',\n",
       " 'علق',\n",
       " 'قام',\n",
       " 'كرب',\n",
       " 'كاد',\n",
       " 'هبّ']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb7439d-58a9-43ca-9005-c565cb43da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Corpus ->Sentence ->words ->stemming/lemma ->stopwords  ->result\n",
    "# -------------------------------------------------------------- ======="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
