{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3be11-a465-4a3c-826e-c9baff070c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Python] <-------------------------->[ DataBase ]\n",
    "                <DBI>                    ...SQL\n",
    "1. import databaseModule (ex: oracledb)\n",
    "2. databaseModule.connect(<connection parameters>) ->establish connection =>connection_object\n",
    "|\n",
    "3. using connection_object.cursor() ->statement\n",
    "4. using cursor_object.execute('Query')\n",
    "                                  |<-- DB Query\n",
    "                                        |->select statement\n",
    "                                                |->fetchone() fetchall() fetch_many()..\n",
    "                                                |->generator =>typecast to list =>list()\n",
    "5. connection_object.commit()\n",
    "6. connection_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949320d7-e4d2-4894-bca5-7d7806f83b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\karth>python\n",
    "Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) \n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>> import oracledb\n",
    ">>> conn = oracledb.connect(user=\"student\",password=\"apelix\",dsn=\"localhost:1521/FREEPDB1\")\n",
    ">>>\n",
    ">>> sth = conn.cursor()\n",
    ">>>\n",
    ">>> sth.execute(\"create table T1(id NUMBER,embedding VECTOR(15))\")\n",
    ">>>\n",
    ">>>\n",
    ">>> sth.execute(\"insert into T1(id,embedding) values(101,'[1,2,3]')\")\n",
    "oracledb.exceptions.DatabaseError: ORA-51803: Vector dimension count must \n",
    "match the dimension count specified in\n",
    "the column definition (actual: 3, required: 15).\n",
    ">>>\n",
    ">>> sth.execute(\"insert into T1(id,embedding) values(101,'[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]')\")\n",
    ">>>\n",
    ">>>\n",
    ">>>\n",
    ">>> sth.execute(\"select *from T1\")\n",
    "<oracledb.Cursor on <oracledb.Connection to student@localhost:1521/FREEPDB1>>\n",
    ">>>\n",
    ">>> list(sth)\n",
    "[(101, array('f', [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0]))]\n",
    ">>>\n",
    "==============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dcb705-fc04-4ec6-bb87-4908ffc1e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "OracleVS - Oracle Vector Store \n",
    "|\n",
    "Connection pool to oracle database\n",
    "---------------\n",
    "     |-> \n",
    "\n",
    "from langchain_community.vectorstores import OracleVS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1a3d1a-3a54-4c4d-8fb0-e3b4f5cc4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import OracleVS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import oracledb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ed38b-33a3-4778-8604-583108b1c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# split data - chunk\n",
    "# embedding\n",
    "# create client pool - from oracledb module\n",
    "# create vector store - OracleVS.from_documents()\n",
    "##################################################### 1 time\n",
    "|\n",
    "# search - do morethan one time\n",
    "  vector_store.similarity_search_with_..()\n",
    "   |->threshold  - condition\n",
    "        ...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f909e891-e734-4c09-9c09-58712a2187ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Streamlit\n",
    "- Open-Source python lib\n",
    "- interactive web apps\n",
    "- DataSet ->DataFrame ->dataanalysis \n",
    "- ML ; GenAI - RAG -> UI\n",
    "|\n",
    "webserver - start automatically\n",
    "|->UI - widgets are available \n",
    "        |->button charts text input slider ..\n",
    "\n",
    "pip install streamlit \n",
    "\n",
    "python -m streamlit run p1.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80092c0-2d38-4302-8c30-865848b6320f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\karth'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57d8dfd9-cf79-42d0-9c4d-51d4db94beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\karth\\Karthik\\\\Oracle23AI-Setup and Docs\\\\RAG-Examples\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a2e7ef-3947-4de3-8665-d5e27a5d0b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\karth\\\\Karthik\\\\Oracle23AI-Setup and Docs\\\\RAG-Examples'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5de4d616-9f19-416b-a882-0300ed44e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "658eba39-85fb-4bb5-9f40-45d33972b873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "! streamlit run s1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c4163-82b5-4dc9-acd0-0741c2ce321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy       -  numerical computation - function() + index-reshape(1D <->MD)\n",
    "pandas      - data analysis - DataFrame(Table - Row X Column) Vs  pyspark - distributed data process\n",
    "                    |->clean data |->drop null |->drop duplicated ...\n",
    "matplotlib  - visualization \n",
    "|\n",
    "Model - Machine Learning - sklearn. .. ML \n",
    "\n",
    "pythonAPI + spark = PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff7ec4b2-9bfb-46de-9405-c1a33909a232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['app.py',\n",
       " 'Data-Base-Shell.txt',\n",
       " 'DAY-1-Notes.txt',\n",
       " 'hybrid_vector_demo.py',\n",
       " 'my.txt',\n",
       " 'my_docs.txt',\n",
       " 'oracle_rag.py',\n",
       " 'OU-RAG-ClassNotes',\n",
       " 'OU-RAG-ClassNotes.zip',\n",
       " 'p1.py',\n",
       " 'p2.py',\n",
       " 'p3.py',\n",
       " 'p4.py',\n",
       " 'Python-Demo1.py',\n",
       " 'Python-Demo2-RAG_Project1.py',\n",
       " 'Python-Demo2-RAG_Project1.py__',\n",
       " 'RAG_Examples.docx',\n",
       " 's1.py',\n",
       " 's2.py',\n",
       " 's3.py',\n",
       " 's4.py',\n",
       " 's5.py',\n",
       " '__pycache__']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b55d44-6ae2-4497-a0c5-b5f10363ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> import sqlite3\n",
    ">>> sqlite3.connect(\"Test1.db\")\n",
    "<sqlite3.Connection object at 0x000002C47E271840>\n",
    ">>>\n",
    ">>> conn = sqlite3.connect(\"Test1.db\")\n",
    ">>> sth = conn.cursor()\n",
    ">>> sth.execute(\"create table product1(id INT,pname TEXT)\")\n",
    "<sqlite3.Cursor object at 0x000002C47E3ADA40>\n",
    ">>> sth.execute(\"insert into product1(id,pname) values(101,'pA')\")\n",
    "<sqlite3.Cursor object at 0x000002C47E3ADA40>\n",
    ">>>\n",
    ">>> sth.execute(\"insert into product1(id,pname) values(102,'pB')\")\n",
    "<sqlite3.Cursor object at 0x000002C47E3ADA40>\n",
    ">>>\n",
    ">>>\n",
    ">>> sth.execute(\"select * from product1\")\n",
    "<sqlite3.Cursor object at 0x000002C47E3ADA40>\n",
    ">>>\n",
    ">>> sth.fetchall()\n",
    "[(101, 'pA'), (102, 'pB')]\n",
    ">>>\n",
    ">>> sth.execute(\"select * from product1\")\n",
    "<sqlite3.Cursor object at 0x000002C47E3ADA40>\n",
    ">>>\n",
    ">>> list(sth)\n",
    "[(101, 'pA'), (102, 'pB')]\n",
    ">>>\n",
    ">>> sth.execute(\"select * from product1\")\n",
    "<sqlite3.Cursor object at 0x000002C47E3ADA40>\n",
    ">>>\n",
    ">>> for var in sth:\n",
    "...     print(var)\n",
    "...\n",
    "(101, 'pA')\n",
    "(102, 'pB')\n",
    ">>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f363fa2-f6a0-4e53-b64b-a29866b376b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d04c54fd-c13b-4ce4-92d5-608a65ef7c0f",
   "metadata": {},
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"criteria\", llm=Ollama(model=\"gemma2:2b\"), criteria=\"faithfulness\")\n",
    "\n",
    "question = \"What is LangChain?\"\n",
    "context = \"LangChain is a Python framework for building applications that use LLMs.\"\n",
    "answer = \"LangChain is a Python framework for LLM applications.\"\n",
    "\n",
    "result = evaluator.evaluate_strings(\n",
    "    input=question,\n",
    "    prediction=answer,\n",
    "    reference=context\n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc3442f3-95dc-476b-9342-2f6d1a23d26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Your local judge model\n",
    "judge = OllamaLLM(model=\"gemma2:2b\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6358591f-7a48-4fb0-a5a8-3b370c88409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an evaluator that checks if an answer is grounded in the reference context.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Reference context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Evaluate if the answer is faithful to the context.\n",
    "Reply only with one of the following:\n",
    "- \"Faithful\" (if it is accurate and grounded)\n",
    "- \"Not faithful\" (if it invents or contradicts information)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d2b9ba2-6fc1-4676-8e83-5f2ed6bcbb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Result: Faithful \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What is LangChain?\"\n",
    "context = \"LangChain is a Python framework for building applications using LLMs.\"\n",
    "answer = \"LangChain is a Python library that helps in developing LLM-based applications.\"\n",
    "\n",
    "# Use your local model to judge\n",
    "grade = judge.invoke(prompt_template.format(question=question, context=context, answer=answer))\n",
    "print(\"Evaluation Result:\", grade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f51bfd-e629-4519-84cb-b3685f45c122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfc371b7-0d61-400d-99ce-9881ee276b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Result: Not faithful \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What is LangChain?\"\n",
    "context = \"LangChain is a Python framework for building applications using LLMs.\"\n",
    "answer = \"LangChain is data XYZ\"\n",
    "\n",
    "# Use your local model to judge\n",
    "grade = judge.invoke(prompt_template.format(question=question, context=context, answer=answer))\n",
    "print(\"Evaluation Result:\", grade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6d104d4-934e-43d8-8849-7526a1ffc7c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: LangChain is a powerful, open-source framework designed to simplify the development of applications leveraging large language models (LLMs). \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "**What it does:**\n",
      "\n",
      "* **Combines LLMs with other tools:**  It connects LLMs like ChatGPT or Bard with data sources, databases, and APIs. This allows for richer, more context-aware responses.\n",
      "* **Automates common tasks:** LangChain provides building blocks to automate common tasks in LLM applications, such as question answering, text summarization, code generation, and more. \n",
      "* **Creates chains of prompts and actions:** It facilitates creating complex workflows by combining multiple actions (prompts) for a specific task within an LLM chain.\n",
      "\n",
      "**Why it's valuable:**\n",
      "\n",
      "* **Simplified development:** LangChain abstracts away the complexities of working with LLMs, enabling developers to focus on application logic rather than underlying implementation details.\n",
      "* **Faster iteration cycles:** Its modular design and pre-built components allow for quicker experimentation and prototyping.\n",
      "* **Enhanced LLM applications:**  It unlocks new possibilities for developing interactive chatbots, data analysis tools, personalized content generation platforms, and more.\n",
      "\n",
      "\n",
      "**In short, LangChain is like a toolbox filled with specialized instruments for building effective and sophisticated LLM applications.** \n",
      "\n",
      "If you're looking to build conversational AI, leverage LLMs for tasks beyond simple text generation, or enhance your application with LLM-powered features, LangChain is worth exploring! \n",
      "\n",
      "Reference Answer: LangChain is a Python framework for building applications that use large language models.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Your local model (you can use llama3, gemma2:2b, mistral, etc.)\n",
    "llm = OllamaLLM(model=\"gemma2:2b\")\n",
    "\n",
    "question = \"What is LangChain?\"\n",
    "reference_answer = \"LangChain is a Python framework for building applications that use large language models.\"\n",
    "\n",
    "# Generate answer\n",
    "prompt = f\"Answer this question:\\n{question}\"\n",
    "generated_answer = llm.invoke(prompt)\n",
    "\n",
    "print(\"Generated Answer:\", generated_answer)\n",
    "print(\"Reference Answer:\", reference_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "627adecf-701f-4370-bd47-da429e891561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU Score: 0.01\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Tokenize answers (BLEU compares word-level overlaps)\n",
    "reference = [reference_answer.split()]  # reference must be a list of lists\n",
    "candidate = generated_answer.split()\n",
    "\n",
    "# Add smoothing (BLEU can be zero if words differ slightly)\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "bleu = sentence_bleu(reference, candidate, smoothing_function=smooth_fn)\n",
    "\n",
    "print(f\"\\nBLEU Score: {bleu:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f9699a1-f527-4a13-8081-9a7f093c2446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1ca0fbd6d50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_Name/p1.pdf \n",
    "    p2.pdf\n",
    "    p3.pdf\n",
    "    ..\n",
    "    pn.pdf\n",
    "\n",
    "PyPDFLoader(\"p1.pdf\")  Vs PyPDFDirectoryLoader(\"directory_Name\")\n",
    "                              |\n",
    "                              RecursiveCharacterTextSplitter\n",
    "\n",
    "1. Create a newFolder/directory\n",
    "2. Keep some pdf files\n",
    "3. PyPDFDirectoryLoader(\"inputFolder/Directory\") ->loader_obj\n",
    "   loader_obj.load() ->docs\n",
    "   RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f5ed29f-0fa6-4ca2-a8b6-53d06e4ea6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "groqK = os.getenv('GROQ_API_KEY')\n",
    "llm_model = ChatGroq(model='llama3-8b-8192',groq_api_key=groqK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2ee4008-65c4-4ca0-8afd-be8cdbcb0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir(\"C:\\\\Users\\\\karth\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3036b962-4746-4fd5-aeb5-43915d8b562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0822e704-44a6-4372-b364-6fad1d04ce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_23072\\2597205676.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFDirectoryLoader(\"docs\")\n",
    "docs = loader.load()\n",
    "text = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=100)\n",
    "final_docs = text.split_documents(docs[:50])\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectors = FAISS.from_documents(final_docs,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2cc98a9-924f-454e-a854-1e9748223d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Your are helpful AI assistant\\n{context}\"),\n",
    "    (\"user\",\"Question:{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7481998b-a719-42e3-8ac5-bbf3e9de22a2",
   "metadata": {},
   "source": [
    "qa_chain = create_stuff_documents_chain(llm_model,prompt)\n",
    "ret = vectors.as_retriever()\n",
    "rag_chain = create_retrieval_chain(ret,qa_chain)\n",
    "response = rag_chain.invoke({\"input\":\"what is transformer?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9808ad71-9ac9-4c9d-8802-d8d1157fe8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Project/\n",
    "    |-> data/\n",
    "          |->raw/\n",
    "          |->structured_data/\n",
    "          |->templates/\n",
    "\n",
    "    |-> models/\n",
    "             |->checkpoints/\n",
    "                     |->saved model (file_model.pkl)\n",
    "             |->tokenizers/\n",
    "              ..\n",
    "    |-> training/\n",
    "            |->dataset\n",
    "            |->config.yml\n",
    "\n",
    "    |-> evaluation/\n",
    "            |->metrics.py \n",
    "            |->benchmark_data/\n",
    "                \n",
    "    |-> utils/\n",
    "            |->logging\n",
    "            |->config\n",
    "\n",
    "    |-> api/ \n",
    "          |->main.py\n",
    "          |->templates/\n",
    "                   |->file.html file.css ..\n",
    "          |->static/\n",
    "                   |->...\n",
    "    |-> UI/\n",
    "          streamlit_app.py\n",
    "\n",
    "    |-> requirements.txt\n",
    "        .env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ea939-5b31-4c51-8b44-f9ccc6f14298",
   "metadata": {},
   "outputs": [],
   "source": [
    "flask \n",
    "=====\n",
    "Web-Project/\n",
    "       main.py\n",
    "       templates/\n",
    "       db.py\n",
    "       ...\n",
    "=============// MVT \n",
    "\n",
    "python+decorator+html+templates(jinja2)+css+js\n",
    "        |->design code\n",
    "--------------------------------------------------//"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e04acc-ade1-47a4-bb67-ab698df28f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decorator \n",
    " - function - metaprogramming\n",
    "    - adding new features to an existing code\n",
    "\n",
    "Home About  News Blogs Contacts <==  - site is running -> v1.0\n",
    "\n",
    "\n",
    "Home About  News Blogs Contacts <==  v1.1\n",
    "              |->city1\n",
    "              |->city2\n",
    "\n",
    "\n",
    "Home About  News Blogs Contacts Services <-- v1.2\n",
    "            |->City1\n",
    "            |->City2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d656ea1-11e0-4666-8c7b-ea7f8745ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorator_function(new_feature):\n",
    "    def wrapper_function():\n",
    "        new_feature()\n",
    "    return wrapper_function\n",
    "\n",
    "def new_feature():\n",
    "        ...\n",
    "\n",
    "mew_feature=decorator_function(new_feature)\n",
    "new_feature()\n",
    " |\n",
    " | same as \n",
    " |\n",
    "@decorator_function\n",
    "def new_feature1():\n",
    "        ...\n",
    "new_feature1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48695e46-2c07-4d41-b176-bc4acf2d6702",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSGI-toolkit\n",
    "jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47014014-1043-4acd-bee8-b4a8bf9cce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "obj = Flask(__name__)\n",
    "\n",
    "@obj.route(\"/\")\n",
    "def f1():\n",
    "    return \"<h1>Hello</h1>\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    obj.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85b39a7-20e2-414c-a33a-7dd037e03eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### end of day3 #########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
