{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba30efc3-a243-4a05-890e-23bee0cdeb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbdfd469-121a-48d1-b925-45f4e8820fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Your local judge model\n",
    "judge = OllamaLLM(model=\"gemma2:2b\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b18121a-d71f-4eec-9ffa-904a5ef03ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an evaluator that checks if an answer is grounded in the reference context.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Reference context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Evaluate if the answer is faithful to the context.\n",
    "Reply only with one of the following:\n",
    "- \"Faithful\" (if it is accurate and grounded)\n",
    "- \"Not faithful\" (if it invents or contradicts information)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a94f10da-a654-412f-aa6a-a7e30b6b326e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Result: Faithful \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What is LangChain?\"\n",
    "context = \"LangChain is a Python framework for building applications using LLMs.\"\n",
    "answer = \"LangChain is a Python library that helps in developing LLM-based applications.\"\n",
    "\n",
    "# Use your local model to judge\n",
    "grade = judge.invoke(prompt_template.format(question=question, context=context, answer=answer))\n",
    "print(\"Evaluation Result:\", grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d7750a-e3a6-47ec-bf3e-03452ba5febd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Result: Not faithful \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What is LangChain?\"\n",
    "context = \"LangChain is a Python framework for building applications using LLMs.\"\n",
    "answer =  \"LangChain components\"\n",
    "\n",
    "# Use your local model to judge\n",
    "grade = judge.invoke(prompt_template.format(question=question, context=context, answer=answer))\n",
    "print(\"Evaluation Result:\", grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20eead79-e667-45ed-a789-3bd5188f764d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: LangChain is a powerful framework for building applications powered by large language models (LLMs).  Think of it as **a toolbox** specifically designed to make working with these complex AI systems easier and more efficient.\n",
      "\n",
      "Here's a breakdown: \n",
      "\n",
      "**What it does:**\n",
      "\n",
      "* **Connects LLMs to other data sources:** LangChain allows you to easily combine the vast knowledge of LLMs (like GPT-3) with real-world information, like databases or documents. This means your applications can access relevant and updated details beyond just the text generated by the LLM. \n",
      "* **Facilitates complex workflows:** It's not just about simple questions and answers. LangChain helps you build chains of actions â€“ a sequence where one action triggers another, resulting in a more complex process (like summarizing a document before asking follow-up questions).\n",
      "* **Improves user experience:** LangChain adds features like chat interfaces, document management systems, and question answering engines, making your applications more interactive and intuitive.\n",
      "\n",
      "**Why it's important:** \n",
      "\n",
      "* **Streamlines LLM development:**  LangChain makes building robust and flexible LLM-powered applications much easier for developers. It removes some of the complexities of working directly with LLMs and their inherent limitations.\n",
      "* **Boosts application potential:** By enabling connections between LLM capabilities and real-world data, LangChain unlocks new possibilities for creating truly intelligent and adaptive applications across various industries like education, healthcare, marketing, and more. \n",
      "\n",
      "**Think of it this way:** If you have a powerful hammer but no nails to use it with, LangChain gives you the framework to build amazing things!\n",
      "\n",
      "\n",
      "You can learn more about LangChain by visiting their official website: [https://pythonlangchain.com/](https://pythonlangchain.com/) \n",
      "\n",
      "Reference Answer: LangChain is a Python framework for building applications that use large language models.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Your local model (you can use llama3, gemma2:2b, mistral, etc.)\n",
    "llm = OllamaLLM(model=\"gemma2:2b\")\n",
    "\n",
    "question = \"What is LangChain?\"\n",
    "reference_answer = \"LangChain is a Python framework for building applications that use large language models.\"\n",
    "\n",
    "# Generate answer\n",
    "prompt = f\"Answer this question:\\n{question}\"\n",
    "generated_answer = llm.invoke(prompt)\n",
    "\n",
    "print(\"Generated Answer:\", generated_answer)\n",
    "print(\"Reference Answer:\", reference_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae5f5626-d094-4228-ab43-b0bf2e4e53f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU Score: 0.01\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Tokenize answers (BLEU compares word-level overlaps)\n",
    "reference = [reference_answer.split()]  # reference must be a list of lists\n",
    "candidate = generated_answer.split()\n",
    "\n",
    "# Add smoothing (BLEU can be zero if words differ slightly)\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "bleu = sentence_bleu(reference, candidate, smoothing_function=smooth_fn)\n",
    "\n",
    "print(f\"\\nBLEU Score: {bleu:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622ca840-07cf-49c2-aadd-bb35e82efd93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
