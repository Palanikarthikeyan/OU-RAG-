{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f69637-e1c7-4954-a7db-258c50e66f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Recap\n",
    "======\n",
    "NLP - tokenziation \n",
    "langchain \n",
    " - model - pre-trained by ML and DL\n",
    " - Ollama \n",
    "      |-> download to local m/c -> act a process(systemprocess)\n",
    " - chat bot QA local m/c \n",
    "\n",
    "[Document-load] ->[chunk ]  -> [embedding] ->[vector-13313] ->[DataBase-vector] \n",
    " raw data                                                           \n",
    "=================\n",
    "\n",
    "HuggingFace \n",
    "- embedding\n",
    "Ollama \n",
    "- llm\n",
    "|\n",
    "groq - LPU\n",
    "|\n",
    "prompt template\n",
    "|\n",
    "oracle23 ai \n",
    "-----------\n",
    "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\n",
    "\n",
    "1. Go to commandline/terminal (for windows user => GitBash)\n",
    "2. vi .env\n",
    "\n",
    "HF_TOKEN=\"<paste>\"\n",
    ":wq <== save and exit\n",
    "3. source .env\n",
    "4. env|grep -i HF_TOKEN \n",
    "-------------------------------------------------------//commandline/terminal \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "600d1bff-b416-4455-b6a6-193b7dd5cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc8ee4d-156d-4440-b4fd-12946efcfa60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'ALLUSERSPROFILE': 'C:\\\\ProgramData',\n",
       "        'APPDATA': 'C:\\\\Users\\\\karth\\\\AppData\\\\Roaming',\n",
       "        'COMMONPROGRAMFILES': 'C:\\\\Program Files\\\\Common Files',\n",
       "        'COMMONPROGRAMFILES(X86)': 'C:\\\\Program Files (x86)\\\\Common Files',\n",
       "        'COMMONPROGRAMW6432': 'C:\\\\Program Files\\\\Common Files',\n",
       "        'COMPUTERNAME': 'PAKA',\n",
       "        'COMSPEC': 'C:\\\\WINDOWS\\\\system32\\\\cmd.exe',\n",
       "        'CONDA_PREFIX': 'C:\\\\ProgramData\\\\anaconda3',\n",
       "        'DRIVERDATA': 'C:\\\\Windows\\\\System32\\\\Drivers\\\\DriverData',\n",
       "        'HADOOP_HOME': 'C:\\\\hadoop',\n",
       "        'HOMEDRIVE': 'C:',\n",
       "        'HOMEPATH': '\\\\Users\\\\karth',\n",
       "        'IGCCSVC_DB': 'AQAAANCMnd8BFdERjHoAwE/Cl+sBAAAA/jMmQiI+3Uu0CJ+M30qPdQQAAAACAAAAAAAQZgAAAAEAACAAAACzWqQ/qnf6OBa6nybKt6MvWAz1MD1IZEfDM6Kfrw93/wAAAAAOgAAAAAIAACAAAAA9i/96dUV1FlnlkFAuDr31D8Mx+C/bU5eWQLMUw0VBTGAAAAD8e9JcM/pqtNvMIPE4lL8YySH4QAhZhJ5ePuvvraESD1Ckvq21kYdpo2hCC2BLPig1b8qSfcU7gEU+SJov/zU5aaexkcnOZn5wXQ53DLd2cD6JgRNVOVxAd7rMz1YEesNAAAAAgOebOXQTGlWcI/yrXwHJFXuiSbxGLXt8GfOCisstAVBzijgjtPxCmVHoAKe53HFFOMyLyUOdgHvAVCSB5yFMvw==',\n",
       "        'IPY_INTERRUPT_EVENT': '1584',\n",
       "        'JAVA_HOME': 'C:\\\\Java\\\\',\n",
       "        'JPY_INTERRUPT_EVENT': '1584',\n",
       "        'JPY_PARENT_PID': '2152',\n",
       "        'JPY_SESSION_NAME': 'C:\\\\Users\\\\karth\\\\Untitled11.ipynb',\n",
       "        'LOCALAPPDATA': 'C:\\\\Users\\\\karth\\\\AppData\\\\Local',\n",
       "        'LOGONSERVER': '\\\\\\\\PAKA',\n",
       "        'NUMBER_OF_PROCESSORS': '16',\n",
       "        'OLLAMA_HOST': 'http://127.0.0.1:11434',\n",
       "        'OLLAMA_MODELS': 'gemma3',\n",
       "        'ONEDRIVE': 'C:\\\\Users\\\\karth\\\\OneDrive',\n",
       "        'ONEDRIVECONSUMER': 'C:\\\\Users\\\\karth\\\\OneDrive',\n",
       "        'OS': 'Windows_NT',\n",
       "        'PATH': 'C:\\\\ProgramData\\\\anaconda3;C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\mingw-w64\\\\bin;C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\usr\\\\bin;C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\bin;C:\\\\ProgramData\\\\anaconda3\\\\Scripts;C:\\\\app\\\\karth\\\\product\\\\23ai\\\\dbhomeFree\\\\bin;C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\;C:\\\\Java\\\\bin;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\WINDOWS\\\\System32\\\\OpenSSH\\\\;C:\\\\Program Files\\\\dotnet\\\\;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\ngrok;C:\\\\Java\\\\\\\\bin;%SPARK_HOME%\\\\bin;C:\\\\hadoop\\\\bin;C:\\\\Program Files\\\\MiKTeX\\\\miktex\\\\bin\\\\x64\\\\;C:\\\\Strawberry\\\\c\\\\bin;C:\\\\Strawberry\\\\perl\\\\site\\\\bin;C:\\\\Strawberry\\\\perl\\\\bin;C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Scripts;C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Scripts\\\\;C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\;C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Launcher\\\\;C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Programs\\\\Ollama;C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\bin;C:\\\\Users\\\\karth\\\\AppData\\\\Roaming\\\\Python\\\\Python313\\\\Scripts',\n",
       "        'PATHEXT': '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC',\n",
       "        'PROCESSOR_ARCHITECTURE': 'AMD64',\n",
       "        'PROCESSOR_IDENTIFIER': 'Intel64 Family 6 Model 186 Stepping 2, GenuineIntel',\n",
       "        'PROCESSOR_LEVEL': '6',\n",
       "        'PROCESSOR_REVISION': 'ba02',\n",
       "        'PROGRAMDATA': 'C:\\\\ProgramData',\n",
       "        'PROGRAMFILES': 'C:\\\\Program Files',\n",
       "        'PROGRAMFILES(X86)': 'C:\\\\Program Files (x86)',\n",
       "        'PROGRAMW6432': 'C:\\\\Program Files',\n",
       "        'PSMODULEPATH': 'C:\\\\Program Files\\\\WindowsPowerShell\\\\Modules;C:\\\\WINDOWS\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules;',\n",
       "        'PUBLIC': 'C:\\\\Users\\\\Public',\n",
       "        'PYSPARK_DRIVER_PYTHON': 'C:\\\\\\\\ProgramData\\\\\\\\anaconda3\\\\\\\\python.exe',\n",
       "        'PYSPARK_PYTHON': 'C:\\\\\\\\ProgramData\\\\\\\\anaconda3\\\\\\\\python.exe',\n",
       "        'PYTHONPATH': 'C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310',\n",
       "        'SUBPROCESS_BAGGAGE': 'sentry-trace_id=a2dd0a5ec00a475194a836ec9ebbb2a3,sentry-environment=local_production,sentry-release=4.20.0,sentry-public_key=f175bff95ca4ae57c532f2bf67cbaa63',\n",
       "        'SUBPROCESS_SENTRY_TRACE': 'a2dd0a5ec00a475194a836ec9ebbb2a3-9f641914d1c205a7',\n",
       "        'SYSTEMDRIVE': 'C:',\n",
       "        'SYSTEMROOT': 'C:\\\\WINDOWS',\n",
       "        'TEMP': 'C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Temp',\n",
       "        'TMP': 'C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Temp',\n",
       "        'USERDOMAIN': 'PAKA',\n",
       "        'USERDOMAIN_ROAMINGPROFILE': 'PAKA',\n",
       "        'USERNAME': 'karth',\n",
       "        'USERPROFILE': 'C:\\\\Users\\\\karth',\n",
       "        'VBOX_MSI_INSTALL_PATH': 'C:\\\\Program Files\\\\Oracle\\\\VirtualBox\\\\',\n",
       "        'WINDIR': 'C:\\\\WINDOWS',\n",
       "        'ZES_ENABLE_SYSMAN': '1',\n",
       "        'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'FORCE_COLOR': '1',\n",
       "        'CLICOLOR_FORCE': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://matplotlib_inline.backend_inline'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11cba35d-cbc4-4584-ba30-65e80ac6e928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10801acf-cfb2-4ef4-b573-61f8989d04a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbaf2e3a-19ea-4ffa-a7bf-9461dfb60851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33e7e7c8-5b88-4f8b-b154-656be6d391d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d={}\n",
    "d['K1']='V1' # adding new data to an existing dict\n",
    "\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN') # like dict style adding new data(key:value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9062ab2f-024b-4afb-b9d6-7fef5ad55a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0a5aebe-e2cf-4ac1-be00-0176a1f8695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_huggingface.embeddings.huggingface.HuggingFaceEmbeddings'>\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(type(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adb15439-135c-4f3b-aafe-c158587bc970",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0627717599272728, 0.05495879426598549, 0.052164845168590546, 0.0857900083065033, -0.0827488973736763, -0.07457297295331955, 0.06855470687150955, 0.018396394327282906, -0.08201129734516144, -0.03738484904170036, 0.012124893255531788, 0.0035183229483664036, -0.004134293645620346, -0.04378441721200943, 0.021807299926877022, -0.00510272104293108, 0.019546564668416977, -0.0423487089574337, -0.11035966873168945, 0.005424527917057276, -0.055734783411026, 0.02805238962173462, -0.02315872348845005, 0.028481375426054, -0.05370955914258957, -0.052601538598537445, 0.03393925353884697, 0.045388635247945786, 0.023718422278761864, -0.07312081754207611, 0.05477774888277054, 0.0170473363250494, 0.08136039972305298, -0.0028627056162804365, 0.011958062648773193, 0.07355856150388718, -0.09423744678497314, -0.0813620463013649, 0.04001539200544357, 0.0006921913009136915, -0.01339334063231945, -0.05453811213374138, 0.005151401273906231, -0.026139769703149796, 0.0368071012198925, -0.03395958989858627, 0.021093212068080902, 0.055948879569768906, 0.05778130516409874, -0.0054183765314519405, -0.06841319799423218, -0.09023699164390564, -0.04286673292517662, 0.023652682080864906, 0.12149356305599213, 0.032392844557762146, -0.02265944518148899, -0.02328176610171795, 0.04886598512530327, -0.059351544827222824, -0.03406606614589691, 0.03596258535981178, -0.08115777373313904, -0.021979473531246185, 0.013187813572585583, -0.04585552215576172, -0.07041982561349869, -0.05260329321026802, -0.04802926629781723, -0.07454610615968704, -0.028731804341077805, 0.012831883504986763, -0.042921729385852814, 0.0053663537837564945, -0.03909760341048241, 0.015919268131256104, 0.020243527367711067, 0.0052666617557406425, 0.009581327438354492, -0.0047676158137619495, 0.04808773100376129, -0.07309659570455551, -0.05034444108605385, 0.009653042070567608, 0.013496600091457367, 0.00043035211274400353, 0.019725199788808823, 0.062495484948158264, -0.01822715625166893, 0.03109716810286045, -0.08885394781827927, 0.04813242703676224, 0.026074277237057686, -0.011494041420519352, -0.09936171025037766, -0.027338232845067978, 0.07621528953313828, -0.002096540527418256, -0.12358873337507248, 0.29806533455848694, 0.05630640685558319, 0.0784800797700882, 0.01104352530092001, 0.052400071173906326, -0.0011880238307639956, 0.000965684128459543, -0.05432729423046112, 0.02676473930478096, -0.012194258160889149, 0.010717053897678852, -3.707817450049333e-05, -0.035565707832574844, -0.03208553418517113, 0.016168801113963127, 0.08674969524145126, 0.018139464780688286, -0.01101755816489458, 0.04974030703306198, 0.02258104830980301, -2.2351243387674913e-05, 0.014939067885279655, -0.011176216416060925, 0.005659336689859629, -0.004114812705665827, 0.003182594431564212, 0.02386500872671604, 0.01805107854306698, -5.850476159002819e-33, 0.05610707402229309, -0.03449849784374237, 0.033111825585365295, 0.1677229106426239, -0.031038135290145874, -0.0048353467136621475, -0.06106499955058098, -0.06271100044250488, 0.027426589280366898, 0.06364015489816666, 0.043340425938367844, 0.06082936003804207, -0.018160779029130936, 0.04280414804816246, 0.019024066627025604, 0.08797787874937057, -0.039132826030254364, 0.04414263367652893, -0.004941055551171303, 0.051052697002887726, -0.05431916192173958, 0.011173113249242306, 0.0266990028321743, 0.07509659975767136, 0.048690132796764374, -0.04354275390505791, 0.01333867758512497, -0.10262981057167053, 0.05215669423341751, 0.02223237417638302, -0.029741793870925903, -0.04227066785097122, 0.022984562441706657, 0.03957443684339523, 0.009094778448343277, 0.020864685997366905, 0.00509262178093195, -0.06276409327983856, -0.05021766945719719, -0.005225018598139286, -0.05349820479750633, 0.029773466289043427, 0.021753670647740364, -0.020678048953413963, 0.023866144940257072, 0.005806638393551111, -0.0012137835146859288, 0.022660311311483383, 0.0031567111145704985, 0.030960429459810257, -0.05296808481216431, 0.01867694966495037, -0.1401534080505371, 0.04147820174694061, -0.010278256610035896, -0.011593146249651909, -0.033458251506090164, -0.05058891698718071, 0.04686781391501427, 0.024715127423405647, 0.03349681571125984, 0.11170760542154312, -0.0403486005961895, -0.0042844959534704685, -0.08074361085891724, -0.05611216649413109, 0.03833862021565437, 0.011508360505104065, 0.06873797625303268, -0.03811735659837723, -0.04598833993077278, -0.016439592465758324, 0.024412987753748894, 0.011722046881914139, 0.008168509230017662, 0.039036914706230164, 0.026186540722846985, 0.01048184558749199, 0.04286017641425133, -0.04638158529996872, 0.006481093820184469, 0.044423412531614304, -0.01883763261139393, 0.00726337032392621, 0.05617952719330788, 0.053477976471185684, -0.02148376777768135, -0.084485724568367, -0.012813972309231758, -0.039475537836551666, -0.05802653357386589, 0.03133402392268181, 0.045417316257953644, 0.011839081533253193, -0.017906315624713898, 4.586215086962156e-33, 0.13150791823863983, 0.07932813465595245, -0.09495951235294342, -0.024472175166010857, -0.055816203355789185, -0.009145256131887436, -0.032090410590171814, 0.11379317939281464, -0.14441627264022827, 0.00850959587842226, 0.030667033046483994, -0.01241487916558981, 0.07017052918672562, 0.028179822489619255, 0.04082208126783371, 0.019590837880969048, 0.14295044541358948, 0.0568699836730957, -0.040122076869010925, -0.017843913286924362, -0.06138443946838379, 0.0008760516648180783, -0.054906200617551804, -0.007102516014128923, -0.0004941486986353993, -0.013119390234351158, -0.0022669434547424316, 0.05856667086482048, -0.09971513599157333, -0.025532379746437073, 0.07856457680463791, 0.020517690107226372, -0.004633414093405008, 0.030302615836262703, 0.016833249479532242, 0.09143757820129395, 0.016769731417298317, -0.07980289310216904, 0.04251027852296829, -0.08439735323190689, -0.0238590519875288, 0.04701947048306465, 0.0024148565717041492, 0.10931778699159622, -0.03390537202358246, -0.06402090191841125, -0.037723854184150696, 0.029074762016534805, -0.04244937375187874, 0.015861935913562775, -0.09060660004615784, -0.05576631799340248, 0.022608749568462372, 0.008000322617590427, -0.02246999368071556, 0.022058147937059402, -0.025199150666594505, 0.03033953718841076, 0.010110795497894287, -0.018547305837273598, 0.016989590600132942, 0.0763101875782013, 0.0416206493973732, 0.08752065151929855, -0.012092760764062405, 0.03113519214093685, -0.03219042718410492, 0.012797970324754715, 0.013538764789700508, -0.02975240908563137, 0.03678983077406883, -0.005846079438924789, -0.01105553936213255, 0.03867030143737793, -0.02074146457016468, -0.011265086010098457, -0.023767532780766487, -0.009984560310840607, -0.023108458146452904, 0.012011353857815266, -0.010644523426890373, 0.0513606034219265, -0.02773405611515045, -0.00021139727323316038, 0.0009463390451855958, -0.03158264234662056, 0.051229290664196014, 0.04432860389351845, -0.003773423843085766, -0.04166809841990471, 0.02895456925034523, 0.03306133672595024, -0.015184277668595314, -0.00015773120685480535, -0.04412437602877617, -1.480350064753111e-08, -0.00869594607502222, 0.00013009380199946463, 0.01647108979523182, 0.05922885611653328, 0.04553878307342529, 0.03312685713171959, -0.09335850179195404, -0.03902722895145416, -0.020656151697039604, 0.012607559561729431, 0.06953950226306915, 0.07919187843799591, -0.07191091775894165, -0.0047850944101810455, 0.08800476789474487, 0.04758848249912262, -0.05219274014234543, -0.00752552505582571, -0.05771181359887123, -0.09295844286680222, -0.0045015024952590466, 0.001178039121441543, 0.02444724552333355, -0.06403965502977371, -0.0032275081612169743, -0.02796536311507225, -0.035407572984695435, 0.025036673992872238, -0.009852757677435875, 0.01325240544974804, 0.0011385205434635282, 0.17805425822734833, -0.036146294325590134, -0.007625906262546778, -0.032201241701841354, -0.042293235659599304, 0.004774454515427351, 0.02853180468082428, 0.07472298294305801, -0.014894150197505951, -0.056220654398202896, 0.02723579667508602, -0.011199447326362133, -0.1016678735613823, -0.019529249519109726, 0.027246462181210518, 0.03508107364177704, -0.0816056951880455, -0.0013377582654356956, -0.07635194808244705, -0.03995734825730324, 0.04078188166022301, 0.060128677636384964, 0.07254577428102493, 0.06967505812644958, 0.08909131586551666, 0.015957804396748543, -0.01487354189157486, -0.04674159735441208, -0.013411267660558224, 0.06513479351997375, 0.050905924290418625, 0.051483552902936935, 0.007092184852808714]\n"
     ]
    }
   ],
   "source": [
    "vectors = embeddings.embed_query('Hello')\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6da00dbd-e9c6-474e-ad4b-0c6b382afddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c0103bd-9659-4ad7-9e95-a62ec32c029a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_20884\\3644036516.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='ea1e06f6-38bd-480f-9cc5-505314b6d4e5', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:'),\n",
       " Document(id='83c11841-27af-476c-9895-538ea5885df6', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\")]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load documents\n",
    "loader = TextLoader('my_docs.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. split text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. Generate embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 4. store vectors in FAISS\n",
    "vectorstore = FAISS.from_documents(docs,embeddings)\n",
    "\n",
    "# 5. Retriever\n",
    "vectorstore.similarity_search('what is langchain?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed8957-6629-44ea-adb0-3f44d4727cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Retrieval QA Chain\n",
    "# -------------------------------\n",
    "combine retrieveal + generate \n",
    "-------------------  =========\n",
    "|                       |__ use llm ->generate an answer from retrieval documents\n",
    "|\n",
    "retrieve relevant documents from vectorstore\n",
    "\n",
    "retrieval + llm + prompt //chain =>from_chain_type() pipeline \n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "RetrievalQA.from_chain_type(llm=llm_object,retriever=retriever_object,return_source_documents=True)\n",
    "|\n",
    "query\n",
    "|\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dd35339-e42a-4d72-b17c-3c298690d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "490d496b-a16d-4615-b9fb-62b8f17bc4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Load documents\n",
    "loader = TextLoader('my_docs.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. split text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. Generate embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 4. store vectors in FAISS\n",
    "vectorstore = FAISS.from_documents(docs,embeddings)\n",
    "\n",
    "# 5. Retriever\n",
    "#vectorstore.similarity_search('what is langchain?')\n",
    "retriever_obj = vectorstore.as_retriever()\n",
    "\n",
    "# 6. use ollama model\n",
    "llm_obj = Ollama(model=\"gemma2:2b\")\n",
    "\n",
    "# 7. Build Retrieval QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriever_obj,\n",
    "                                       return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45c28832-0d7d-4795-a58d-3bfe635ecb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_20884\\2455835386.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain('what is langchain?')\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what is langchain?', 'result': 'LangChain is a framework for developing applications powered by large language models (LLMs). \\n', 'source_documents': [Document(id='ecc5f9d0-c347-4239-a198-457a98af90e1', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:'), Document(id='a36e5784-9eb8-45b2-b214-85da16337182', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\")]}\n"
     ]
    }
   ],
   "source": [
    "# 8. Ask query \n",
    "response = qa_chain('what is langchain?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "889c2ec7-5585-46ae-85de-f13c611f1039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework for developing applications powered by large language models (LLMs). \n",
      "\n",
      "\n",
      "Source:\n",
      "my_docs.txt\n",
      "my_docs.txt\n"
     ]
    }
   ],
   "source": [
    "print(response['result'])\n",
    "print(\"\\nSource:\")\n",
    "for var in response[\"source_documents\"]:\n",
    "    print(var.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53cea360-c91e-4533-a8ce-439f3d6f195e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what is langchain and how RAG is working?', 'result': \"LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies the development, productionization, and deployment of LLM-powered applications. \\n\\n**RAG** stands for **Reasoning About Graphs**, but it's not directly explained in the provided context.  Therefore, I don't have information to answer how RAG works within LangChain. \\n\", 'source_documents': [Document(id='ecc5f9d0-c347-4239-a198-457a98af90e1', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:'), Document(id='a36e5784-9eb8-45b2-b214-85da16337182', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\")]}\n",
      "LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies the development, productionization, and deployment of LLM-powered applications. \n",
      "\n",
      "**RAG** stands for **Reasoning About Graphs**, but it's not directly explained in the provided context.  Therefore, I don't have information to answer how RAG works within LangChain. \n",
      "\n",
      "\n",
      "Source:\n",
      "my_docs.txt\n",
      "my_docs.txt\n"
     ]
    }
   ],
   "source": [
    "# 8. Ask query \n",
    "response = qa_chain('what is langchain and how RAG is working?')\n",
    "print(response)\n",
    "print(response['result'])\n",
    "print(\"\\nSource:\")\n",
    "for var in response[\"source_documents\"]:\n",
    "    print(var.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "041630a9-4e9e-459d-8b25-442a245f076f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How to write simple python program?', 'result': \"This text describes LangChain and its various components, but it doesn't provide information on how to write a simple Python program. \\n\\n\\nTo learn how to write a simple Python program, you could refer to resources like:\\n\\n* **Official Python documentation:** [https://docs.python.org/3/tutorial/index.html](https://docs.python.org/3/tutorial/index.html)\\n* **W3Schools Python tutorial:** [https://www.w3schools.com/python/default.asp](https://www.w3schools.com/python/default.asp) \\n* **Codecademy Python courses:** [https://www.codecademy.com/catalog/language/python](https://www.codecademy.com/catalog/language/python)  \\n\\nLet me know if you want to explore specific aspects of Python programming or have any more questions! \\n\", 'source_documents': [Document(id='a36e5784-9eb8-45b2-b214-85da16337182', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\"), Document(id='ecc5f9d0-c347-4239-a198-457a98af90e1', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:')]}\n",
      "This text describes LangChain and its various components, but it doesn't provide information on how to write a simple Python program. \n",
      "\n",
      "\n",
      "To learn how to write a simple Python program, you could refer to resources like:\n",
      "\n",
      "* **Official Python documentation:** [https://docs.python.org/3/tutorial/index.html](https://docs.python.org/3/tutorial/index.html)\n",
      "* **W3Schools Python tutorial:** [https://www.w3schools.com/python/default.asp](https://www.w3schools.com/python/default.asp) \n",
      "* **Codecademy Python courses:** [https://www.codecademy.com/catalog/language/python](https://www.codecademy.com/catalog/language/python)  \n",
      "\n",
      "Let me know if you want to explore specific aspects of Python programming or have any more questions! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain('How to write simple python program?')\n",
    "print(response)\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "606da23a-8525-4d90-8d5f-b2a47835cf27",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'explain about langchain and tell me how to write simple python program?', 'result': 'LangChain is a framework designed for building applications powered by large language models (LLMs). Think of it as a toolbox that helps developers create sophisticated LLM-driven systems. \\n\\nHere\\'s what makes LangChain special:\\n\\n* **Simplifies Development:** It provides pre-built components and integrations, making it easier to get started with complex LLM tasks like answering questions, summarizing text, and even generating different creative content formats.\\n* **Enhances Statefulness:** LangChain allows you to build applications that can remember previous interactions (\"stateful agents\").  This means your application\\'s behavior will be more consistent and relevant over time. \\n* **Streamlined Production and Deployment:** It makes it easier to monitor, evaluate, and optimize your LLM applications after they\\'re built. This ensures reliable performance and a smoother user experience.\\n\\n**How to Write a Simple Python Program with LangChain**\\n\\nHere\\'s an example of how you could write a simple program using LangChain that leverages the power of OpenAI\\'s language model: \\n\\n```python\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import ConversationChain\\n\\n# Initialize the OpenAI LLM instance\\nllm = OpenAI(temperature=0.7)  \\n\\n# Create a conversation chain to interact with the LLM\\nconversation_chain = ConversationChain(llm=llm, verbose=True) \\n\\n# Start the conversation \\nprint(\"Let\\'s talk! What would you like me to do?\")\\nresponse = conversation_chain.run(input=\"You are an AI assistant who can help people with their daily tasks\")  \\nprint(\"Assistant:\", response)\\n```\\n\\n\\n**Explanation:**\\n\\n1. **Import Libraries:** We start by importing the necessary libraries from LangChain. `OpenAI` allows us to interact with OpenAI\\'s language model, and `ConversationChain` helps us create conversational AI systems. \\n2. **Instantiate LLM:** We create an instance of `OpenAI` with specific settings, like `temperature=0.7`, for controlling the creativity of the generated text (higher values mean more creative output).\\n3. **Create Conversation Chain:** We build a `ConversationChain` using our LLM and set up some verbose logging to see what\\'s happening within the system.  This helps you understand how your program interacts with the model. \\n4. **Start a Conversation:** The code then prompts the user for input (\"Let\\'s talk!\").  The `conversation_chain.run` method initiates the interaction, and the result is stored in the `response` variable, which we print to the console.\\n\\n\\n\\n**Keep Learning:** \\n\\nLangChain offers many more capabilities like integration with various LLM models (Google, Hugging Face), building complex applications with multiple LLMs or agents, and deploying your LLM-powered applications to different platforms!  There\\'s much more to explore within LangChain\\'s documentation: [https://langchain.com/](https://langchain.com/) \\n\\n\\n\\nLet me know if you have any other questions about LangChain.\\n', 'source_documents': [Document(id='ecc5f9d0-c347-4239-a198-457a98af90e1', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:'), Document(id='a36e5784-9eb8-45b2-b214-85da16337182', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\")]}\n",
      "LangChain is a framework designed for building applications powered by large language models (LLMs). Think of it as a toolbox that helps developers create sophisticated LLM-driven systems. \n",
      "\n",
      "Here's what makes LangChain special:\n",
      "\n",
      "* **Simplifies Development:** It provides pre-built components and integrations, making it easier to get started with complex LLM tasks like answering questions, summarizing text, and even generating different creative content formats.\n",
      "* **Enhances Statefulness:** LangChain allows you to build applications that can remember previous interactions (\"stateful agents\").  This means your application's behavior will be more consistent and relevant over time. \n",
      "* **Streamlined Production and Deployment:** It makes it easier to monitor, evaluate, and optimize your LLM applications after they're built. This ensures reliable performance and a smoother user experience.\n",
      "\n",
      "**How to Write a Simple Python Program with LangChain**\n",
      "\n",
      "Here's an example of how you could write a simple program using LangChain that leverages the power of OpenAI's language model: \n",
      "\n",
      "```python\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.chains import ConversationChain\n",
      "\n",
      "# Initialize the OpenAI LLM instance\n",
      "llm = OpenAI(temperature=0.7)  \n",
      "\n",
      "# Create a conversation chain to interact with the LLM\n",
      "conversation_chain = ConversationChain(llm=llm, verbose=True) \n",
      "\n",
      "# Start the conversation \n",
      "print(\"Let's talk! What would you like me to do?\")\n",
      "response = conversation_chain.run(input=\"You are an AI assistant who can help people with their daily tasks\")  \n",
      "print(\"Assistant:\", response)\n",
      "```\n",
      "\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. **Import Libraries:** We start by importing the necessary libraries from LangChain. `OpenAI` allows us to interact with OpenAI's language model, and `ConversationChain` helps us create conversational AI systems. \n",
      "2. **Instantiate LLM:** We create an instance of `OpenAI` with specific settings, like `temperature=0.7`, for controlling the creativity of the generated text (higher values mean more creative output).\n",
      "3. **Create Conversation Chain:** We build a `ConversationChain` using our LLM and set up some verbose logging to see what's happening within the system.  This helps you understand how your program interacts with the model. \n",
      "4. **Start a Conversation:** The code then prompts the user for input (\"Let's talk!\").  The `conversation_chain.run` method initiates the interaction, and the result is stored in the `response` variable, which we print to the console.\n",
      "\n",
      "\n",
      "\n",
      "**Keep Learning:** \n",
      "\n",
      "LangChain offers many more capabilities like integration with various LLM models (Google, Hugging Face), building complex applications with multiple LLMs or agents, and deploying your LLM-powered applications to different platforms!  There's much more to explore within LangChain's documentation: [https://langchain.com/](https://langchain.com/) \n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other questions about LangChain.\n",
      "\n",
      "\n",
      "Source:\n",
      "my_docs.txt\n",
      "my_docs.txt\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain('explain about langchain and tell me how to write simple python program?')\n",
    "print(response)\n",
    "print(response['result'])\n",
    "print(\"\\nSource:\")\n",
    "for var in response[\"source_documents\"]:\n",
    "    print(var.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd535a8b-7a8c-4f84-9737-5bb040b72217",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'explain #@$#@ASDF@4 info?', 'result': 'This context does not contain the information necessary to answer \"explain #@$#@ASDF@4 info?\" \\n\\nPlease provide me with more context or information.  \\n', 'source_documents': [Document(id='ecc5f9d0-c347-4239-a198-457a98af90e1', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:'), Document(id='a36e5784-9eb8-45b2-b214-85da16337182', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\")]}\n",
      "This context does not contain the information necessary to answer \"explain #@$#@ASDF@4 info?\" \n",
      "\n",
      "Please provide me with more context or information.  \n",
      "\n",
      "\n",
      "Source:\n",
      "my_docs.txt\n",
      "my_docs.txt\n"
     ]
    }
   ],
   "source": [
    "# 8. Ask query \n",
    "response = qa_chain('explain #@$#@ASDF@4 info?')\n",
    "print(response)\n",
    "print(response['result'])\n",
    "print(\"\\nSource:\")\n",
    "for var in response[\"source_documents\"]:\n",
    "    print(var.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbbffe8c-452d-4036-bac5-c43ab9d5130f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I likes to read python book')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "PromptTemplate.from_template('I likes to read python book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c3a2f90-51e9-4a77-8118-9cbb1cec4ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['myvar'], input_types={}, partial_variables={}, template='I likes to read {myvar} book')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate.from_template('I likes to read {myvar} book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdeb16d2-9d81-468e-ba33-2a06bbccb3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read java book'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate.from_template('I likes to read {myvar} book').format(myvar=\"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b4faff1-e50a-44bf-aef6-8918ad7d2a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read GenAI book'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate.from_template('I likes to read {myvar} book').format(myvar=\"GenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32f41dcd-cae2-4b23-aa21-036463b77efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read GenAI book'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_obj = PromptTemplate.from_template('I likes to read {myvar} book')\n",
    "prompt_obj.format(myvar='GenAI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ede338-41da-47b3-bb39-ed2899a66fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. prompt\n",
    "    - text => give to the model\n",
    "2. context\n",
    "    - constraints \n",
    "3. role prompting\n",
    "    - assign model a role \n",
    "4. structed output\n",
    "    - fixed format(json,table,bullet points,summary, keywords..)\n",
    "5. Constraints - limit\n",
    "        - ex: 150 words 5lines 3 rows \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e9393b3-54e2-4cd0-bdd8-ac5d7ac1dac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain short story about birds in english writen by Mr.abc'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_obj = PromptTemplate.from_template('Explain short story about {topic} in {lng} writen by {author}')\n",
    "\n",
    "prompt_obj.format(topic='birds',lng='english',author='Mr.abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3a6a7a8-914e-458f-9ac4-73e2d8955c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Your are helpful AI assistant'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Question:{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# role and content\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "#help(ChatPromptTemplate.from_messages)\n",
    "\n",
    "ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Your are helpful AI assistant\"),\n",
    "    (\"user\",\"Question:{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "010eff96-9d1c-4903-8706-66487bc1022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Your are helpful AI assistant\"),\n",
    "    (\"user\",\"Question:{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a07d4135-3b52-438e-aeca-4db319eeafce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Your are helpful AI assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Question:what is langchain?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt.format_messages(question=\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f82c3fd-366e-4282-ac43-8a0e8714930c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Your are helpful AI assistant'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Question:{question}'), additional_kwargs={})])\n",
       "| Ollama(model='gemma2:2b')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_llm_obj = Ollama(model=\"gemma2:2b\")\n",
    "chat_prompt|my_llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d95fc841-0fff-4cdf-856b-1f19ac0bc91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Gen AI\" stands for **Generative Artificial Intelligence**. \\n\\nHere\\'s a breakdown:\\n\\n**What it means:**\\n\\n* **Generative:**  It\\'s about creating new things - not just analyzing or classifying existing information. Gen AI can generate content like text, images, music, code, and even 3D models based on what you give it as input (e.g., a description of a sunset).\\n* **Artificial Intelligence:** This refers to systems that learn from data and perform tasks that typically require human intelligence, such as:\\n    * Understanding natural language\\n    * Recognizing objects and patterns\\n    * Making predictions\\n\\n**How it works:**\\n\\nGen AI uses complex algorithms trained on massive datasets. Think of it like a super-smart computer \"learning\" by analyzing tons of examples and figuring out the best way to create something similar. \\n\\n**Examples of Gen AI in action:**\\n\\n* **ChatGPT:** This popular chatbot can write stories, answer your questions, even generate different kinds of creative content.\\n* **DALL-E 2:** Generates images from text descriptions (\"a picture of a cat wearing a hat\"). \\n* **GitHub Copilot:** Helps programmers write code faster and more efficiently by suggesting completions based on context and project requirements.\\n\\n**Overall, Gen AI is a revolutionary technology that has the potential to transform many fields and aspects of our lives.**\\n\\n\\nLet me know if you have any other questions! 😊 \\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_prompt|my_llm_obj\n",
    "chain.invoke({'question':'what is gen AI'}) # chainObject.inovkeMethod({'placeholder':'userQuery'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba14e46a-5348-4e7b-9d77-6e4d3362094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"don't know\"),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "my_llm_obj = Ollama(model=\"gemma2:2b\")\n",
    "\n",
    "chain = prompt|my_llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b20034e-765d-4999-95d7-074fda352ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = chain.invoke({'question': 'what is python?'})\n",
    "r2 = chain.invoke({'question': 'asdfsfsfasdfas'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b526959e-e263-408f-949d-e9ceb3772fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    }
   ],
   "source": [
    "print(bool(r1),bool(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "20ef0003-b812-4d1f-8660-16b013860c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a popular, versatile programming language known for its readability and beginner-friendliness. Here's a breakdown of what makes Python special:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **High-Level Language:** Python simplifies complex tasks by abstracting away low-level details, making it easier to write code. \n",
      "* **Interpreted:**  Unlike compiled languages like C++, you don't need to translate the code into machine instructions before execution – Python runs line by line directly. This makes development faster and more interactive.\n",
      "* **Dynamically Typed:** You don't have to explicitly define data types; Python infers them based on your code. This reduces the amount of boilerplate code. \n",
      "* **Object-Oriented:**  Python supports object-oriented programming (OOP), a structure for organizing code into reusable modules and objects.\n",
      "\n",
      "**Why is it Popular?**\n",
      "\n",
      "* **Versatility:** Python shines in various domains, including:\n",
      "    * **Web Development:** Building websites, web apps, and backend systems (using frameworks like Django and Flask).\n",
      "    * **Data Science & Machine Learning:**  Analyzing data, building predictive models, and creating AI solutions (with libraries like Pandas, Scikit-learn, TensorFlow).\n",
      "    * **Scripting and Automation:** Automating tasks, from system administration to web scraping. \n",
      "    * **Game Development:** Creating games through libraries like Pygame.\n",
      "\n",
      "**Getting Started with Python:**\n",
      "\n",
      "1.  **Install Python:** Download the latest version from [https://www.python.org/](https://www.python.org/).\n",
      "2.  **Write Code:** Use a text editor (like Notepad, Sublime Text, VS Code) to create files with `.py` extension. \n",
      "3.  **Run your code:** Open a terminal or command prompt, navigate to the directory where you saved your file, and run `python filename.py`.\n",
      "\n",
      "**Where can you learn more?**\n",
      "* **Python documentation:** [https://docs.python.org/](https://docs.python.org/) \n",
      "* **Online courses and tutorials:**  Coursera, Codecademy, Udemy offer comprehensive resources. \n",
      "\n",
      "\n",
      "Let me know if you'd like to explore specific aspects of Python or have further questions! \n"
     ]
    }
   ],
   "source": [
    "print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b59c45d6-5f6f-4d00-9759-489bb2f65d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you're trying to communicate something, but the input is just a bunch of random letters.  \n",
      "\n",
      "Could you try rephrasing your question or providing more information? \n",
      "\n",
      "For example, are you wondering about something specific, like:\n",
      "\n",
      "* **A translation:** \"What does 'asdfsfsfasdfas' mean in another language?\"\n",
      "* **Something technical:** \"I'm having trouble with my system and don't know what to do. Can you help me troubleshoot?\" \n",
      "* **Getting started with a task:** \"Help me understand how to start using this website.\"\n",
      "\n",
      "\n",
      "The more information you give, the better I can understand and help you! 😄 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b808a9-817a-46d2-aa45-da6ace55f584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72949979-b531-480d-8cb0-21762fa4d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"\"),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "my_llm_obj = Ollama(model=\"gemma2:2b\")\n",
    "\n",
    "chain = prompt|my_llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36bb1319-5798-48bf-91de-508cc2cb2e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you might be typing a question, but the input is jumbled.  Can you please re-type your question clearly so I can help? 😊 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({'question':'xyzafdsf?'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "48d83e29-6ab7-4d47-b2df-39f3adf8e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"Say don't know\"),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "my_llm_obj = Ollama(model=\"gemma2:2b\")\n",
    "\n",
    "chain = prompt|my_llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a9b25b4-8d86-4159-9130-d52f916600dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I say \"Don't know\" \n",
      "\n",
      "Is there anything else I can help you with? 😊  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({'question':'xyzafdsf?'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d0f4219-6c24-48f4-a2fd-b58a73e19512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get standard output format\n",
    "# ---------------------------------\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "becc6d95-6e6a-460b-9819-3eddb37373d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenAI, or **Generative Artificial Intelligence**, is a fascinating area of AI focused on creating new content rather than just analyzing or manipulating existing information.  Think of it as an AI artist, writer, musician, or chef!\n",
      "\n",
      "Here's how it works in simple terms:\n",
      "\n",
      "* **Learning:** GenAI learns from tons of data (text, images, code, etc.) to understand patterns and relationships.\n",
      "* **Generating:** Based on this learning, it can generate something new – a poem, an image, a piece of music, even a design for a product! \n",
      "\n",
      "**Examples of GenAI in action:**\n",
      "\n",
      "* **Text Generation:** Creating stories, articles, scripts, summaries, emails, and more\n",
      "* **Image Generation:** Generating realistic images based on descriptions (think Midjourney or DALL-E 2)\n",
      "* **Music Generation:** Composing melodies, harmonies, or even entire songs.\n",
      "* **Code Generation:** Helping programmers write code faster and easier.\n",
      "\n",
      "**It's important to note:**\n",
      "\n",
      "* **Creativity vs. Imitation:** While GenAI can create original content, it often relies on existing data as inspiration. It's still learning and improving how to think creatively! \n",
      "* **Ethical Considerations:**  GenAI raises important questions about ownership, copyright, and the potential impact of AI-generated content on society.\n",
      "\n",
      "\n",
      "**In short, GenAI is a powerful tool that's changing how we create and interact with information in new and exciting ways!** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = StrOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"\"),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "my_llm_obj = Ollama(model=\"gemma2:2b\")\n",
    "\n",
    "chain = prompt|my_llm_obj|output\n",
    "response = chain.invoke({'question':'what is genAI?'})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "69272a71-7874-44e8-9546-327fb016fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"Your are a helpful AI assistant\"),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "my_llm_obj = Ollama(model=\"gemma2:2b\")\n",
    "output = StrOutputParser()\n",
    "chain = prompt|my_llm_obj|output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d712344-09eb-4335-9f8e-02a4883b5c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly, let's explore how to calculate factorials in Python.\n",
      "\n",
      "**Understanding Factorial**\n",
      "\n",
      "The factorial of a non-negative integer *n*, denoted by *n!*, is the product of all positive integers less than or equal to *n*. For example:\n",
      "\n",
      "5! = 5 * 4 * 3 * 2 * 1 = 120\n",
      "\n",
      "**Python Code for Factorial Calculation**\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "  \"\"\"Calculates the factorial of a non-negative integer.\n",
      "\n",
      "  Args:\n",
      "      n: The non-negative integer to calculate the factorial of.\n",
      "\n",
      "  Returns:\n",
      "      The factorial of n, if n is non-negative. \n",
      "      Raises ValueError if n is negative.\n",
      "  \"\"\"\n",
      "  if n < 0:\n",
      "    raise ValueError(\"Factorial is not defined for negative numbers.\")\n",
      "  elif n == 0:\n",
      "    return 1  # Factorial of 0 is 1\n",
      "  else:\n",
      "    result = 1\n",
      "    for i in range(1, n+1):\n",
      "      result *= i\n",
      "    return result\n",
      "\n",
      "# Get input from the user\n",
      "num = int(input(\"Enter a non-negative integer: \"))\n",
      "\n",
      "# Calculate and print the factorial\n",
      "print(f\"The factorial of {num} is {factorial(num)}\") \n",
      "```\n",
      "\n",
      "\n",
      "**Explanation**\n",
      "\n",
      "1. **`factorial(n)` Function:**\n",
      "   - Takes an integer `n` as input.\n",
      "   - Checks if `n` is negative; if so, it raises a `ValueError`.  \n",
      "   - If `n` is 0, returns 1 (base case for factorial).\n",
      "   - If `n` is positive, iteratively calculates the product of numbers from 1 to `n` using a loop.\n",
      "\n",
      "2. **User Input:**\n",
      "   - Prompts the user to enter a non-negative integer using `input()`.\n",
      "   - The entered value is converted to an integer using `int()`.\n",
      "\n",
      "3. **Calculation and Output:**\n",
      "   - Calls the `factorial()` function with the input number (`num`). \n",
      "   - Prints the result of the factorial calculation formatted nicely.\n",
      "\n",
      "\n",
      "Let me know if you have any further questions or would like me to elaborate on any specific part! \n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({'question':'How to write factorial program in python?'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "791c8831-d53c-443b-b0f3-bd3f0ba75c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you might have made a typo!  \"fasfsfasfas\" doesn't mean anything. \n",
      "\n",
      "Is there something specific you meant to ask? 😊 I can help with lots of things, just let me know! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({'question':'what is fasfsfasfas?'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f563871-34ed-444d-976e-dc29754fe7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4bcbc5c6-367f-4aad-be91-00ead34262da",
   "metadata": {},
   "outputs": [],
   "source": [
    "groqK = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bee6134b-2940-45e9-8f28-12ef187a9586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000270D965F8C0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000270D972C590>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "ChatGroq(model='gemma2-9b-it',groq_api_key=groqK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3d6d5b35-9133-4325-b42c-43c4ae5932e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = ChatGroq(model='gemma2-9b-it',groq_api_key=groqK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f886df96-942f-4c24-b6a2-96d0a8701ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c199be1c-4fc0-4de3-a05b-72f5752e64b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Karthik! \\n\\nWhat can I help you with today? 😊  Are you teaching something specific, or just here to chat? \\n\\n\\nLet me know if you have any questions or if there's anything I can do for you. 👍  \\n\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "my_llm_obj.invoke([HumanMessage(content=\"Hello my name is karthik and i am instructor\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e1135297-2242-4054-9f4f-7fc0dd965a45",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'The model `gemma2-9b-it` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm_model\u001b[38;5;241m.\u001b[39minvoke([HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello my name is karthik and i am instructor\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    391\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    392\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 395\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    396\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    397\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    398\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    399\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    400\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    401\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    402\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    403\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    404\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    405\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1025\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m   1018\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1023\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m   1024\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m-> 1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:842\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    841\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 842\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    843\u001b[0m                 m,\n\u001b[0;32m    844\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    845\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    846\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    847\u001b[0m             )\n\u001b[0;32m    848\u001b[0m         )\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    850\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1091\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1091\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m   1092\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1093\u001b[0m     )\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1095\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:557\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    553\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    556\u001b[0m }\n\u001b[1;32m--> 557\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, params)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:368\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    231\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    370\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    371\u001b[0m             {\n\u001b[0;32m    372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    373\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    374\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: exclude_domains,\n\u001b[0;32m    375\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_domains,\n\u001b[0;32m    379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    380\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    386\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    387\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_format,\n\u001b[0;32m    389\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    390\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m: search_settings,\n\u001b[0;32m    391\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    392\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    393\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    394\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    395\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    396\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    397\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    398\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    399\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    400\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    401\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    402\u001b[0m             },\n\u001b[0;32m    403\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    404\u001b[0m         ),\n\u001b[0;32m    405\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    406\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    407\u001b[0m         ),\n\u001b[0;32m    408\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    409\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    410\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    411\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1232\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1220\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1228\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1229\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1230\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1231\u001b[0m     )\n\u001b[1;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1034\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1031\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1033\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1034\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'The model `gemma2-9b-it` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}"
     ]
    }
   ],
   "source": [
    "llm_model.invoke([HumanMessage(content=\"Hello my name is karthik and i am instructor\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d7cee-b363-4bf6-820f-5ca1598456cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
