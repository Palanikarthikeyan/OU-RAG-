{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f69637-e1c7-4954-a7db-258c50e66f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Recap\n",
    "======\n",
    "NLP - tokenziation \n",
    "langchain \n",
    " - model - pre-trained by ML and DL\n",
    " - Ollama \n",
    "      |-> download to local m/c -> act a process(systemprocess)\n",
    " - chat bot QA local m/c \n",
    "\n",
    "[Document-load] ->[chunk ]  -> [embedding] ->[vector-13313] ->[DataBase-vector] \n",
    " raw data                                                           \n",
    "=================\n",
    "\n",
    "HuggingFace \n",
    "- embedding\n",
    "Ollama \n",
    "- llm\n",
    "|\n",
    "groq - LPU\n",
    "|\n",
    "prompt template\n",
    "|\n",
    "oracle23 ai \n",
    "-----------\n",
    "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\n",
    "\n",
    "1. Go to commandline/terminal (for windows user => GitBash)\n",
    "2. vi .env\n",
    "\n",
    "HF_TOKEN=\"<paste>\"\n",
    ":wq <== save and exit\n",
    "3. source .env\n",
    "4. env|grep -i HF_TOKEN \n",
    "-------------------------------------------------------//commandline/terminal \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "600d1bff-b416-4455-b6a6-193b7dd5cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc8ee4d-156d-4440-b4fd-12946efcfa60",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'ALLUSERSPROFILE': 'C:\\\\ProgramData',\n",
       "        'APPDATA': 'C:\\\\Users\\\\karth\\\\AppData\\\\Roaming',\n",
       "        'COMMONPROGRAMFILES': 'C:\\\\Program Files\\\\Common Files',\n",
       "        'COMMONPROGRAMFILES(X86)': 'C:\\\\Program Files (x86)\\\\Common Files',\n",
       "        'COMMONPROGRAMW6432': 'C:\\\\Program Files\\\\Common Files',\n",
       "        'COMPUTERNAME': 'PAKA',\n",
       "        'COMSPEC': 'C:\\\\WINDOWS\\\\system32\\\\cmd.exe',\n",
       "        'CONDA_PREFIX': 'C:\\\\ProgramData\\\\anaconda3',\n",
       "        'DRIVERDATA': 'C:\\\\Windows\\\\System32\\\\Drivers\\\\DriverData',\n",
       "        'HADOOP_HOME': 'C:\\\\hadoop',\n",
       "        'HOMEDRIVE': 'C:',\n",
       "        'HOMEPATH': '\\\\Users\\\\karth',\n",
       "        'IGCCSVC_DB': 'AQAAANCMnd8BFdERjHoAwE/Cl+sBAAAA/jMmQiI+3Uu0CJ+M30qPdQQAAAACAAAAAAAQZgAAAAEAACAAAACzWqQ/qnf6OBa6nybKt6MvWAz1MD1IZEfDM6Kfrw93/wAAAAAOgAAAAAIAACAAAAA9i/96dUV1FlnlkFAuDr31D8Mx+C/bU5eWQLMUw0VBTGAAAAD8e9JcM/pqtNvMIPE4lL8YySH4QAhZhJ5ePuvvraESD1Ckvq21kYdpo2hCC2BLPig1b8qSfcU7gEU+SJov/zU5aaexkcnOZn5wXQ53DLd2cD6JgRNVOVxAd7rMz1YEesNAAAAAgOebOXQTGlWcI/yrXwHJFXuiSbxGLXt8GfOCisstAVBzijgjtPxCmVHoAKe53HFFOMyLyUOdgHvAVCSB5yFMvw==',\n",
       "        'IPY_INTERRUPT_EVENT': '1584',\n",
       "        'JAVA_HOME': 'C:\\\\Java\\\\',\n",
       "        'JPY_INTERRUPT_EVENT': '1584',\n",
       "        'JPY_PARENT_PID': '2152',\n",
       "        'JPY_SESSION_NAME': 'C:\\\\Users\\\\karth\\\\Untitled11.ipynb',\n",
       "        'LOCALAPPDATA': 'C:\\\\Users\\\\karth\\\\AppData\\\\Local',\n",
       "        'LOGONSERVER': '\\\\\\\\PAKA',\n",
       "        'NUMBER_OF_PROCESSORS': '16',\n",
       "        'OLLAMA_HOST': 'http://127.0.0.1:11434',\n",
       "        'OLLAMA_MODELS': 'gemma3',\n",
       "        'ONEDRIVE': 'C:\\\\Users\\\\karth\\\\OneDrive',\n",
       "        'ONEDRIVECONSUMER': 'C:\\\\Users\\\\karth\\\\OneDrive',\n",
       "        'OS': 'Windows_NT',\n",
       "        'PATH': 'C:\\\\ProgramData\\\\anaconda3;C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\mingw-w64\\\\bin;C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\usr\\\\bin;C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\bin;C:\\\\ProgramData\\\\anaconda3\\\\Scripts;C:\\\\app\\\\karth\\\\product\\\\23ai\\\\dbhomeFree\\\\bin;C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\;C:\\\\Java\\\\bin;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\WINDOWS\\\\System32\\\\OpenSSH\\\\;C:\\\\Program Files\\\\dotnet\\\\;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\ngrok;C:\\\\Java\\\\\\\\bin;%SPARK_HOME%\\\\bin;C:\\\\hadoop\\\\bin;C:\\\\Program Files\\\\MiKTeX\\\\miktex\\\\bin\\\\x64\\\\;C:\\\\Strawberry\\\\c\\\\bin;C:\\\\Strawberry\\\\perl\\\\site\\\\bin;C:\\\\Strawberry\\\\perl\\\\bin;C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Scripts;C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Scripts\\\\;C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\;C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Launcher\\\\;C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Programs\\\\Ollama;C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\bin;C:\\\\Users\\\\karth\\\\AppData\\\\Roaming\\\\Python\\\\Python313\\\\Scripts',\n",
       "        'PATHEXT': '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC',\n",
       "        'PROCESSOR_ARCHITECTURE': 'AMD64',\n",
       "        'PROCESSOR_IDENTIFIER': 'Intel64 Family 6 Model 186 Stepping 2, GenuineIntel',\n",
       "        'PROCESSOR_LEVEL': '6',\n",
       "        'PROCESSOR_REVISION': 'ba02',\n",
       "        'PROGRAMDATA': 'C:\\\\ProgramData',\n",
       "        'PROGRAMFILES': 'C:\\\\Program Files',\n",
       "        'PROGRAMFILES(X86)': 'C:\\\\Program Files (x86)',\n",
       "        'PROGRAMW6432': 'C:\\\\Program Files',\n",
       "        'PSMODULEPATH': 'C:\\\\Program Files\\\\WindowsPowerShell\\\\Modules;C:\\\\WINDOWS\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules;',\n",
       "        'PUBLIC': 'C:\\\\Users\\\\Public',\n",
       "        'PYSPARK_DRIVER_PYTHON': 'C:\\\\\\\\ProgramData\\\\\\\\anaconda3\\\\\\\\python.exe',\n",
       "        'PYSPARK_PYTHON': 'C:\\\\\\\\ProgramData\\\\\\\\anaconda3\\\\\\\\python.exe',\n",
       "        'PYTHONPATH': 'C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310',\n",
       "        'SUBPROCESS_BAGGAGE': 'sentry-trace_id=a2dd0a5ec00a475194a836ec9ebbb2a3,sentry-environment=local_production,sentry-release=4.20.0,sentry-public_key=f175bff95ca4ae57c532f2bf67cbaa63',\n",
       "        'SUBPROCESS_SENTRY_TRACE': 'a2dd0a5ec00a475194a836ec9ebbb2a3-9f641914d1c205a7',\n",
       "        'SYSTEMDRIVE': 'C:',\n",
       "        'SYSTEMROOT': 'C:\\\\WINDOWS',\n",
       "        'TEMP': 'C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Temp',\n",
       "        'TMP': 'C:\\\\Users\\\\karth\\\\AppData\\\\Local\\\\Temp',\n",
       "        'USERDOMAIN': 'PAKA',\n",
       "        'USERDOMAIN_ROAMINGPROFILE': 'PAKA',\n",
       "        'USERNAME': 'karth',\n",
       "        'USERPROFILE': 'C:\\\\Users\\\\karth',\n",
       "        'VBOX_MSI_INSTALL_PATH': 'C:\\\\Program Files\\\\Oracle\\\\VirtualBox\\\\',\n",
       "        'WINDIR': 'C:\\\\WINDOWS',\n",
       "        'ZES_ENABLE_SYSMAN': '1',\n",
       "        'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'FORCE_COLOR': '1',\n",
       "        'CLICOLOR_FORCE': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://matplotlib_inline.backend_inline'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11cba35d-cbc4-4584-ba30-65e80ac6e928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29f9be75-b125-4e0f-974a-66f04c3b2996",
   "metadata": {},
   "source": [
    "os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a63baec3-9d91-4fd1-a383-def04ce7125d",
   "metadata": {},
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33e7e7c8-5b88-4f8b-b154-656be6d391d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d={}\n",
    "d['K1']='V1' # adding new data to an existing dict\n",
    "\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN') # like dict style adding new data(key:value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9062ab2f-024b-4afb-b9d6-7fef5ad55a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0a5aebe-e2cf-4ac1-be00-0176a1f8695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_huggingface.embeddings.huggingface.HuggingFaceEmbeddings'>\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(type(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adb15439-135c-4f3b-aafe-c158587bc970",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0627717599272728, 0.05495879426598549, 0.052164845168590546, 0.0857900083065033, -0.0827488973736763, -0.07457297295331955, 0.06855470687150955, 0.018396394327282906, -0.08201129734516144, -0.03738484904170036, 0.012124893255531788, 0.0035183229483664036, -0.004134293645620346, -0.04378441721200943, 0.021807299926877022, -0.00510272104293108, 0.019546564668416977, -0.0423487089574337, -0.11035966873168945, 0.005424527917057276, -0.055734783411026, 0.02805238962173462, -0.02315872348845005, 0.028481375426054, -0.05370955914258957, -0.052601538598537445, 0.03393925353884697, 0.045388635247945786, 0.023718422278761864, -0.07312081754207611, 0.05477774888277054, 0.0170473363250494, 0.08136039972305298, -0.0028627056162804365, 0.011958062648773193, 0.07355856150388718, -0.09423744678497314, -0.0813620463013649, 0.04001539200544357, 0.0006921913009136915, -0.01339334063231945, -0.05453811213374138, 0.005151401273906231, -0.026139769703149796, 0.0368071012198925, -0.03395958989858627, 0.021093212068080902, 0.055948879569768906, 0.05778130516409874, -0.0054183765314519405, -0.06841319799423218, -0.09023699164390564, -0.04286673292517662, 0.023652682080864906, 0.12149356305599213, 0.032392844557762146, -0.02265944518148899, -0.02328176610171795, 0.04886598512530327, -0.059351544827222824, -0.03406606614589691, 0.03596258535981178, -0.08115777373313904, -0.021979473531246185, 0.013187813572585583, -0.04585552215576172, -0.07041982561349869, -0.05260329321026802, -0.04802926629781723, -0.07454610615968704, -0.028731804341077805, 0.012831883504986763, -0.042921729385852814, 0.0053663537837564945, -0.03909760341048241, 0.015919268131256104, 0.020243527367711067, 0.0052666617557406425, 0.009581327438354492, -0.0047676158137619495, 0.04808773100376129, -0.07309659570455551, -0.05034444108605385, 0.009653042070567608, 0.013496600091457367, 0.00043035211274400353, 0.019725199788808823, 0.062495484948158264, -0.01822715625166893, 0.03109716810286045, -0.08885394781827927, 0.04813242703676224, 0.026074277237057686, -0.011494041420519352, -0.09936171025037766, -0.027338232845067978, 0.07621528953313828, -0.002096540527418256, -0.12358873337507248, 0.29806533455848694, 0.05630640685558319, 0.0784800797700882, 0.01104352530092001, 0.052400071173906326, -0.0011880238307639956, 0.000965684128459543, -0.05432729423046112, 0.02676473930478096, -0.012194258160889149, 0.010717053897678852, -3.707817450049333e-05, -0.035565707832574844, -0.03208553418517113, 0.016168801113963127, 0.08674969524145126, 0.018139464780688286, -0.01101755816489458, 0.04974030703306198, 0.02258104830980301, -2.2351243387674913e-05, 0.014939067885279655, -0.011176216416060925, 0.005659336689859629, -0.004114812705665827, 0.003182594431564212, 0.02386500872671604, 0.01805107854306698, -5.850476159002819e-33, 0.05610707402229309, -0.03449849784374237, 0.033111825585365295, 0.1677229106426239, -0.031038135290145874, -0.0048353467136621475, -0.06106499955058098, -0.06271100044250488, 0.027426589280366898, 0.06364015489816666, 0.043340425938367844, 0.06082936003804207, -0.018160779029130936, 0.04280414804816246, 0.019024066627025604, 0.08797787874937057, -0.039132826030254364, 0.04414263367652893, -0.004941055551171303, 0.051052697002887726, -0.05431916192173958, 0.011173113249242306, 0.0266990028321743, 0.07509659975767136, 0.048690132796764374, -0.04354275390505791, 0.01333867758512497, -0.10262981057167053, 0.05215669423341751, 0.02223237417638302, -0.029741793870925903, -0.04227066785097122, 0.022984562441706657, 0.03957443684339523, 0.009094778448343277, 0.020864685997366905, 0.00509262178093195, -0.06276409327983856, -0.05021766945719719, -0.005225018598139286, -0.05349820479750633, 0.029773466289043427, 0.021753670647740364, -0.020678048953413963, 0.023866144940257072, 0.005806638393551111, -0.0012137835146859288, 0.022660311311483383, 0.0031567111145704985, 0.030960429459810257, -0.05296808481216431, 0.01867694966495037, -0.1401534080505371, 0.04147820174694061, -0.010278256610035896, -0.011593146249651909, -0.033458251506090164, -0.05058891698718071, 0.04686781391501427, 0.024715127423405647, 0.03349681571125984, 0.11170760542154312, -0.0403486005961895, -0.0042844959534704685, -0.08074361085891724, -0.05611216649413109, 0.03833862021565437, 0.011508360505104065, 0.06873797625303268, -0.03811735659837723, -0.04598833993077278, -0.016439592465758324, 0.024412987753748894, 0.011722046881914139, 0.008168509230017662, 0.039036914706230164, 0.026186540722846985, 0.01048184558749199, 0.04286017641425133, -0.04638158529996872, 0.006481093820184469, 0.044423412531614304, -0.01883763261139393, 0.00726337032392621, 0.05617952719330788, 0.053477976471185684, -0.02148376777768135, -0.084485724568367, -0.012813972309231758, -0.039475537836551666, -0.05802653357386589, 0.03133402392268181, 0.045417316257953644, 0.011839081533253193, -0.017906315624713898, 4.586215086962156e-33, 0.13150791823863983, 0.07932813465595245, -0.09495951235294342, -0.024472175166010857, -0.055816203355789185, -0.009145256131887436, -0.032090410590171814, 0.11379317939281464, -0.14441627264022827, 0.00850959587842226, 0.030667033046483994, -0.01241487916558981, 0.07017052918672562, 0.028179822489619255, 0.04082208126783371, 0.019590837880969048, 0.14295044541358948, 0.0568699836730957, -0.040122076869010925, -0.017843913286924362, -0.06138443946838379, 0.0008760516648180783, -0.054906200617551804, -0.007102516014128923, -0.0004941486986353993, -0.013119390234351158, -0.0022669434547424316, 0.05856667086482048, -0.09971513599157333, -0.025532379746437073, 0.07856457680463791, 0.020517690107226372, -0.004633414093405008, 0.030302615836262703, 0.016833249479532242, 0.09143757820129395, 0.016769731417298317, -0.07980289310216904, 0.04251027852296829, -0.08439735323190689, -0.0238590519875288, 0.04701947048306465, 0.0024148565717041492, 0.10931778699159622, -0.03390537202358246, -0.06402090191841125, -0.037723854184150696, 0.029074762016534805, -0.04244937375187874, 0.015861935913562775, -0.09060660004615784, -0.05576631799340248, 0.022608749568462372, 0.008000322617590427, -0.02246999368071556, 0.022058147937059402, -0.025199150666594505, 0.03033953718841076, 0.010110795497894287, -0.018547305837273598, 0.016989590600132942, 0.0763101875782013, 0.0416206493973732, 0.08752065151929855, -0.012092760764062405, 0.03113519214093685, -0.03219042718410492, 0.012797970324754715, 0.013538764789700508, -0.02975240908563137, 0.03678983077406883, -0.005846079438924789, -0.01105553936213255, 0.03867030143737793, -0.02074146457016468, -0.011265086010098457, -0.023767532780766487, -0.009984560310840607, -0.023108458146452904, 0.012011353857815266, -0.010644523426890373, 0.0513606034219265, -0.02773405611515045, -0.00021139727323316038, 0.0009463390451855958, -0.03158264234662056, 0.051229290664196014, 0.04432860389351845, -0.003773423843085766, -0.04166809841990471, 0.02895456925034523, 0.03306133672595024, -0.015184277668595314, -0.00015773120685480535, -0.04412437602877617, -1.480350064753111e-08, -0.00869594607502222, 0.00013009380199946463, 0.01647108979523182, 0.05922885611653328, 0.04553878307342529, 0.03312685713171959, -0.09335850179195404, -0.03902722895145416, -0.020656151697039604, 0.012607559561729431, 0.06953950226306915, 0.07919187843799591, -0.07191091775894165, -0.0047850944101810455, 0.08800476789474487, 0.04758848249912262, -0.05219274014234543, -0.00752552505582571, -0.05771181359887123, -0.09295844286680222, -0.0045015024952590466, 0.001178039121441543, 0.02444724552333355, -0.06403965502977371, -0.0032275081612169743, -0.02796536311507225, -0.035407572984695435, 0.025036673992872238, -0.009852757677435875, 0.01325240544974804, 0.0011385205434635282, 0.17805425822734833, -0.036146294325590134, -0.007625906262546778, -0.032201241701841354, -0.042293235659599304, 0.004774454515427351, 0.02853180468082428, 0.07472298294305801, -0.014894150197505951, -0.056220654398202896, 0.02723579667508602, -0.011199447326362133, -0.1016678735613823, -0.019529249519109726, 0.027246462181210518, 0.03508107364177704, -0.0816056951880455, -0.0013377582654356956, -0.07635194808244705, -0.03995734825730324, 0.04078188166022301, 0.060128677636384964, 0.07254577428102493, 0.06967505812644958, 0.08909131586551666, 0.015957804396748543, -0.01487354189157486, -0.04674159735441208, -0.013411267660558224, 0.06513479351997375, 0.050905924290418625, 0.051483552902936935, 0.007092184852808714]\n"
     ]
    }
   ],
   "source": [
    "vectors = embeddings.embed_query('Hello')\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6da00dbd-e9c6-474e-ad4b-0c6b382afddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c0103bd-9659-4ad7-9e95-a62ec32c029a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_20884\\3644036516.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='ea1e06f6-38bd-480f-9cc5-505314b6d4e5', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:'),\n",
       " Document(id='83c11841-27af-476c-9895-538ea5885df6', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\")]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load documents\n",
    "loader = TextLoader('my_docs.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. split text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. Generate embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 4. store vectors in FAISS\n",
    "vectorstore = FAISS.from_documents(docs,embeddings)\n",
    "\n",
    "# 5. Retriever\n",
    "vectorstore.similarity_search('what is langchain?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed8957-6629-44ea-adb0-3f44d4727cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Retrieval QA Chain\n",
    "# -------------------------------\n",
    "combine retrieveal + generate \n",
    "-------------------  =========\n",
    "|                       |__ use llm ->generate an answer from retrieval documents\n",
    "|\n",
    "retrieve relevant documents from vectorstore\n",
    "\n",
    "retrieval + llm + prompt //chain =>from_chain_type() pipeline \n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "RetrievalQA.from_chain_type(llm=llm_object,retriever=retriever_object,return_source_documents=True)\n",
    "|\n",
    "query\n",
    "|\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dd35339-e42a-4d72-b17c-3c298690d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "490d496b-a16d-4615-b9fb-62b8f17bc4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Load documents\n",
    "loader = TextLoader('my_docs.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. split text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. Generate embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 4. store vectors in FAISS\n",
    "vectorstore = FAISS.from_documents(docs,embeddings)\n",
    "\n",
    "# 5. Retriever\n",
    "#vectorstore.similarity_search('what is langchain?')\n",
    "retriever_obj = vectorstore.as_retriever()\n",
    "\n",
    "# 6. use ollama model\n",
    "llm_obj = Ollama(model=\"gemma2:2b\")\n",
    "\n",
    "# 7. Build Retrieval QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriever_obj,\n",
    "                                       return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45c28832-0d7d-4795-a58d-3bfe635ecb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_20884\\2455835386.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain('what is langchain?')\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what is langchain?', 'result': 'LangChain is a framework for developing applications powered by large language models (LLMs). \\n', 'source_documents': [Document(id='ecc5f9d0-c347-4239-a198-457a98af90e1', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:'), Document(id='a36e5784-9eb8-45b2-b214-85da16337182', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\")]}\n"
     ]
    }
   ],
   "source": [
    "# 8. Ask query \n",
    "response = qa_chain('what is langchain?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "889c2ec7-5585-46ae-85de-f13c611f1039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework for developing applications powered by large language models (LLMs). \n",
      "\n",
      "\n",
      "Source:\n",
      "my_docs.txt\n",
      "my_docs.txt\n"
     ]
    }
   ],
   "source": [
    "print(response['result'])\n",
    "print(\"\\nSource:\")\n",
    "for var in response[\"source_documents\"]:\n",
    "    print(var.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53cea360-c91e-4533-a8ce-439f3d6f195e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what is langchain and how RAG is working?', 'result': \"LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies the development, productionization, and deployment of LLM-powered applications. \\n\\n**RAG** stands for **Reasoning About Graphs**, but it's not directly explained in the provided context.  Therefore, I don't have information to answer how RAG works within LangChain. \\n\", 'source_documents': [Document(id='ecc5f9d0-c347-4239-a198-457a98af90e1', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:'), Document(id='a36e5784-9eb8-45b2-b214-85da16337182', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\")]}\n",
      "LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies the development, productionization, and deployment of LLM-powered applications. \n",
      "\n",
      "**RAG** stands for **Reasoning About Graphs**, but it's not directly explained in the provided context.  Therefore, I don't have information to answer how RAG works within LangChain. \n",
      "\n",
      "\n",
      "Source:\n",
      "my_docs.txt\n",
      "my_docs.txt\n"
     ]
    }
   ],
   "source": [
    "# 8. Ask query \n",
    "response = qa_chain('what is langchain and how RAG is working?')\n",
    "print(response)\n",
    "print(response['result'])\n",
    "print(\"\\nSource:\")\n",
    "for var in response[\"source_documents\"]:\n",
    "    print(var.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "041630a9-4e9e-459d-8b25-442a245f076f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How to write simple python program?', 'result': \"This text describes LangChain and its various components, but it doesn't provide information on how to write a simple Python program. \\n\\n\\nTo learn how to write a simple Python program, you could refer to resources like:\\n\\n* **Official Python documentation:** [https://docs.python.org/3/tutorial/index.html](https://docs.python.org/3/tutorial/index.html)\\n* **W3Schools Python tutorial:** [https://www.w3schools.com/python/default.asp](https://www.w3schools.com/python/default.asp) \\n* **Codecademy Python courses:** [https://www.codecademy.com/catalog/language/python](https://www.codecademy.com/catalog/language/python)  \\n\\nLet me know if you want to explore specific aspects of Python programming or have any more questions! \\n\", 'source_documents': [Document(id='a36e5784-9eb8-45b2-b214-85da16337182', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\"), Document(id='ecc5f9d0-c347-4239-a198-457a98af90e1', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:')]}\n",
      "This text describes LangChain and its various components, but it doesn't provide information on how to write a simple Python program. \n",
      "\n",
      "\n",
      "To learn how to write a simple Python program, you could refer to resources like:\n",
      "\n",
      "* **Official Python documentation:** [https://docs.python.org/3/tutorial/index.html](https://docs.python.org/3/tutorial/index.html)\n",
      "* **W3Schools Python tutorial:** [https://www.w3schools.com/python/default.asp](https://www.w3schools.com/python/default.asp) \n",
      "* **Codecademy Python courses:** [https://www.codecademy.com/catalog/language/python](https://www.codecademy.com/catalog/language/python)  \n",
      "\n",
      "Let me know if you want to explore specific aspects of Python programming or have any more questions! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain('How to write simple python program?')\n",
    "print(response)\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "606da23a-8525-4d90-8d5f-b2a47835cf27",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'explain about langchain and tell me how to write simple python program?', 'result': 'LangChain is a framework designed for building applications powered by large language models (LLMs). Think of it as a toolbox that helps developers create sophisticated LLM-driven systems. \\n\\nHere\\'s what makes LangChain special:\\n\\n* **Simplifies Development:** It provides pre-built components and integrations, making it easier to get started with complex LLM tasks like answering questions, summarizing text, and even generating different creative content formats.\\n* **Enhances Statefulness:** LangChain allows you to build applications that can remember previous interactions (\"stateful agents\").  This means your application\\'s behavior will be more consistent and relevant over time. \\n* **Streamlined Production and Deployment:** It makes it easier to monitor, evaluate, and optimize your LLM applications after they\\'re built. This ensures reliable performance and a smoother user experience.\\n\\n**How to Write a Simple Python Program with LangChain**\\n\\nHere\\'s an example of how you could write a simple program using LangChain that leverages the power of OpenAI\\'s language model: \\n\\n```python\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import ConversationChain\\n\\n# Initialize the OpenAI LLM instance\\nllm = OpenAI(temperature=0.7)  \\n\\n# Create a conversation chain to interact with the LLM\\nconversation_chain = ConversationChain(llm=llm, verbose=True) \\n\\n# Start the conversation \\nprint(\"Let\\'s talk! What would you like me to do?\")\\nresponse = conversation_chain.run(input=\"You are an AI assistant who can help people with their daily tasks\")  \\nprint(\"Assistant:\", response)\\n```\\n\\n\\n**Explanation:**\\n\\n1. **Import Libraries:** We start by importing the necessary libraries from LangChain. `OpenAI` allows us to interact with OpenAI\\'s language model, and `ConversationChain` helps us create conversational AI systems. \\n2. **Instantiate LLM:** We create an instance of `OpenAI` with specific settings, like `temperature=0.7`, for controlling the creativity of the generated text (higher values mean more creative output).\\n3. **Create Conversation Chain:** We build a `ConversationChain` using our LLM and set up some verbose logging to see what\\'s happening within the system.  This helps you understand how your program interacts with the model. \\n4. **Start a Conversation:** The code then prompts the user for input (\"Let\\'s talk!\").  The `conversation_chain.run` method initiates the interaction, and the result is stored in the `response` variable, which we print to the console.\\n\\n\\n\\n**Keep Learning:** \\n\\nLangChain offers many more capabilities like integration with various LLM models (Google, Hugging Face), building complex applications with multiple LLMs or agents, and deploying your LLM-powered applications to different platforms!  There\\'s much more to explore within LangChain\\'s documentation: [https://langchain.com/](https://langchain.com/) \\n\\n\\n\\nLet me know if you have any other questions about LangChain.\\n', 'source_documents': [Document(id='ecc5f9d0-c347-4239-a198-457a98af90e1', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:'), Document(id='a36e5784-9eb8-45b2-b214-85da16337182', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\")]}\n",
      "LangChain is a framework designed for building applications powered by large language models (LLMs). Think of it as a toolbox that helps developers create sophisticated LLM-driven systems. \n",
      "\n",
      "Here's what makes LangChain special:\n",
      "\n",
      "* **Simplifies Development:** It provides pre-built components and integrations, making it easier to get started with complex LLM tasks like answering questions, summarizing text, and even generating different creative content formats.\n",
      "* **Enhances Statefulness:** LangChain allows you to build applications that can remember previous interactions (\"stateful agents\").  This means your application's behavior will be more consistent and relevant over time. \n",
      "* **Streamlined Production and Deployment:** It makes it easier to monitor, evaluate, and optimize your LLM applications after they're built. This ensures reliable performance and a smoother user experience.\n",
      "\n",
      "**How to Write a Simple Python Program with LangChain**\n",
      "\n",
      "Here's an example of how you could write a simple program using LangChain that leverages the power of OpenAI's language model: \n",
      "\n",
      "```python\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.chains import ConversationChain\n",
      "\n",
      "# Initialize the OpenAI LLM instance\n",
      "llm = OpenAI(temperature=0.7)  \n",
      "\n",
      "# Create a conversation chain to interact with the LLM\n",
      "conversation_chain = ConversationChain(llm=llm, verbose=True) \n",
      "\n",
      "# Start the conversation \n",
      "print(\"Let's talk! What would you like me to do?\")\n",
      "response = conversation_chain.run(input=\"You are an AI assistant who can help people with their daily tasks\")  \n",
      "print(\"Assistant:\", response)\n",
      "```\n",
      "\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. **Import Libraries:** We start by importing the necessary libraries from LangChain. `OpenAI` allows us to interact with OpenAI's language model, and `ConversationChain` helps us create conversational AI systems. \n",
      "2. **Instantiate LLM:** We create an instance of `OpenAI` with specific settings, like `temperature=0.7`, for controlling the creativity of the generated text (higher values mean more creative output).\n",
      "3. **Create Conversation Chain:** We build a `ConversationChain` using our LLM and set up some verbose logging to see what's happening within the system.  This helps you understand how your program interacts with the model. \n",
      "4. **Start a Conversation:** The code then prompts the user for input (\"Let's talk!\").  The `conversation_chain.run` method initiates the interaction, and the result is stored in the `response` variable, which we print to the console.\n",
      "\n",
      "\n",
      "\n",
      "**Keep Learning:** \n",
      "\n",
      "LangChain offers many more capabilities like integration with various LLM models (Google, Hugging Face), building complex applications with multiple LLMs or agents, and deploying your LLM-powered applications to different platforms!  There's much more to explore within LangChain's documentation: [https://langchain.com/](https://langchain.com/) \n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other questions about LangChain.\n",
      "\n",
      "\n",
      "Source:\n",
      "my_docs.txt\n",
      "my_docs.txt\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain('explain about langchain and tell me how to write simple python program?')\n",
    "print(response)\n",
    "print(response['result'])\n",
    "print(\"\\nSource:\")\n",
    "for var in response[\"source_documents\"]:\n",
    "    print(var.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd535a8b-7a8c-4f84-9737-5bb040b72217",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'explain #@$#@ASDF@4 info?', 'result': 'This context does not contain the information necessary to answer \"explain #@$#@ASDF@4 info?\" \\n\\nPlease provide me with more context or information.  \\n', 'source_documents': [Document(id='ecc5f9d0-c347-4239-a198-457a98af90e1', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:'), Document(id='a36e5784-9eb8-45b2-b214-85da16337182', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\")]}\n",
      "This context does not contain the information necessary to answer \"explain #@$#@ASDF@4 info?\" \n",
      "\n",
      "Please provide me with more context or information.  \n",
      "\n",
      "\n",
      "Source:\n",
      "my_docs.txt\n",
      "my_docs.txt\n"
     ]
    }
   ],
   "source": [
    "# 8. Ask query \n",
    "response = qa_chain('explain #@$#@ASDF@4 info?')\n",
    "print(response)\n",
    "print(response['result'])\n",
    "print(\"\\nSource:\")\n",
    "for var in response[\"source_documents\"]:\n",
    "    print(var.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbbffe8c-452d-4036-bac5-c43ab9d5130f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I likes to read python book')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "PromptTemplate.from_template('I likes to read python book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c3a2f90-51e9-4a77-8118-9cbb1cec4ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['myvar'], input_types={}, partial_variables={}, template='I likes to read {myvar} book')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate.from_template('I likes to read {myvar} book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdeb16d2-9d81-468e-ba33-2a06bbccb3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read java book'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate.from_template('I likes to read {myvar} book').format(myvar=\"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b4faff1-e50a-44bf-aef6-8918ad7d2a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read GenAI book'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate.from_template('I likes to read {myvar} book').format(myvar=\"GenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32f41dcd-cae2-4b23-aa21-036463b77efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read GenAI book'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_obj = PromptTemplate.from_template('I likes to read {myvar} book')\n",
    "prompt_obj.format(myvar='GenAI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ede338-41da-47b3-bb39-ed2899a66fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. prompt\n",
    "    - text => give to the model\n",
    "2. context\n",
    "    - constraints \n",
    "3. role prompting\n",
    "    - assign model a role \n",
    "4. structed output\n",
    "    - fixed format(json,table,bullet points,summary, keywords..)\n",
    "5. Constraints - limit\n",
    "        - ex: 150 words 5lines 3 rows \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e9393b3-54e2-4cd0-bdd8-ac5d7ac1dac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain short story about birds in english writen by Mr.abc'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_obj = PromptTemplate.from_template('Explain short story about {topic} in {lng} writen by {author}')\n",
    "\n",
    "prompt_obj.format(topic='birds',lng='english',author='Mr.abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3a6a7a8-914e-458f-9ac4-73e2d8955c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Your are helpful AI assistant'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Question:{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# role and content\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "#help(ChatPromptTemplate.from_messages)\n",
    "\n",
    "ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Your are helpful AI assistant\"),\n",
    "    (\"user\",\"Question:{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "010eff96-9d1c-4903-8706-66487bc1022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Your are helpful AI assistant\"),\n",
    "    (\"user\",\"Question:{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a07d4135-3b52-438e-aeca-4db319eeafce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Your are helpful AI assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Question:what is langchain?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt.format_messages(question=\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f82c3fd-366e-4282-ac43-8a0e8714930c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Your are helpful AI assistant'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Question:{question}'), additional_kwargs={})])\n",
       "| Ollama(model='gemma2:2b')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_llm_obj = Ollama(model=\"gemma2:2b\")\n",
    "chat_prompt|my_llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d95fc841-0fff-4cdf-856b-1f19ac0bc91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Gen AI\" stands for **Generative Artificial Intelligence**. \\n\\nHere\\'s a breakdown:\\n\\n**What it means:**\\n\\n* **Generative:**  It\\'s about creating new things - not just analyzing or classifying existing information. Gen AI can generate content like text, images, music, code, and even 3D models based on what you give it as input (e.g., a description of a sunset).\\n* **Artificial Intelligence:** This refers to systems that learn from data and perform tasks that typically require human intelligence, such as:\\n    * Understanding natural language\\n    * Recognizing objects and patterns\\n    * Making predictions\\n\\n**How it works:**\\n\\nGen AI uses complex algorithms trained on massive datasets. Think of it like a super-smart computer \"learning\" by analyzing tons of examples and figuring out the best way to create something similar. \\n\\n**Examples of Gen AI in action:**\\n\\n* **ChatGPT:** This popular chatbot can write stories, answer your questions, even generate different kinds of creative content.\\n* **DALL-E 2:** Generates images from text descriptions (\"a picture of a cat wearing a hat\"). \\n* **GitHub Copilot:** Helps programmers write code faster and more efficiently by suggesting completions based on context and project requirements.\\n\\n**Overall, Gen AI is a revolutionary technology that has the potential to transform many fields and aspects of our lives.**\\n\\n\\nLet me know if you have any other questions!  \\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_prompt|my_llm_obj\n",
    "chain.invoke({'question':'what is gen AI'}) # chainObject.inovkeMethod({'placeholder':'userQuery'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba14e46a-5348-4e7b-9d77-6e4d3362094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"don't know\"),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "my_llm_obj = Ollama(model=\"gemma2:2b\")\n",
    "\n",
    "chain = prompt|my_llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b20034e-765d-4999-95d7-074fda352ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = chain.invoke({'question': 'what is python?'})\n",
    "r2 = chain.invoke({'question': 'asdfsfsfasdfas'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b526959e-e263-408f-949d-e9ceb3772fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    }
   ],
   "source": [
    "print(bool(r1),bool(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "20ef0003-b812-4d1f-8660-16b013860c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a popular, versatile programming language known for its readability and beginner-friendliness. Here's a breakdown of what makes Python special:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **High-Level Language:** Python simplifies complex tasks by abstracting away low-level details, making it easier to write code. \n",
      "* **Interpreted:**  Unlike compiled languages like C++, you don't need to translate the code into machine instructions before execution  Python runs line by line directly. This makes development faster and more interactive.\n",
      "* **Dynamically Typed:** You don't have to explicitly define data types; Python infers them based on your code. This reduces the amount of boilerplate code. \n",
      "* **Object-Oriented:**  Python supports object-oriented programming (OOP), a structure for organizing code into reusable modules and objects.\n",
      "\n",
      "**Why is it Popular?**\n",
      "\n",
      "* **Versatility:** Python shines in various domains, including:\n",
      "    * **Web Development:** Building websites, web apps, and backend systems (using frameworks like Django and Flask).\n",
      "    * **Data Science & Machine Learning:**  Analyzing data, building predictive models, and creating AI solutions (with libraries like Pandas, Scikit-learn, TensorFlow).\n",
      "    * **Scripting and Automation:** Automating tasks, from system administration to web scraping. \n",
      "    * **Game Development:** Creating games through libraries like Pygame.\n",
      "\n",
      "**Getting Started with Python:**\n",
      "\n",
      "1.  **Install Python:** Download the latest version from [https://www.python.org/](https://www.python.org/).\n",
      "2.  **Write Code:** Use a text editor (like Notepad, Sublime Text, VS Code) to create files with `.py` extension. \n",
      "3.  **Run your code:** Open a terminal or command prompt, navigate to the directory where you saved your file, and run `python filename.py`.\n",
      "\n",
      "**Where can you learn more?**\n",
      "* **Python documentation:** [https://docs.python.org/](https://docs.python.org/) \n",
      "* **Online courses and tutorials:**  Coursera, Codecademy, Udemy offer comprehensive resources. \n",
      "\n",
      "\n",
      "Let me know if you'd like to explore specific aspects of Python or have further questions! \n"
     ]
    }
   ],
   "source": [
    "print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b59c45d6-5f6f-4d00-9759-489bb2f65d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you're trying to communicate something, but the input is just a bunch of random letters.  \n",
      "\n",
      "Could you try rephrasing your question or providing more information? \n",
      "\n",
      "For example, are you wondering about something specific, like:\n",
      "\n",
      "* **A translation:** \"What does 'asdfsfsfasdfas' mean in another language?\"\n",
      "* **Something technical:** \"I'm having trouble with my system and don't know what to do. Can you help me troubleshoot?\" \n",
      "* **Getting started with a task:** \"Help me understand how to start using this website.\"\n",
      "\n",
      "\n",
      "The more information you give, the better I can understand and help you!  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b808a9-817a-46d2-aa45-da6ace55f584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72949979-b531-480d-8cb0-21762fa4d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"\"),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "my_llm_obj = Ollama(model=\"gemma2:2b\")\n",
    "\n",
    "chain = prompt|my_llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36bb1319-5798-48bf-91de-508cc2cb2e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you might be typing a question, but the input is jumbled.  Can you please re-type your question clearly so I can help?  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({'question':'xyzafdsf?'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "48d83e29-6ab7-4d47-b2df-39f3adf8e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"Say don't know\"),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "my_llm_obj = Ollama(model=\"gemma2:2b\")\n",
    "\n",
    "chain = prompt|my_llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a9b25b4-8d86-4159-9130-d52f916600dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I say \"Don't know\" \n",
      "\n",
      "Is there anything else I can help you with?   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({'question':'xyzafdsf?'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d0f4219-6c24-48f4-a2fd-b58a73e19512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get standard output format\n",
    "# ---------------------------------\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "becc6d95-6e6a-460b-9819-3eddb37373d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenAI, or **Generative Artificial Intelligence**, is a fascinating area of AI focused on creating new content rather than just analyzing or manipulating existing information.  Think of it as an AI artist, writer, musician, or chef!\n",
      "\n",
      "Here's how it works in simple terms:\n",
      "\n",
      "* **Learning:** GenAI learns from tons of data (text, images, code, etc.) to understand patterns and relationships.\n",
      "* **Generating:** Based on this learning, it can generate something new  a poem, an image, a piece of music, even a design for a product! \n",
      "\n",
      "**Examples of GenAI in action:**\n",
      "\n",
      "* **Text Generation:** Creating stories, articles, scripts, summaries, emails, and more\n",
      "* **Image Generation:** Generating realistic images based on descriptions (think Midjourney or DALL-E 2)\n",
      "* **Music Generation:** Composing melodies, harmonies, or even entire songs.\n",
      "* **Code Generation:** Helping programmers write code faster and easier.\n",
      "\n",
      "**It's important to note:**\n",
      "\n",
      "* **Creativity vs. Imitation:** While GenAI can create original content, it often relies on existing data as inspiration. It's still learning and improving how to think creatively! \n",
      "* **Ethical Considerations:**  GenAI raises important questions about ownership, copyright, and the potential impact of AI-generated content on society.\n",
      "\n",
      "\n",
      "**In short, GenAI is a powerful tool that's changing how we create and interact with information in new and exciting ways!** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = StrOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"\"),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "my_llm_obj = Ollama(model=\"gemma2:2b\")\n",
    "\n",
    "chain = prompt|my_llm_obj|output\n",
    "response = chain.invoke({'question':'what is genAI?'})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "69272a71-7874-44e8-9546-327fb016fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"Your are a helpful AI assistant\"),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "my_llm_obj = Ollama(model=\"gemma2:2b\")\n",
    "output = StrOutputParser()\n",
    "chain = prompt|my_llm_obj|output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d712344-09eb-4335-9f8e-02a4883b5c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly, let's explore how to calculate factorials in Python.\n",
      "\n",
      "**Understanding Factorial**\n",
      "\n",
      "The factorial of a non-negative integer *n*, denoted by *n!*, is the product of all positive integers less than or equal to *n*. For example:\n",
      "\n",
      "5! = 5 * 4 * 3 * 2 * 1 = 120\n",
      "\n",
      "**Python Code for Factorial Calculation**\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "  \"\"\"Calculates the factorial of a non-negative integer.\n",
      "\n",
      "  Args:\n",
      "      n: The non-negative integer to calculate the factorial of.\n",
      "\n",
      "  Returns:\n",
      "      The factorial of n, if n is non-negative. \n",
      "      Raises ValueError if n is negative.\n",
      "  \"\"\"\n",
      "  if n < 0:\n",
      "    raise ValueError(\"Factorial is not defined for negative numbers.\")\n",
      "  elif n == 0:\n",
      "    return 1  # Factorial of 0 is 1\n",
      "  else:\n",
      "    result = 1\n",
      "    for i in range(1, n+1):\n",
      "      result *= i\n",
      "    return result\n",
      "\n",
      "# Get input from the user\n",
      "num = int(input(\"Enter a non-negative integer: \"))\n",
      "\n",
      "# Calculate and print the factorial\n",
      "print(f\"The factorial of {num} is {factorial(num)}\") \n",
      "```\n",
      "\n",
      "\n",
      "**Explanation**\n",
      "\n",
      "1. **`factorial(n)` Function:**\n",
      "   - Takes an integer `n` as input.\n",
      "   - Checks if `n` is negative; if so, it raises a `ValueError`.  \n",
      "   - If `n` is 0, returns 1 (base case for factorial).\n",
      "   - If `n` is positive, iteratively calculates the product of numbers from 1 to `n` using a loop.\n",
      "\n",
      "2. **User Input:**\n",
      "   - Prompts the user to enter a non-negative integer using `input()`.\n",
      "   - The entered value is converted to an integer using `int()`.\n",
      "\n",
      "3. **Calculation and Output:**\n",
      "   - Calls the `factorial()` function with the input number (`num`). \n",
      "   - Prints the result of the factorial calculation formatted nicely.\n",
      "\n",
      "\n",
      "Let me know if you have any further questions or would like me to elaborate on any specific part! \n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({'question':'How to write factorial program in python?'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "791c8831-d53c-443b-b0f3-bd3f0ba75c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you might have made a typo!  \"fasfsfasfas\" doesn't mean anything. \n",
      "\n",
      "Is there something specific you meant to ask?  I can help with lots of things, just let me know! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({'question':'what is fasfsfasfas?'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f563871-34ed-444d-976e-dc29754fe7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4bcbc5c6-367f-4aad-be91-00ead34262da",
   "metadata": {},
   "outputs": [],
   "source": [
    "groqK = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bee6134b-2940-45e9-8f28-12ef187a9586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000270D965F8C0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000270D972C590>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "ChatGroq(model='gemma2-9b-it',groq_api_key=groqK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3d6d5b35-9133-4325-b42c-43c4ae5932e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = ChatGroq(model='llama3-8b-8192',groq_api_key=groqK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f886df96-942f-4c24-b6a2-96d0a8701ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c199be1c-4fc0-4de3-a05b-72f5752e64b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Karthik! \\n\\nWhat can I help you with today?   Are you teaching something specific, or just here to chat? \\n\\n\\nLet me know if you have any questions or if there's anything I can do for you.   \\n\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "my_llm_obj.invoke([HumanMessage(content=\"Hello my name is karthik and i am instructor\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b862d949-0f61-4d89-9203-50195f5f12e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Karthik, it's nice to meet you. You're an instructor, that's great. What subject or field do you teach? I'm here to help with any questions or topics you'd like to discuss.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 46, 'total_tokens': 92, 'completion_time': 0.054661895, 'prompt_time': 0.002121855, 'queue_time': 0.041865791, 'total_time': 0.05678375}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--7118961f-e21a-4a79-a546-20aa097da9d9-0', usage_metadata={'input_tokens': 46, 'output_tokens': 46, 'total_tokens': 92})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.invoke([HumanMessage(content=\"Hello my name is karthik and i am instructor\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02000d68-5054-42ac-89df-c22a8553a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9c2d7cee-b363-4bf6-820f-5ca1598456cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Karthik, it's nice to meet you. As an instructor, I'm sure you have a lot of experience in teaching and guiding students. What subject or topic do you typically teach, and what kind of students do you work with?\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "groqK = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "llm_model = ChatGroq(model='llama-3.1-8b-instant', groq_api_key=groqK)\n",
    "response = llm_model.invoke([HumanMessage(content=\"Hello my name is Karthik and I am an instructor\")])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17aa47b-84bb-46b6-b085-34fc3cc1dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama-3.1-8b-instant\n",
    "llama-3.1-70b-versatile\n",
    "mixtral-8x7b\n",
    "gemma-7b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb227c3e-0a47-4ae2-adcb-542f096f0db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HumanMessage - user input \n",
    "\n",
    "AIMessage - llm_response(ai assistant response)\n",
    "\n",
    "SystemMessage - sets context (or) behaviour of the model (ex: your are helpful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aad3037f-aaad-4bbd-9d99-419b2b9595f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\". Do you have any questions or topics you'd like to discuss?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 41, 'total_tokens': 56, 'completion_time': 0.017079602, 'prompt_time': 0.002269297, 'queue_time': 0.042094363, 'total_time': 0.019348899}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_e32974efee', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--535e4a33-761e-49b6-a7b7-8628e6f589b3-0', usage_metadata={'input_tokens': 41, 'output_tokens': 15, 'total_tokens': 56})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "llm_model.invoke([AIMessage(content=\"Hello Karthik, it's nice to meet you\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "15d09860-782c-4bde-ad4b-b5e466659c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='It looks like you forgot to say something. Do you want to chat about something in particular or just say hello?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 57, 'total_tokens': 81, 'completion_time': 0.044928818, 'prompt_time': 0.003100231, 'queue_time': 0.041000049, 'total_time': 0.048029049}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_7b3cfae3af', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--78cf7db0-d081-4293-adb4-9a2ff28b54cb-0', usage_metadata={'input_tokens': 57, 'output_tokens': 24, 'total_tokens': 81})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "llm_model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hello,My name is Tom\"),\n",
    "        AIMessage(content=\"Hello Tom,How are you\"),\n",
    "        HumanMessage(content=\"\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3bd2cb-6cd4-4f0e-bfb9-29cca2760549",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Message history and memory system\n",
    "## ------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "066ee35e-949a-4e1f-bf0e-746b919e4c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p1': 'pA', 'p2': 'pB'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products={}\n",
    "# add new product - pid ->pname\n",
    "products['p1']=['pA',1010,'v1']\n",
    "products['p2']='pB'\n",
    "products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a5f59d52-e479-445e-baec-d9340d0edf73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'K0': 'Value-100',\n",
       " 'K1': 'Value-101',\n",
       " 'K2': 'Value-102',\n",
       " 'K3': 'Value-103',\n",
       " 'K4': 'Value-104'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d={}\n",
    "for var in range(5):\n",
    "    d['K'+str(var)] = 'Value-'+str(var+100)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b2f7e-1faa-4cfb-86d3-cdf2f633c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "every chat -> session_id => message //dict logic\n",
    "\n",
    "1. dict holding all message histories \n",
    "2. session_id - key ; Message - value\n",
    "                        |->ChatMessageHistory([])\n",
    "\n",
    "Chathistory\n",
    "|->ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "732ba180-7a44-4add-b066-95952e04db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ea2b0d8a-41cd-423e-8dcf-6abdf2114b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "store={}\n",
    "def get_session_history(session_id: str) ->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(llm_model,get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8823d154-8be6-46d0-b636-52572ce15ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Tom, it's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 41, 'total_tokens': 67, 'completion_time': 0.024495948, 'prompt_time': 0.001983163, 'queue_time': 0.040976643, 'total_time': 0.026479111}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--88e72fe8-baa0-46d9-b9f9-e640dc85a40e-0', usage_metadata={'input_tokens': 41, 'output_tokens': 26, 'total_tokens': 67})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(RunnableWithMessageHistory)\n",
    "my_config = {\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "\n",
    "with_message_history.invoke([HumanMessage(content='Hi,My name is Tom')],config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dfee6d1d-cb49-48c2-b863-6c9e94c28794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We've had this conversation before, Tom. What's on your mind today?\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke([HumanMessage(content='Hi,My name is Tom')],config=my_config)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6b5a3379-f98f-4ff0-85d8-4c9bd864b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config = {\"configurable\":{\"session_id\":\"chat2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2702d6ff-787d-44c7-a859-df44e5909023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have any information about your name. I'm a large language model, I don't have the ability to retain information about individual users or their personal details. Each time you interact with me, it's a new conversation and I don't have any prior knowledge about you. If you'd like to share your name, I'd be happy to chat with you!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 39, 'total_tokens': 115, 'completion_time': 0.079759811, 'prompt_time': 0.00206808, 'queue_time': 0.042249699, 'total_time': 0.081827891}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_33e8adf159', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--773a08fd-fb68-4617-afb6-8798fd35237b-0', usage_metadata={'input_tokens': 39, 'output_tokens': 76, 'total_tokens': 115})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke([HumanMessage(content=\"what's my name\")],config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "47600720-415f-4501-a131-eb57bc416273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Tom.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 112, 'total_tokens': 118, 'completion_time': 0.004327025, 'prompt_time': 0.006048733, 'queue_time': 0.041685657, 'total_time': 0.010375758}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_7b3cfae3af', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--d9ab4ed8-5d89-4a01-954f-0bee69f0db96-0', usage_metadata={'input_tokens': 112, 'output_tokens': 6, 'total_tokens': 118})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_config = {\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "with_message_history.invoke([HumanMessage(content=\"what's my name\")],config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1eb20496-37bd-44a9-81e1-5849c51adfa7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chat1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi,My name is Tom', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Tom, it's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 41, 'total_tokens': 67, 'completion_time': 0.024495948, 'prompt_time': 0.001983163, 'queue_time': 0.040976643, 'total_time': 0.026479111}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--88e72fe8-baa0-46d9-b9f9-e640dc85a40e-0', usage_metadata={'input_tokens': 41, 'output_tokens': 26, 'total_tokens': 67}), HumanMessage(content='Hi,My name is Tom', additional_kwargs={}, response_metadata={}), AIMessage(content=\"We've had this conversation before, Tom. What's on your mind today?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 82, 'total_tokens': 99, 'completion_time': 0.028912401, 'prompt_time': 0.00503029, 'queue_time': 0.04076803, 'total_time': 0.033942691}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_e32974efee', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--81128373-b01b-48d5-8957-2b900b7bcb8c-0', usage_metadata={'input_tokens': 82, 'output_tokens': 17, 'total_tokens': 99}), HumanMessage(content=\"what's my name\", additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Tom.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 112, 'total_tokens': 118, 'completion_time': 0.004327025, 'prompt_time': 0.006048733, 'queue_time': 0.041685657, 'total_time': 0.010375758}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_7b3cfae3af', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--d9ab4ed8-5d89-4a01-954f-0bee69f0db96-0', usage_metadata={'input_tokens': 112, 'output_tokens': 6, 'total_tokens': 118})]),\n",
      " 'chat2': InMemoryChatMessageHistory(messages=[HumanMessage(content=\"what's my name\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't have any information about your name. I'm a large language model, I don't have the ability to retain information about individual users or their personal details. Each time you interact with me, it's a new conversation and I don't have any prior knowledge about you. If you'd like to share your name, I'd be happy to chat with you!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 39, 'total_tokens': 115, 'completion_time': 0.079759811, 'prompt_time': 0.00206808, 'queue_time': 0.042249699, 'total_time': 0.081827891}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_33e8adf159', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--773a08fd-fb68-4617-afb6-8798fd35237b-0', usage_metadata={'input_tokens': 39, 'output_tokens': 76, 'total_tokens': 115})])}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5f05639c-7d15-48ac-9d05-9e2e72df5c62",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Groq provides a range of AI models, including LLM (Large Language Model) models. While I can guide you through the process, I need to clarify that Groq's models are typically available to Groq customers or through their APIs.\\n\\nHowever, you can explore the following options to obtain free or low-cost access to LLM models from Groq or other sources:\\n\\n1. **Groq's Open-Source Models**: Groq has open-sourced some of their models, which can be found on GitHub. You can review and utilize these models for free.\\n2. **Low-Code or No-Code Platforms**: Some low-code or no-code platforms, like Google Cloud's AutoML or Microsoft Azure's Form Recognizer, offer free or low-cost access to pre-trained LLM models. You can explore these platforms to see if they meet your requirements.\\n3. **Hugging Face Transformers**: Hugging Face's Transformers library provides a wide range of pre-trained LLM models, including some that are similar to those used by Groq. You can download and use these models in your applications.\\n4. **TensorFlow or PyTorch**: You can also use TensorFlow or PyTorch to train your own LLM models. These frameworks provide pre-built models and tools to help you train and fine-tune your models.\\n5. **Groq's Free Trial**: If you're interested in using Groq's commercial models, you can sign up for a free trial to access their services.\\n\\nKeep in mind that free models or trials often come with limitations, such as lower model capacity or usage quotas. Be sure to review the terms and conditions before using any model.\\n\\nWould you like me to elaborate on any of these options or help you get started with the process?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 358, 'prompt_tokens': 137, 'total_tokens': 495, 'completion_time': 0.779792768, 'prompt_time': 0.010784392, 'queue_time': 0.044004133, 'total_time': 0.79057716}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--1aac4740-7c83-4f24-a813-87ddd8a895f4-0', usage_metadata={'input_tokens': 137, 'output_tokens': 358, 'total_tokens': 495})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_config = {\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "with_message_history.invoke([HumanMessage(content=\"help to get free llm models from groq\")],config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "94a2c868-0436-4049-88ea-42d996f6a373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Tom. How can I assist you further in exploring LLM models from Groq?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 508, 'total_tokens': 529, 'completion_time': 0.030441034, 'prompt_time': 0.036902984, 'queue_time': 0.042035616, 'total_time': 0.067344018}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_e32974efee', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--47dde1b4-400c-4a35-97ff-9b639df8f06b-0', usage_metadata={'input_tokens': 508, 'output_tokens': 21, 'total_tokens': 529})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_config = {\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "with_message_history.invoke([HumanMessage(content=\"remember my name?\")],config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4be0ced9-0329-4ded-b75a-f2ccfdbc6d7a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi,My name is Tom', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Tom, it's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 41, 'total_tokens': 67, 'completion_time': 0.024495948, 'prompt_time': 0.001983163, 'queue_time': 0.040976643, 'total_time': 0.026479111}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--88e72fe8-baa0-46d9-b9f9-e640dc85a40e-0', usage_metadata={'input_tokens': 41, 'output_tokens': 26, 'total_tokens': 67}), HumanMessage(content='Hi,My name is Tom', additional_kwargs={}, response_metadata={}), AIMessage(content=\"We've had this conversation before, Tom. What's on your mind today?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 82, 'total_tokens': 99, 'completion_time': 0.028912401, 'prompt_time': 0.00503029, 'queue_time': 0.04076803, 'total_time': 0.033942691}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_e32974efee', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--81128373-b01b-48d5-8957-2b900b7bcb8c-0', usage_metadata={'input_tokens': 82, 'output_tokens': 17, 'total_tokens': 99}), HumanMessage(content=\"what's my name\", additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Tom.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 112, 'total_tokens': 118, 'completion_time': 0.004327025, 'prompt_time': 0.006048733, 'queue_time': 0.041685657, 'total_time': 0.010375758}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_7b3cfae3af', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--d9ab4ed8-5d89-4a01-954f-0bee69f0db96-0', usage_metadata={'input_tokens': 112, 'output_tokens': 6, 'total_tokens': 118}), HumanMessage(content='help to get free llm models from groq', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Groq provides a range of AI models, including LLM (Large Language Model) models. While I can guide you through the process, I need to clarify that Groq's models are typically available to Groq customers or through their APIs.\\n\\nHowever, you can explore the following options to obtain free or low-cost access to LLM models from Groq or other sources:\\n\\n1. **Groq's Open-Source Models**: Groq has open-sourced some of their models, which can be found on GitHub. You can review and utilize these models for free.\\n2. **Low-Code or No-Code Platforms**: Some low-code or no-code platforms, like Google Cloud's AutoML or Microsoft Azure's Form Recognizer, offer free or low-cost access to pre-trained LLM models. You can explore these platforms to see if they meet your requirements.\\n3. **Hugging Face Transformers**: Hugging Face's Transformers library provides a wide range of pre-trained LLM models, including some that are similar to those used by Groq. You can download and use these models in your applications.\\n4. **TensorFlow or PyTorch**: You can also use TensorFlow or PyTorch to train your own LLM models. These frameworks provide pre-built models and tools to help you train and fine-tune your models.\\n5. **Groq's Free Trial**: If you're interested in using Groq's commercial models, you can sign up for a free trial to access their services.\\n\\nKeep in mind that free models or trials often come with limitations, such as lower model capacity or usage quotas. Be sure to review the terms and conditions before using any model.\\n\\nWould you like me to elaborate on any of these options or help you get started with the process?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 358, 'prompt_tokens': 137, 'total_tokens': 495, 'completion_time': 0.779792768, 'prompt_time': 0.010784392, 'queue_time': 0.044004133, 'total_time': 0.79057716}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--1aac4740-7c83-4f24-a813-87ddd8a895f4-0', usage_metadata={'input_tokens': 137, 'output_tokens': 358, 'total_tokens': 495}), HumanMessage(content='remember my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Tom. How can I assist you further in exploring LLM models from Groq?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 508, 'total_tokens': 529, 'completion_time': 0.030441034, 'prompt_time': 0.036902984, 'queue_time': 0.042035616, 'total_time': 0.067344018}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_e32974efee', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--47dde1b4-400c-4a35-97ff-9b639df8f06b-0', usage_metadata={'input_tokens': 508, 'output_tokens': 21, 'total_tokens': 529})]),\n",
       " 'chat2': InMemoryChatMessageHistory(messages=[HumanMessage(content=\"what's my name\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't have any information about your name. I'm a large language model, I don't have the ability to retain information about individual users or their personal details. Each time you interact with me, it's a new conversation and I don't have any prior knowledge about you. If you'd like to share your name, I'd be happy to chat with you!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 39, 'total_tokens': 115, 'completion_time': 0.079759811, 'prompt_time': 0.00206808, 'queue_time': 0.042249699, 'total_time': 0.081827891}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_33e8adf159', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--773a08fd-fb68-4617-afb6-8798fd35237b-0', usage_metadata={'input_tokens': 39, 'output_tokens': 76, 'total_tokens': 115})])}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "16f8b85a-8900-454f-b1ab-7ef0421e39cb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Tom, it's nice to meet you. Is there something I can help you with today? Do you have any questions or topics you'd like to discuss?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 46, 'total_tokens': 80, 'completion_time': 0.042351455, 'prompt_time': 0.002077108, 'queue_time': 0.042493752, 'total_time': 0.044428563}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_e32974efee', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--9b5da659-35cc-4d38-b96b-dc2c7f1f5bd3-0', usage_metadata={'input_tokens': 46, 'output_tokens': 34, 'total_tokens': 80})"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatMessagePromptTemplate,MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages([('system','you are helpful AI assistant'),\n",
    "                                  MessagesPlaceholder(variable_name=\"question\")])\n",
    "\n",
    "chain = prompt|llm_model\n",
    "\n",
    "chain.invoke({'question':[HumanMessage(content=\"Hello,My name is Tom\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e1a2a313-b061-4da6-b0dc-99d964fd5e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bff79e03-132b-4f91-b21c-d0b408c7a65c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have any information about your name as our conversation has just started. If you'd like to share your name, I'd be happy to know it and address you by it in our conversation.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 44, 'total_tokens': 86, 'completion_time': 0.063420772, 'prompt_time': 0.006324023, 'queue_time': 0.041893817, 'total_time': 0.069744795}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_7b3cfae3af', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--17985126-7ada-4b5b-9225-b0146f1bee8c-0', usage_metadata={'input_tokens': 44, 'output_tokens': 42, 'total_tokens': 86})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_config={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "with_message_history.invoke(HumanMessage(content=\"What's my name\"),config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c433e1d8-c736-47db-9ba4-19a70d90c4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Tom.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 547, 'total_tokens': 553, 'completion_time': 0.007812393, 'prompt_time': 0.043550404, 'queue_time': 0.046164706, 'total_time': 0.051362797}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--930677a8-be30-42bf-8468-5764240b79fc-0', usage_metadata={'input_tokens': 547, 'output_tokens': 6, 'total_tokens': 553})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_config={\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "with_message_history.invoke(HumanMessage(content=\"What's my name\"),config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "54593b45-855b-4fd5-9bee-5bb6b86d30e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Tom.\n"
     ]
    }
   ],
   "source": [
    "my_config={\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "response = with_message_history.invoke(HumanMessage(content=\"What's my name\"),config=my_config)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e8954632-d9f4-4c06-925f-3f1c686bf8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour Tom, comment allez-vous ? (Hello Tom, how are you ?) Je suis l pour vous aider, n'hsitez pas  me poser vos questions. (I am here to help you, don't hesitate to ask me your questions.)\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([('system','your are helpful AI assistant,Answer all the question in {language}.'),\n",
    "                                  MessagesPlaceholder(variable_name=\"message\")])\n",
    "chain = prompt|llm_model\n",
    "response = chain.invoke({'message':[HumanMessage(content='Hello, My name is Tom')],\"language\":\"French\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9e971a1d-c192-45df-8ffc-6240d2a3d106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ,                  ?\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'message':[HumanMessage(content='Hello, My name is Tom')],\"language\":\"Hindi\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7c2c640c-463a-4396-bd65-94af3dccaeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ,      .    ?\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'message':[HumanMessage(content='Hello, My name is Tom')],\"language\":\"telugu\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6bcbf8ca-230d-4462-aa41-b3eb9e0e09d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!     ,    .    ?\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'message':[HumanMessage(content='Hello, My name is Tom')],\"language\":\"tamil\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e558389d-1aae-47e8-9271-5dad569dbf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ! (Nimahella Haringya Tom avara swagatha!)     ! (Nimgage sahaaya maadalu nanu illide)\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'message':[HumanMessage(content='Hello, My name is Tom')],\"language\":\"kannada\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8f1dd-ae7c-42e1-8067-1cec4a48b5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4b986451-1f00-4958-b18d-7d157bba0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7f732-08e7-49b0-bbc1-d4417d75471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split data\n",
    "loader = TextLoader('my_docs.txt')\n",
    "docs = loader.load()\n",
    "splitter = CharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks,embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# llm\n",
    "llm_model = ChatGroq(model='llama3-8b-8192',groq_api_key=groqK)\n",
    "\n",
    "# Prompt and chain\n",
    "prompt = ChatPromptTemplate.from_template(\"Use the context to answer:\\n{context}\\nQuestion:{input}\")\n",
    "qa_chain = create_stuff_documents_chain(llm_model,prompt)\n",
    "rag_chain = create_retrieval_chain(retriever,qa_chain)\n",
    "\n",
    "# Query\n",
    "response = rag_chain.invoke({'input':\"what is self-refelection?\"})\n",
    "\n",
    "# Response\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065931aa-aac7-4518-be4c-d4b994a2e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### end of day2 #################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
